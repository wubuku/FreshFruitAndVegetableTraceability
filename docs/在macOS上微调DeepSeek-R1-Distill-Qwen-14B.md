# åœ¨macOS (Apple Silicon)ä¸Šå¾®è°ƒDeepSeek-R1-Distill-Qwen-14BæŒ‡å—

æœ¬æŒ‡å—å°†å¸®åŠ©æ‚¨åœ¨macOSï¼ˆApple Siliconï¼‰è®¾å¤‡ä¸Šä½¿ç”¨Hugging Face PEFTåº“å¾®è°ƒDeepSeek-R1-Distill-Qwen-14Bæ¨¡å‹ã€‚åŸæŒ‡å—åŸºäºä½¿ç”¨Unslothæ¡†æ¶åœ¨RTX 4090ä¸Šå¾®è°ƒï¼Œä½†ç”±äºUnslothç›®å‰ä¸æ”¯æŒApple Siliconçš„MPSåç«¯ï¼Œæœ¬æŒ‡å—å·²é’ˆå¯¹Apple Siliconç¯å¢ƒåšäº†å…¨é¢è°ƒæ•´ï¼Œæ”¹ç”¨æ ‡å‡†PEFTåº“å®ç°ã€‚

## ä¸€ã€ç¡¬ä»¶è¦æ±‚ä¸å‡†å¤‡

### æ¨èé…ç½®
- æœºå‹ï¼šM2 Pro/Maxæˆ–æ›´é«˜é…ç½®çš„Mac
- å†…å­˜ï¼šè‡³å°‘32GBï¼Œå»ºè®®64GBä»¥ä¸Š
- å­˜å‚¨ï¼šè‡³å°‘100GBå¯ç”¨ç©ºé—´
- macOSç‰ˆæœ¬ï¼šVentura (13.0)æˆ–æ›´æ–°ç‰ˆæœ¬

> **æ³¨æ„**ï¼š14Bå‚æ•°æ¨¡å‹åœ¨macOSä¸Šæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œè¯·ç¡®ä¿æ‚¨æœ‰è¶³å¤Ÿå†…å­˜å¹¶åšå¥½æ€§èƒ½é¢„æœŸã€‚æ­¤å¤–ï¼ŒDeepSeek-R1-Distill-Qwenç³»åˆ—æ¨¡å‹çš„æŸäº›æ“ä½œå¯èƒ½ä¸MPSåç«¯å­˜åœ¨å…¼å®¹æ€§é—®é¢˜ï¼Œä¸‹æ–‡æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚

> **âš ï¸ ç‰¹åˆ«æé†’**ï¼šMPSåç«¯åœ¨Hugging Faceçš„Accelerateåº“ä¸­**ä¸è¢«è¯†åˆ«ä¸ºæ”¯æŒfp16æ··åˆç²¾åº¦è®­ç»ƒ**çš„è®¾å¤‡ã€‚æœ¬æŒ‡å—å·²é’ˆå¯¹æ­¤é—®é¢˜æä¾›äº†è§£å†³æ–¹æ¡ˆï¼Œç¡®ä¿å°†è®­ç»ƒå‚æ•°ä¸­çš„`fp16=False`ã€‚

## äºŒã€ç¯å¢ƒé…ç½®

### 1. å®‰è£…å¿…è¦çš„è½¯ä»¶

```bash
# å®‰è£…Homebrewï¼ˆå¦‚æœå°šæœªå®‰è£…ï¼‰
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

# å®‰è£…Pythonç›¸å…³å·¥å…·
brew install python@3.11 cmake

# å®‰è£…git-lfsï¼ˆç”¨äºæ¨¡å‹ä¸‹è½½ï¼‰
brew install git-lfs
git lfs install
```

### 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

```bash
# åˆ›å»ºé¡¹ç›®ç›®å½•
mkdir -p ~/Documents/llm-finetune
cd ~/Documents/llm-finetune

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3 -m venv peft-env
source peft-env/bin/activate
```

### 3. å®‰è£…ä¾èµ–

```bash
# ç¡®ä¿pipæ˜¯æœ€æ–°ç‰ˆæœ¬
pip install --upgrade pip

# å®‰è£…PyTorch with MPSæ”¯æŒ
pip install torch torchvision torchaudio

# å®‰è£…NumPy 1.xç‰ˆæœ¬è§£å†³å…¼å®¹æ€§é—®é¢˜
pip install numpy==1.26.4

# å®‰è£…æ ¸å¿ƒä¾èµ–åº“
pip install transformers datasets peft accelerate huggingface_hub wandb psutil
```

#### PyTorchå®‰è£…è¯¦è§£

ä¸Šè¿°å‘½ä»¤ `pip install torch torchvision torchaudio` ä¸€æ¬¡æ€§å®‰è£…äº†ä¸‰ä¸ªç›¸å…³çš„PythonåŒ…ï¼š

1. **torch**ï¼š
   - PyTorchçš„æ ¸å¿ƒåº“
   - ç”±Meta(åŸFacebook)å¼€å‘çš„æµè¡Œæ·±åº¦å­¦ä¹ æ¡†æ¶
   - æä¾›å¼ é‡è®¡ç®—å’ŒåŠ¨æ€ç¥ç»ç½‘ç»œåŠŸèƒ½

2. **torchvision**ï¼š
   - PyTorchçš„è®¡ç®—æœºè§†è§‰æ‰©å±•åº“
   - åŒ…å«å¸¸ç”¨çš„æ•°æ®é›†ã€æ¨¡å‹æ¶æ„å’Œå›¾åƒå¤„ç†å·¥å…·

3. **torchaudio**ï¼š
   - PyTorchçš„éŸ³é¢‘å¤„ç†æ‰©å±•åº“
   - æä¾›éŸ³é¢‘åŠ è½½ã€å¤„ç†å’Œç‰¹å¾æå–åŠŸèƒ½

**åœ¨Apple Siliconä¸Š**ï¼Œæ­¤å‘½ä»¤ä¼šè‡ªåŠ¨å®‰è£…æ”¯æŒMPS (Metal Performance Shaders)çš„PyTorchç‰ˆæœ¬ï¼Œä½¿å¾—æ·±åº¦å­¦ä¹ ä»»åŠ¡èƒ½å¤Ÿåˆ©ç”¨Mç³»åˆ—èŠ¯ç‰‡çš„GPUåŠ é€Ÿã€‚

> âš ï¸ **é‡è¦è¯´æ˜**ï¼šæœ¬æŒ‡å—ç›®å‰ä½¿ç”¨æ ‡å‡†çš„Hugging Face PEFTåº“è¿›è¡ŒLoRAå¾®è°ƒï¼Œè€ŒéUnslothã€‚è¿™æ˜¯å› ä¸ºç»è¿‡æµ‹è¯•ï¼Œ**Unslothç›®å‰ä¸å®Œå…¨æ”¯æŒApple Siliconçš„MPSåç«¯**ã€‚

### 4. åŸå§‹æ–¹æ³•ï¼šUnslothå®‰è£…ï¼ˆå‚è€ƒï¼Œä¸é€‚ç”¨äºApple Siliconï¼‰

<details>
<summary>ğŸ‘‰ å±•å¼€æŸ¥çœ‹åŸå§‹Unslothå®‰è£…æ–¹æ³•ï¼ˆä»…ä¾›å‚è€ƒï¼Œä¸æ¨èä½¿ç”¨ï¼‰</summary>

```bash
# å…‹éš†Unsloth
git clone https://github.com/unslothai/unsloth.git
cd unsloth

# å®‰è£…Unslothï¼ˆApple Siliconä¼˜åŒ–ç‰ˆæœ¬ï¼‰
# æ³¨æ„ï¼šåœ¨zshä¸­éœ€è¦ç”¨å¼•å·é¿å…æ–¹æ‹¬å·è¢«è§£æä¸ºé€šé…ç¬¦
pip install ".[mps]"

# å¦‚æœå‡ºç° "zsh: no matches found: .[mps]" é”™è¯¯ï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ä¹‹ä¸€ï¼š
# pip install ".[mps]"    # ä½¿ç”¨å¼•å·
# pip install .\[mps\]    # ä½¿ç”¨è½¬ä¹‰ç¬¦
# pip install -e . --extra-index-url="mps"  # ä½¿ç”¨-eé€‰é¡¹
```

#### å…³äºUnslothå®‰è£…æ–¹å¼çš„è¯´æ˜

ä¸Šè¿°å®‰è£…æ–¹å¼ä½¿ç”¨äº†æºç å®‰è£…çš„æ–¹æ³•ï¼Œè€Œä¸æ˜¯ç›´æ¥é€šè¿‡pipå®‰è£…ã€‚è¿™æ ·åšçš„åŸå› æœ‰ï¼š

1. **è·å–æœ€æ–°MPSæ”¯æŒ**ï¼š
   - ç›´æ¥ä»GitHubè·å–æœ€æ–°çš„æºç ï¼ŒåŒ…å«å¯¹Apple Siliconçš„æœ€æ–°ä¼˜åŒ–
   - å¯ä»¥è®¿é—®å¯èƒ½å°šæœªåœ¨PyPIæ­£å¼å‘å¸ƒçš„åŠŸèƒ½

2. **å¯ç¼–è¾‘æ¨¡å¼**ï¼š
   - å®‰è£…åœ¨"å¯ç¼–è¾‘æ¨¡å¼"ï¼Œå…è®¸åœ¨éœ€è¦æ—¶ä¿®æ”¹æºç 
   - å¯¹äºè°ƒè¯•å’Œè‡ªå®šä¹‰å¾®è°ƒæµç¨‹å¾ˆæœ‰å¸®åŠ©

3. **ç‰ˆæœ¬çµæ´»æ€§**ï¼š
   - å¯ä»¥é€šè¿‡Gitåˆ‡æ¢åˆ°ç‰¹å®šç‰ˆæœ¬æˆ–åˆ†æ”¯

**æ›¿ä»£æ–¹æ¡ˆ**ï¼šæ‚¨ä¹Ÿå¯ä»¥ç›´æ¥é€šè¿‡pipå®‰è£…ï¼š
```bash
# ç›´æ¥ä»PyPIå®‰è£…ï¼ˆç®€å•ä½†å¯èƒ½ä¸æ˜¯æœ€æ–°ç‰ˆæœ¬ï¼‰
pip install unsloth[mps]
```

**âš ï¸ æç¤ºï¼š** ç»æµ‹è¯•ï¼Œè™½ç„¶Unslothå¯ä»¥åœ¨MacOSä¸Šå®‰è£…ï¼Œä½†å®é™…ä½¿ç”¨æ—¶ä¼šé‡åˆ°å…¼å®¹æ€§é—®é¢˜ï¼Œå› æ­¤æœ¬æŒ‡å—é‡‡ç”¨æ ‡å‡†PEFTåº“å®ç°LoRAå¾®è°ƒã€‚
</details>

### 5. å¸¸è§ç¯å¢ƒé—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

#### Pythonç‰ˆæœ¬å†²çªé—®é¢˜

å¦‚æœæ‚¨åœ¨å®‰è£…Python 3.11æ—¶é‡åˆ°ç±»ä¼¼ä»¥ä¸‹é”™è¯¯ï¼š

```
Error: The `brew link` step did not complete successfully
Could not symlink bin/2to3-3.11
Target /usr/local/bin/2to3-3.11 already exists.
```

è¿™è¡¨æ˜ç³»ç»Ÿä¸­å­˜åœ¨å¤šä¸ªPythonç‰ˆæœ¬å†²çªã€‚è§£å†³æ–¹æ³•ï¼š

1. **ä½¿ç”¨ç‰¹å®šç‰ˆæœ¬è·¯å¾„**ï¼š
   ```bash
   # ä½¿ç”¨Homebrewå®‰è£…çš„Python 3.11çš„å®Œæ•´è·¯å¾„åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
   /usr/local/Cellar/python@3.11/3.11.11/bin/python3.11 -m venv unsloth-env
   ```

2. **å¼ºåˆ¶åˆ›å»ºé“¾æ¥**ï¼ˆå¯é€‰ï¼‰ï¼š
   ```bash
   brew link --overwrite python@3.11
   ```

3. **éªŒè¯æ­£åœ¨ä½¿ç”¨çš„Pythonç‰ˆæœ¬**ï¼š
   ```bash
   python --version  # åº”æ˜¾ç¤º3.11.x
   ```

#### è™šæ‹Ÿç¯å¢ƒä½¿ç”¨é¡»çŸ¥

è¯·æ³¨æ„ï¼Œæ¿€æ´»è™šæ‹Ÿç¯å¢ƒï¼ˆ`source unsloth-env/bin/activate`ï¼‰**ä»…å¯¹å½“å‰ç»ˆç«¯ä¼šè¯æœ‰æ•ˆ**ï¼š

- å…³é—­ç»ˆç«¯çª—å£åï¼Œæ¿€æ´»çŠ¶æ€ä¼šä¸¢å¤±
- æ–°å¼€çš„ç»ˆç«¯çª—å£éœ€è¦é‡æ–°æ¿€æ´»ç¯å¢ƒ
- å¤šä¸ªç»ˆç«¯çª—å£éœ€è¦åˆ†åˆ«æ¿€æ´»

**å®ç”¨å»ºè®®**ï¼š

1. **åˆ›å»ºåˆ«å**ï¼šåœ¨`.zshrc`ä¸­æ·»åŠ ï¼š
   ```bash
   echo 'alias activate-unsloth="cd ~/Documents/unsloth-finetune && source unsloth-env/bin/activate"' >> ~/.zshrc
   ```

2. **åˆ›å»ºå¯åŠ¨è„šæœ¬**ï¼š
   ```bash
   echo '#!/bin/bash
   cd ~/Documents/unsloth-finetune
   source unsloth-env/bin/activate
   exec "$@"' > ~/start-unsloth.sh
   chmod +x ~/start-unsloth.sh
   ```
   ä½¿ç”¨ï¼š`~/start-unsloth.sh python your_script.py`

#### PyTorchä¸‹è½½è¶…æ—¶é—®é¢˜

å¦‚æœæ‚¨åœ¨å®‰è£…PyTorchæ—¶é‡åˆ°ç±»ä¼¼ä»¥ä¸‹é”™è¯¯:

```
HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.
```

è¿™é€šå¸¸æ˜¯å› ä¸ºPyTorchåŒ…ä½“ç§¯è¾ƒå¤§(çº¦150MB)ï¼Œåœ¨ç½‘ç»œçŠ¶å†µä¸ä½³æ—¶å®¹æ˜“ä¸‹è½½è¶…æ—¶ã€‚è§£å†³æ–¹æ³•:

1. **ä½¿ç”¨é•œåƒæºå®‰è£…**:
   ```bash
   pip install torch torchvision torchaudio -i https://pypi.tuna.tsinghua.edu.cn/simple
   ```

2. **å¢åŠ pipè¶…æ—¶æ—¶é—´**:
   ```bash
   pip --default-timeout=1000 install torch torchvision torchaudio
   ```

3. **åˆ†æ­¥ä¸‹è½½**:
   ```bash
   # å…ˆåªå®‰è£…torchæ ¸å¿ƒåº“
   pip install torch -i https://pypi.tuna.tsinghua.edu.cn/simple
   # æˆåŠŸåå†å®‰è£…å…¶ä»–ç»„ä»¶
   pip install torchvision torchaudio -i https://pypi.tuna.tsinghua.edu.cn/simple
   ```

4. **ç¡®è®¤Apple Siliconå…¼å®¹æ€§**:
   å¯¹äºMç³»åˆ—èŠ¯ç‰‡ï¼Œç¡®ä¿ä¸‹è½½çš„æ˜¯ARMæ¶æ„çš„åŒ…:
   ```bash
   pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu
   ```

## ä¸‰ã€ä¸‹è½½æ¨¡å‹ä¸æ•°æ®

### 1. è®¾ç½®HuggingFaceç¼“å­˜ç›®å½•

```bash
# åˆ›å»ºç¼“å­˜ç›®å½•
mkdir -p ~/Documents/models
mkdir -p ~/Documents/datasets

# è®¾ç½®ç¯å¢ƒå˜é‡
export HF_HOME=~/Documents/models
export TRANSFORMERS_CACHE=~/Documents/models
export HUGGINGFACE_HUB_CACHE=~/Documents/models
```

### 2. ä¸‹è½½æ¨¡å‹

#### å‡†å¤‡å·¥ä½œï¼šå®‰è£…å¿…è¦çš„åº“

```bash
# æ ¹æ®æ‚¨é€‰æ‹©çš„ä¸‹è½½æ–¹å¼ï¼Œå®‰è£…å¯¹åº”çš„åº“
# æ–¹å¼ä¸€ï¼šModelScopeï¼ˆéœ€è¦å®‰è£…modelscopeåº“ï¼‰
pip install modelscope

# æ–¹å¼äºŒï¼šHuggingFace APIï¼ˆéœ€è¦å®‰è£…huggingface_hubï¼‰
pip install huggingface_hub
```

> **âš ï¸ é‡è¦è­¦å‘Š**ï¼šåœ¨Pythonè„šæœ¬ä¸­ï¼Œ`~/` è¿™æ ·çš„è·¯å¾„ç®€å†™**ä¸ä¼š**è¢«è‡ªåŠ¨å±•å¼€ä¸ºç”¨æˆ·ä¸»ç›®å½•ï¼å¿…é¡»ä½¿ç”¨ç»å¯¹è·¯å¾„æˆ–`os.path.expanduser()`å‡½æ•°è¿›è¡Œè½¬æ¢ï¼Œå¦åˆ™ä¼šå¯¼è‡´è·¯å¾„é”™è¯¯ï¼

é€‰æ‹©ä»¥ä¸‹ä¸¤ç§æ–¹å¼ä¹‹ä¸€ï¼š

#### ä½¿ç”¨ModelScopeï¼ˆå¦‚æœéœ€è¦é•œåƒåŠ é€Ÿï¼‰
```python
# åˆ›å»ºå¹¶è¿è¡ŒPythonè„šæœ¬
cat > download_model.py << 'EOF'
import os
from modelscope import snapshot_download

# æ­£ç¡®: ä½¿ç”¨os.path.expanduserå±•å¼€ç”¨æˆ·ä¸»ç›®å½•
model_cache_dir = os.path.expanduser('~/Documents/models')

# ä¸‹è½½æ¨¡å‹åˆ°å±•å¼€åçš„è·¯å¾„
snapshot_download('deepseek-ai/DeepSeek-R1-Distill-Qwen-14B', cache_dir=model_cache_dir)

# é”™è¯¯ç¤ºä¾‹ - ä¸è¦è¿™æ ·åš: âŒ
# snapshot_download('deepseek-ai/DeepSeek-R1-Distill-Qwen-14B', cache_dir='~/Documents/models')
EOF

python download_model.py
```

#### æˆ–ä½¿ç”¨HuggingFace API
```bash
# è®¾ç½®é•œåƒï¼ˆå¯é€‰ï¼Œå¦‚æœä¸‹è½½æ…¢ï¼‰
export HF_ENDPOINT=https://hf-mirror.com

# ä¸‹è½½æ¨¡å‹ - ç”¨ç»å¯¹è·¯å¾„æˆ–os.path.expanduser()
python -c "import os; from huggingface_hub import snapshot_download; snapshot_download('deepseek-ai/DeepSeek-R1-Distill-Qwen-14B', cache_dir=os.path.expanduser('~/Documents/models'))"
```

### 3. ä¸‹è½½åŒ»å­¦æ•°æ®é›†

```bash
# è®¾ç½®é•œåƒï¼ˆå¯é€‰ï¼‰
export HF_ENDPOINT=https://hf-mirror.com

# ä¸‹è½½æ•°æ®é›†
huggingface-cli download --resume-download --repo-type dataset FreedomIntelligence/medical-o1-reasoning-SFT --local-dir ~/Documents/datasets/medical-o1-reasoning-SFT
```

### 4. éªŒè¯æ•°æ®é›†

```bash
# æŸ¥çœ‹æ•°æ®é›†çš„æœ€åä¸€æ¡è®°å½•
tail -6 ~/Documents/datasets/medical-o1-reasoning-SFT/medical_o1_sft_Chinese.json
```


## å››ã€å¾®è°ƒè„šæœ¬é…ç½®

**Unsloth ç›®å‰ä¸æ”¯æŒ Apple Silicon çš„ MPS åç«¯**ã€‚æˆ‘ä»¬éœ€è¦ä½¿ç”¨æ ‡å‡†çš„ Hugging Face PEFT (Parameter-Efficient Fine-Tuning) åº“æ¥è¿›è¡Œ LoRA å¾®è°ƒã€‚

> **âš ï¸æç¤º**ï¼šåœ¨Pythonè„šæœ¬ä¸­å¤„ç†æ–‡ä»¶è·¯å¾„åº”ä½¿ç”¨`os.path.expanduser()`å‡½æ•°å¤„ç†åŒ…å«`~`çš„è·¯å¾„ï¼Œå¦‚æœ¬ç¤ºä¾‹ä¸­æ‰€ç¤ºã€‚æ­¤ç±»è·¯å¾„é”™è¯¯æ˜¯å¯¼è‡´æ¨¡å‹åŠ è½½å¤±è´¥çš„æœ€å¸¸è§åŸå› ä¹‹ä¸€ã€‚


é¦–å…ˆï¼Œå®‰è£…å¿…è¦çš„åº“ï¼š

```bash
# å®‰è£…æ ¸å¿ƒåº“ 
pip install transformers datasets peft accelerate wandb

# æ³¨æ„ï¼šNumPyç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜ - ç¡®ä¿å®‰è£…1.xç³»åˆ—ç‰ˆæœ¬ä»¥é¿å…å…¼å®¹æ€§é”™è¯¯
pip install numpy==1.26.4
```

åˆ›å»ºå¾®è°ƒè„šæœ¬ï¼š

```bash
cd ~/Documents/unsloth-finetune
touch r1-lora-peft-mac.py
```

ç¼–è¾‘`r1-lora-peft-mac.py`ï¼Œå¡«å…¥ä»¥ä¸‹ä»£ç ï¼š

```python
import os
import json
import torch
import wandb
import gc  # ç”¨äºä¸»åŠ¨åƒåœ¾æ”¶é›†
from pathlib import Path

# ==================== æ ¸å¿ƒå†…å­˜ä¼˜åŒ–è®¾ç½® ====================
# è¿™äº›è®¾ç½®å¯¹è§£å†³æ¥è¿‘å°¾å£°æ—¶å´©æºƒçš„é—®é¢˜è‡³å…³é‡è¦
os.environ["TOKENIZERS_PARALLELISM"] = "false"   # é¿å…tokenizerå¹¶è¡Œå¯¼è‡´çš„å†…å­˜å‹åŠ›
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"  # å¯ç”¨å›é€€é€‰é¡¹ï¼Œé¿å…æœªå®ç°çš„æ“ä½œæŠ¥é”™
os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"  # å®Œå…¨ç¦ç”¨å†…å­˜ä¸Šé™
os.environ["PYTORCH_MPS_LOW_WATERMARK_RATIO"] = "0.0"   # å®Œå…¨ç¦ç”¨å†…å­˜ä¸‹é™

# ç¡¬ç¼–ç å¼€å…³ï¼Œè®¾ç½®ä¸ºTrueå¯è·³è¿‡åˆæ¬¡æ¨ç†
SKIP_INITIAL_INFERENCE = True #False

# å®šä¹‰å†…å­˜æ¸…ç†å‡½æ•°
def clean_memory():
    """æ¸…ç†GPUå’ŒCPUå†…å­˜ç¼“å­˜"""
    gc.collect()  # å¼ºåˆ¶Pythonåƒåœ¾æ”¶é›†
    if torch.backends.mps.is_available():
        torch.mps.empty_cache()  # æ¸…ç†MPSç¼“å­˜

# å¯é€‰ï¼šç™»å½•wandbè¿›è¡Œå®éªŒè·Ÿè¸ª
# wandb.login(key="ä½ çš„wandb.aiç½‘ç«™ä¸Šçš„token")
# åˆå§‹åŒ–wandbé¡¹ç›®
run = wandb.init(
    project='Lora-DeepSeek-R1-Distill-Qwen-14B-Mac',
    job_type="training",
    anonymous="allow"
)

####################################################################################################
# 1.åŠ è½½æ¨¡å‹

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model

# å†…å­˜ä¼˜åŒ–ä½†ä¸è¿‡åº¦é™ä½æ€§èƒ½çš„å¹³è¡¡å‚æ•°
max_seq_length = 2048  # æ¯”åŸå§‹çš„3072å°ï¼Œä½†ä»èƒ½ç»´æŒè¾ƒå¥½çš„ä¸Šä¸‹æ–‡çª—å£
dtype = torch.float32  # æ”¹ä¸ºå§‹ç»ˆä½¿ç”¨float32ä»¥é¿å…æ•°æ®ç±»å‹ä¸åŒ¹é…é—®é¢˜

# é‡è¦: ä½¿ç”¨os.path.expanduser()å±•å¼€è·¯å¾„ä¸­çš„~ç¬¦å·
model_path = os.path.expanduser("~/Documents/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B")
# ç¡®ä¿ç›®å½•å­˜åœ¨
if not os.path.exists(model_path):
    raise ValueError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}ï¼Œè¯·æ£€æŸ¥ä¸‹è½½æ˜¯å¦å®Œæˆæˆ–è·¯å¾„æ˜¯å¦æ­£ç¡®")

print(f"æ­£åœ¨ä»æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹: {model_path}")

# åŠ è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ - æ·»åŠ ä¸€äº›å…³é”®çš„å†…å­˜ä¼˜åŒ–é€‰é¡¹
print("å¼€å§‹åŠ è½½æ¨¡å‹ï¼Œè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...")
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float32,  # æ”¹ä¸ºä½¿ç”¨float32ä»¥é¿å…æ•°æ®ç±»å‹ä¸åŒ¹é…é—®é¢˜
    device_map="mps",  # ä½¿ç”¨Apple Siliconçš„Metalæ€§èƒ½ç€è‰²å™¨
    trust_remote_code=True,
    # å†…å­˜ä¼˜åŒ–é€‰é¡¹
    low_cpu_mem_usage=True,     # é™ä½CPUå†…å­˜ä½¿ç”¨
)

# åŠ è½½åæ¸…ç†å†…å­˜
clean_memory()
print(f"Model loaded: {model.__class__.__name__}")

####################################################################################################
# 2. å®šä¹‰æç¤ºæ¨¡æ¿ï¼Œå¹¶åœ¨å¾®è°ƒå‰åšä¸€æ¬¡æ¨ç†

prompt_style = """ä»¥ä¸‹æ˜¯æè¿°ä»»åŠ¡çš„æŒ‡ä»¤ï¼Œä»¥åŠæä¾›æ›´å¤šä¸Šä¸‹æ–‡çš„è¾“å…¥ã€‚
  è¯·å†™å‡ºæ°å½“å®Œæˆè¯¥è¯·æ±‚çš„å›ç­”ã€‚
  åœ¨å›ç­”ä¹‹å‰ï¼Œè¯·ä»”ç»†æ€è€ƒé—®é¢˜ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªé€æ­¥çš„æ€ç»´é“¾ï¼Œä»¥ç¡®ä¿å›ç­”åˆä¹é€»è¾‘ä¸”å‡†ç¡®ã€‚
  ### Instruction:
  ä½ æ˜¯ä¸€ä½åœ¨ä¸´åºŠæ¨ç†ã€è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’æ–¹é¢å…·æœ‰ä¸“ä¸šçŸ¥è¯†çš„åŒ»å­¦ä¸“å®¶ã€‚
  è¯·å›ç­”ä»¥ä¸‹åŒ»å­¦é—®é¢˜ã€‚
  ### Question:
  {}
  ### Response:
  <think>{}"""
train_prompt_style = prompt_style + """
  </think>
  {}"""

# æµ‹è¯•ç”¨åŒ»å­¦é—®é¢˜
question = "ä¸€å70å²çš„ç”·æ€§æ‚£è€…å› èƒ¸ç—›ä¼´å‘•å16å°æ—¶å°±åŒ»ï¼Œå¿ƒç”µå›¾æ˜¾ç¤ºä¸‹å£å¯¼è”å’Œå³èƒ¸å¯¼è”STæ®µæŠ¬é«˜0.1~0.3mVï¼Œç»è¡¥æ¶²åè¡€å‹é™è‡³80/60mmHgï¼Œæ‚£è€…å‡ºç°å‘¼å¸å›°éš¾å’Œä¸èƒ½å¹³å§çš„ç—‡çŠ¶ï¼Œä½“æ£€å‘ç°åŒè‚ºæœ‰å¤§é‡æ°´æ³¡éŸ³ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæœ€æ°å½“çš„è¯ç‰©å¤„ç†æ˜¯ä»€ä¹ˆï¼Ÿ"

# æ™ºèƒ½æ¨ç†å‡½æ•° - åªåœ¨å¿…è¦æ—¶å›é€€åˆ°CPU
def smart_inference(model, inputs, max_new_tokens=1200, temperature=0.7):
    """æ™ºèƒ½æ¨ç†å‡½æ•° - åªæœ‰åœ¨ç‰¹å®šæ“ä½œå¤±è´¥æ—¶å›é€€åˆ°CPU"""
    try:
        # å°è¯•ç›´æ¥ä½¿ç”¨MPSè¿›è¡Œæ¨ç†
        print("ä½¿ç”¨MPSè¿›è¡Œæ¨ç†...")
        # ç¦ç”¨gradient_checkpointingä»¥ä½¿ç”¨KVç¼“å­˜
        was_gradient_checkpointing = model.is_gradient_checkpointing
        if was_gradient_checkpointing:
            model.gradient_checkpointing_disable()
            
        # ç¡®ä¿æ‰€æœ‰è¾“å…¥å¼ é‡çš„æ•°æ®ç±»å‹ä¸€è‡´
        for k, v in inputs.items():
            if isinstance(v, torch.Tensor) and v.dtype == torch.float16:
                inputs[k] = v.to(dtype=torch.float32)
                
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,  # å‡å°‘ç”Ÿæˆçš„tokenæ•°é‡
            temperature=temperature,
            do_sample=True,
            use_cache=True,  # å¯ç”¨KVç¼“å­˜
        )
        
        # æ¢å¤gradient_checkpointingçŠ¶æ€
        if was_gradient_checkpointing:
            model.gradient_checkpointing_enable()
            
        return outputs
    except RuntimeError as e:
        error_msg = str(e)
        if "MPS backend out of memory" in error_msg:
            print(f"é‡åˆ°MPSå†…å­˜é”™è¯¯: {error_msg}")
            print("æ¸…ç†å†…å­˜å¹¶å°è¯•æ™ºèƒ½å›é€€ç­–ç•¥...")
            
            # æ¸…ç†å†…å­˜
            clean_memory()
            
            try:
                # å°è¯•ä½¿ç”¨æ›´å°çš„max_new_tokenså’Œæ›´ä½çš„æ¸©åº¦
                print("å°è¯•ä½¿ç”¨æ›´ä¿å®ˆçš„ç”Ÿæˆå‚æ•°...")
                # ç¦ç”¨gradient_checkpointingä»¥ä½¿ç”¨KVç¼“å­˜
                was_gradient_checkpointing = model.is_gradient_checkpointing
                if was_gradient_checkpointing:
                    model.gradient_checkpointing_disable()
                    
                # ç¡®ä¿æ‰€æœ‰è¾“å…¥å¼ é‡çš„æ•°æ®ç±»å‹ä¸€è‡´
                for k, v in inputs.items():
                    if isinstance(v, torch.Tensor) and v.dtype == torch.float16:
                        inputs[k] = v.to(dtype=torch.float32)
                        
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens // 2,  # å‡åŠ
                    temperature=0.1,  # æ›´ç¡®å®šæ€§çš„ç”Ÿæˆ
                    do_sample=False,  # ç¦ç”¨é‡‡æ ·
                    use_cache=True,
                )
                
                # æ¢å¤gradient_checkpointingçŠ¶æ€
                if was_gradient_checkpointing:
                    model.gradient_checkpointing_enable()
                    
                return outputs
            except RuntimeError as e2:
                print(f"ç¬¬äºŒæ¬¡å°è¯•ä¹Ÿå¤±è´¥: {str(e2)}")
                # å¦‚æœä»ç„¶å¤±è´¥ï¼Œå°†æ¨ç†ç§»è‡³CPU
                print("MPSæ¨ç†ä»ç„¶å¤±è´¥ï¼Œå›é€€åˆ°CPU...")
                model.to("cpu")
                cpu_inputs = {k: v.to("cpu") for k, v in inputs.items() if hasattr(v, "to")}
                
                with torch.no_grad():
                    outputs = model.generate(
                        **cpu_inputs,
                        max_new_tokens=max_new_tokens,
                        temperature=temperature,
                        do_sample=True,
                        use_cache=True,  # CPUä¸Šå¯ä»¥å®‰å…¨ä½¿ç”¨
                    )
                
                # æ¨ç†åç§»å›MPS
                model.to("mps")
                return outputs
        elif "does not match" in error_msg or "requires the same element type" in error_msg:
            # å¤„ç†æ•°æ®ç±»å‹ä¸åŒ¹é…é”™è¯¯
            print(f"é‡åˆ°æ•°æ®ç±»å‹ä¸åŒ¹é…é”™è¯¯: {error_msg}")
            print("å°è¯•ç»Ÿä¸€æ•°æ®ç±»å‹ä¸ºfloat32...")
            
            # ç¡®ä¿æ‰€æœ‰è¾“å…¥å¼ é‡çš„æ•°æ®ç±»å‹ä¸€è‡´ä¸ºfloat32
            for k, v in inputs.items():
                if isinstance(v, torch.Tensor):
                    inputs[k] = v.to(dtype=torch.float32)
            
            # ç¦ç”¨gradient_checkpointingä»¥ä½¿ç”¨KVç¼“å­˜
            was_gradient_checkpointing = model.is_gradient_checkpointing
            if was_gradient_checkpointing:
                model.gradient_checkpointing_disable()
                
            try:
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    do_sample=True,
                    use_cache=True,
                )
                
                # æ¢å¤gradient_checkpointingçŠ¶æ€
                if was_gradient_checkpointing:
                    model.gradient_checkpointing_enable()
                    
                return outputs
            except RuntimeError:
                # å¦‚æœä»ç„¶å¤±è´¥ï¼Œå°†æ¨ç†ç§»è‡³CPU
                print("MPSæ¨ç†ä»ç„¶å¤±è´¥ï¼Œå›é€€åˆ°CPU...")
                model.to("cpu")
                cpu_inputs = {k: v.to("cpu") for k, v in inputs.items() if hasattr(v, "to")}
                
                with torch.no_grad():
                    outputs = model.generate(
                        **cpu_inputs,
                        max_new_tokens=max_new_tokens,
                        temperature=temperature,
                        do_sample=True,
                        use_cache=True,
                    )
                
                # æ¨ç†åç§»å›MPS
                model.to("mps")
                return outputs
        else:
            # å¦‚æœä¸æ˜¯å·²çŸ¥é”™è¯¯ï¼Œé‡æ–°æŠ›å‡º
            raise

# æ‰§è¡Œåˆæ¬¡æ¨ç†
model.eval()
if not SKIP_INITIAL_INFERENCE:
    # åªæœ‰å½“SKIP_INITIAL_INFERENCE=Falseæ—¶æ‰æ‰§è¡Œæ¨ç†
    with torch.no_grad():
        # å‡†å¤‡è¾“å…¥
        model_input = prompt_style.format(question, "")
        inputs = tokenizer(model_input, return_tensors="pt").to("mps")
        
        # ä½¿ç”¨æ™ºèƒ½æ¨ç†
        outputs = smart_inference(model, inputs, max_new_tokens=1200, temperature=0.7)
        
        # è§£ç è¾“å‡º
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print("### å¾®è°ƒå‰æ¨¡å‹æ¨ç†ç»“æœï¼š")
        print(response.split("### Response:")[1])
else:
    print("å·²è·³è¿‡åˆæ¬¡æ¨ç†ï¼Œç›´æ¥å¼€å§‹è®­ç»ƒæµç¨‹...")

# æ¸…ç†å†…å­˜
clean_memory()

####################################################################################################
# 3. åº”ç”¨LoRAè¿›è¡Œå¾®è°ƒå‡†å¤‡

# é…ç½®LoRA - ä¿æŒåŸå§‹çš„target_modulesä½†ç•¥å¾®å‡å°rå€¼
target_modules = [
    "q_proj", "k_proj", "v_proj", "o_proj", 
    "gate_proj", "up_proj", "down_proj",
]

# åˆ›å»ºLoRAé…ç½® - ä½¿ç”¨ç¨å°çš„rå€¼ä»¥èŠ‚çœå†…å­˜
lora_config = LoraConfig(
    r=16,                      # ä»24å‡å°åˆ°16ï¼Œæ›´å¥½åœ°å¹³è¡¡å†…å­˜ä½¿ç”¨å’Œæ€§èƒ½
    lora_alpha=16,             # åŸå§‹alphaå€¼
    target_modules=target_modules,
    lora_dropout=0,            # åŸå§‹dropout
    bias="none",               # åŸå§‹biasè®¾ç½®
    task_type="CAUSAL_LM",     # ä»»åŠ¡ç±»å‹ä¸å˜
    inference_mode=False,      # è®­ç»ƒæ¨¡å¼
)

# å®‰å…¨çš„å‚æ•°è½¬æ¢å‡½æ•° - ä½¿ç”¨ç»Ÿä¸€çš„æ•°æ®ç±»å‹
def safe_prepare_model_for_kbit_training(model):
    """å®‰å…¨åœ°å‡†å¤‡æ¨¡å‹è¿›è¡Œé‡åŒ–è®­ç»ƒï¼Œæ‰¹é‡å¤„ç†å‚æ•°ä»¥é¿å…å†…å­˜å³°å€¼"""
    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥å‡å°‘å†…å­˜ä½¿ç”¨
    model.gradient_checkpointing_enable()
    # ç¡®ä¿å‚æ•°ç¼“å­˜æ˜¯ç¦ç”¨çš„ï¼Œä»¥é¿å…ä¸æ¢¯åº¦æ£€æŸ¥ç‚¹å†²çª
    model.config.use_cache = False
    
    print("åˆ†æ‰¹å¤„ç†å‚æ•°è½¬æ¢ï¼Œé¿å…å†…å­˜å³°å€¼...")
    
    # é¦–å…ˆå¤„ç†æ³¨æ„åŠ›å±‚
    print("å¤„ç†æ³¨æ„åŠ›å±‚å‚æ•°...")
    attention_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]
    for name, module in model.named_modules():
        if any(target in name for target in attention_modules):
            for param in module.parameters():
                if param.requires_grad:
                    # ç¡®ä¿ç»Ÿä¸€ä½¿ç”¨float32ç±»å‹
                    param.data = param.data.to(torch.float32)
            # æ¯ä¸ªæ¨¡å—åæ¸…ç†å†…å­˜
            clean_memory()
    
    # å¤„ç†å‰é¦ˆå±‚
    print("å¤„ç†å‰é¦ˆå±‚å‚æ•°...")
    feedforward_modules = ["gate_proj", "up_proj", "down_proj"]
    for name, module in model.named_modules():
        if any(target in name for target in feedforward_modules):
            for param in module.parameters():
                if param.requires_grad:
                    # ç¡®ä¿ç»Ÿä¸€ä½¿ç”¨float32ç±»å‹
                    param.data = param.data.to(torch.float32)
            # æ¯ä¸ªæ¨¡å—åæ¸…ç†å†…å­˜
            clean_memory()
    
    print("å‚æ•°è½¬æ¢å®Œæˆï¼")
    return model

# ä½¿ç”¨å®‰å…¨çš„å‚æ•°è½¬æ¢å‡½æ•°
try:
    model = safe_prepare_model_for_kbit_training(model)
except Exception as e:
    print(f"å‚æ•°è½¬æ¢å‡ºé”™: {e}")
    print("å°è¯•ä½¿ç”¨æ ‡å‡†æ–¹æ³•...")
    # å†æ¬¡æ¸…ç†å†…å­˜
    clean_memory()
    # å°è¯•æ ‡å‡†æ–¹æ³•
    model = prepare_model_for_kbit_training(model)

# åº”ç”¨LoRAé…ç½®
model = get_peft_model(model, lora_config)
print(f"Trainable params: {model.print_trainable_parameters()}")

# æ¸…ç†å†…å­˜
clean_memory()

####################################################################################################
# 4. å¤„ç†æ•°æ®é›†

EOS_TOKEN = tokenizer.eos_token

# æ ¼å¼åŒ–æç¤ºå‡½æ•°ï¼Œç”¨äºå¤„ç†æ•°æ®é›†ä¸­çš„ç¤ºä¾‹
def formatting_prompts_func(examples):
    # ä»examplesä¸­æå–é—®é¢˜ã€æ€ç»´é“¾å’Œå›ç­”
    inputs = examples["Question"]      # åŒ»å­¦é—®é¢˜åˆ—è¡¨
    cots = examples["Complex_CoT"]     # æ€ç»´é“¾åˆ—è¡¨
    outputs = examples["Response"]     # å›ç­”åˆ—è¡¨
    
    # å­˜å‚¨æ ¼å¼åŒ–åçš„æ–‡æœ¬
    texts = []
    
    # éå†æ¯ä¸ªç¤ºä¾‹ï¼Œå°†é—®é¢˜ã€æ€ç»´é“¾å’Œå›ç­”ç»„åˆæˆæŒ‡å®šæ ¼å¼
    for input, cot, output in zip(inputs, cots, outputs):
        # ä½¿ç”¨train_prompt_styleæ¨¡æ¿æ ¼å¼åŒ–æ–‡æœ¬ï¼Œå¹¶æ·»åŠ ç»“æŸç¬¦
        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN
        texts.append(text)
        
    # è¿”å›æ ¼å¼åŒ–åçš„æ–‡æœ¬å­—å…¸
    return {
        "text": texts,
    }

# åŠ è½½æ•°æ®é›†å¹¶åº”ç”¨æ ¼å¼åŒ–
from datasets import load_dataset
# é‡è¦: ä½¿ç”¨os.path.expanduser()å±•å¼€è·¯å¾„ä¸­çš„~ç¬¦å·
data_path = os.path.expanduser("~/Documents/datasets/medical-o1-reasoning-SFT/medical_o1_sft_Chinese.json")
if not os.path.exists(data_path):
    raise ValueError(f"æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {data_path}ï¼Œè¯·æ£€æŸ¥ä¸‹è½½æ˜¯å¦å®Œæˆæˆ–è·¯å¾„æ˜¯å¦æ­£ç¡®")

# ä½¿ç”¨åˆç†æ•°é‡çš„æ ·æœ¬ - åˆå§‹éªŒè¯æ—¶ä½¿ç”¨è¾ƒå°‘æ ·æœ¬
dataset = load_dataset(
    "json",  # æŒ‡å®šæ•°æ®æ ¼å¼ä¸ºJSON
    data_files=data_path,
    split="train[0:200]",  # ä½¿ç”¨å‰200æ¡æ•°æ®ç”¨äºåˆå§‹éªŒè¯
    trust_remote_code=True  # å…¼å®¹remote codeçš„è¡Œä¸º
)

# å¦‚æœè¿”å›çš„æ˜¯DatasetDictï¼Œåˆ™å–å‡º"train"è¿™ä¸€éƒ¨åˆ†
if isinstance(dataset, dict):  
    dataset = dataset["train"]
    
# åˆ†æ‰¹å¤„ç†ä»¥é¿å…å†…å­˜å³°å€¼
dataset = dataset.map(
    formatting_prompts_func, 
    batched=True,
    batch_size=10,  # åˆç†çš„æ‰¹å¤„ç†å¤§å°
    num_proc=1,     # å•è¿›ç¨‹ä»¥é¿å…é¢å¤–å†…å­˜å¼€é”€
)
print(f"Dataset loaded: {len(dataset)} examples")

# æ¸…ç†å†…å­˜
clean_memory()

####################################################################################################
# 5. é…ç½®è®­ç»ƒå‚æ•°å¯åŠ¨è®­ç»ƒ

from transformers import TrainingArguments, Trainer
import transformers

# å®šä¹‰æ•°æ®æ•´ç†å‡½æ•° - é’ˆå¯¹MPSä¼˜åŒ–
def data_collator(features):
    """ç¡®ä¿è¾“å‡ºçš„å¼ é‡ç±»å‹ä¸€è‡´ä¸ºfloat32å¹¶æ·»åŠ å¿…è¦çš„labels"""
    texts = [f["text"] for f in features]
    batch = tokenizer(
        texts, 
        padding="longest",  # åªå¡«å……åˆ°æœ€é•¿åºåˆ—é•¿åº¦ï¼Œè€Œä¸æ˜¯max_length
        truncation=True, 
        max_length=max_seq_length, 
        return_tensors="pt"
    )
    
    # åˆ›å»ºæ ‡ç­¾å¼ é‡ï¼šå¯¹äºå› æœè¯­è¨€æ¨¡å‹ï¼Œæ ‡ç­¾é€šå¸¸ä¸è¾“å…¥IDç›¸åŒ
    # ä½†æ³¨æ„ï¼šæˆ‘ä»¬è¦å¤åˆ¶input_idsä½œä¸ºlabels
    batch["labels"] = batch["input_ids"].clone()
    
    # ç¡®ä¿æ‰€æœ‰å¼ é‡ä¸ºfloat32ç±»å‹
    for key, value in batch.items():
        if isinstance(value, torch.Tensor) and value.dtype == torch.float16:
            batch[key] = value.to(dtype=torch.float32)
    
    return batch

# è®­ç»ƒå‚æ•° - æ ¹æ®ä¸åŒå†…å­˜å¤§å°è°ƒæ•´
# 32GBå†…å­˜: batch_size=1, gradient_steps=8
# 64GBå†…å­˜: batch_size=1, gradient_steps=4
# 96GB+å†…å­˜: batch_size=2, gradient_steps=4
training_args = TrainingArguments(
    output_dir="outputs",
    per_device_train_batch_size=1,    # é™ä½æ‰¹æ¬¡å¤§å°ä»¥å‡å°‘å†…å­˜ä½¿ç”¨
    gradient_accumulation_steps=8,    # å¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ä»¥ä¿æŒæœ‰æ•ˆæ‰¹å¤§å°
    learning_rate=2e-4,               # å­¦ä¹ ç‡
    lr_scheduler_type="linear",       # çº¿æ€§å­¦ä¹ ç‡è°ƒåº¦å™¨
    warmup_steps=5,                   # é¢„çƒ­æ­¥æ•°
    max_steps=20,                     # åˆå§‹éªŒè¯åªéœ€å°‘é‡æ­¥éª¤
    logging_steps=1,                  # æ¯æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—ä»¥ä¾¿äºéªŒè¯
    save_steps=10,                    # æ¯10æ­¥ä¿å­˜ä¸€æ¬¡
    fp16=False,                       # ä¸ä½¿ç”¨åŠç²¾åº¦ï¼ŒMPSä¸æ”¯æŒ
    bf16=False,                       # åŒæ ·ä¸ä½¿ç”¨bf16
    remove_unused_columns=False,      # é¿å…æ•°æ®é›†åˆ—ä¸åŒ¹é…é”™è¯¯
    gradient_checkpointing=True,      # æ¢¯åº¦æ£€æŸ¥ç‚¹
    weight_decay=0.01,                # æƒé‡è¡°å‡
    seed=8137,                        # éšæœºæ•°ç§å­
    # å†…å­˜ä¼˜åŒ–å‚æ•°
    dataloader_num_workers=0,         # ä¸ä½¿ç”¨å¤šè¿›ç¨‹æ•°æ®åŠ è½½
    dataloader_pin_memory=False,      # ä¸ä½¿ç”¨å›ºå®šå†…å­˜
    report_to="none" if not wandb.run else "wandb",  # æ ¹æ®wandbæ˜¯å¦å¯ç”¨å†³å®šæŠ¥å‘Š
    run_name="medical-o1-sft-experiment-mac",  # wandbè¿è¡Œåç§°
)

# åˆ›å»ºè®­ç»ƒå™¨
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    data_collator=data_collator,
)

# åœ¨å¼€å§‹è®­ç»ƒå‰ç¡®ä¿æ¨¡å‹é…ç½®æ­£ç¡®
model.config.use_cache = False  # ç¡®ä¿ç¦ç”¨ç¼“å­˜ï¼Œä¸æ¢¯åº¦æ£€æŸ¥ç‚¹å…¼å®¹

# å¼€å§‹è®­ç»ƒ
print(f"å¼€å§‹è®­ç»ƒ: {training_args.max_steps} æ­¥ï¼Œæ‰¹æ¬¡å¤§å°: {training_args.per_device_train_batch_size}ï¼Œæ¢¯åº¦ç´¯ç§¯: {training_args.gradient_accumulation_steps}")

# æ·»åŠ é¢å¤–çš„è®­ç»ƒå‰å‡†å¤‡
def prepare_model_for_mps_training(model):
    """ç‰¹æ®Šå¤„ç†ä»¥è§£å†³MPSåç«¯æ•°æ®ç±»å‹ä¸åŒ¹é…é—®é¢˜"""
    print("æ­£åœ¨ä¸ºMPSè®­ç»ƒå‡†å¤‡æ¨¡å‹...")
    
    # ç¡®ä¿æ‰€æœ‰attentionå±‚å’Œfeedforwardå±‚çš„å‚æ•°ä½¿ç”¨ç›¸åŒçš„æ•°æ®ç±»å‹
    # è¿™æ˜¯ä¸ºäº†è§£å†³ç‰¹å®šçš„tensor<1x688x5120xf16>, tensor<5120xf32>æ•°æ®ç±»å‹ä¸åŒ¹é…é—®é¢˜
    for name, module in model.named_modules():
        if any(x in name for x in ["attention", "mlp", "down_proj", "up_proj", "gate_proj"]):
            # æ£€æŸ¥æ˜¯å¦æœ‰æƒé‡å’Œåç½®ä½¿ç”¨ä¸åŒçš„æ•°æ®ç±»å‹
            for child_name, child in module.named_children():
                if hasattr(child, "weight") and hasattr(child, "bias") and child.bias is not None:
                    # ç¡®ä¿æƒé‡å’Œåç½®ä½¿ç”¨ç›¸åŒçš„æ•°æ®ç±»å‹
                    if child.weight.dtype != child.bias.dtype:
                        print(f"ä¿®å¤æ•°æ®ç±»å‹ä¸åŒ¹é…: {name}.{child_name}, æƒé‡={child.weight.dtype}, åç½®={child.bias.dtype}")
                        child.bias.data = child.bias.data.to(dtype=child.weight.dtype)
    
    # ç¦ç”¨KVç¼“å­˜ï¼Œä»¥é˜²æ­¢ä¸æ¢¯åº¦æ£€æŸ¥ç‚¹å†²çª
    model.config.use_cache = False
    
    print("MPSè®­ç»ƒå‡†å¤‡å®Œæˆã€‚")
    return model

# åº”ç”¨é¢å¤–çš„MPSè®­ç»ƒå‡†å¤‡
model = prepare_model_for_mps_training(model)

try:
    trainer.train()
    print(f"è®­ç»ƒå®Œæˆã€‚æ­¥æ•°: {trainer.state.global_step}")
except RuntimeError as e:
    error_msg = str(e)
    if "MPS backend out of memory" in error_msg:
        print("\nè®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°å†…å­˜ä¸è¶³ã€‚è¯·å°è¯•ä»¥ä¸‹è§£å†³æ–¹æ¡ˆ:")
        print("1. å‡å°max_seq_lengthå€¼è‡³1536æˆ–1024")
        print("2. å‡å°LoRAé…ç½®ä¸­çš„rå€¼è‡³8")
        print("3. å‡å°‘target_modulesçš„æ•°é‡ï¼Œä»…ä¿ç•™[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]")
        print("4. å¢åŠ gradient_accumulation_stepsè‡³16")
    elif "requires the same element type" in error_msg or "does not match" in error_msg:
        print("\nè®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°æ•°æ®ç±»å‹ä¸åŒ¹é…é”™è¯¯:")
        print(error_msg)
        print("\nè¯·å°è¯•ä»¥ä¸‹è§£å†³æ–¹æ¡ˆ:")
        print("1. ç¡®ä¿æ‰€æœ‰æ¨¡å‹å‚æ•°ç»Ÿä¸€ä½¿ç”¨torch.float32ç±»å‹")
        print("2. å‡å°‘LoRAé…ç½®ä¸­çš„target_modulesï¼Œä»…ä¿ç•™attentionæ¨¡å—")
        print("3. å°è¯•è®¾ç½®ç¯å¢ƒå˜é‡ï¼šexport PYTORCH_ENABLE_MPS_FALLBACK=1")
    else:
        raise e

# æ¸…ç†å†…å­˜
clean_memory()

####################################################################################################
# 6. å¾®è°ƒåçš„æ¨¡å‹åšä¸€æ¬¡æ¨ç†

# è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
model.eval()

with torch.no_grad():
    # å‡†å¤‡è¾“å…¥
    model_input = prompt_style.format(question, "")
    inputs = tokenizer(model_input, return_tensors="pt").to("mps")
    
    # ä½¿ç”¨æ™ºèƒ½æ¨ç†
    outputs = smart_inference(model, inputs, max_new_tokens=1200, temperature=0.7)
    
    # è§£ç è¾“å‡º
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print("### å¾®è°ƒåæ¨¡å‹æ¨ç†ç»“æœï¼š")
    print(response.split("### Response:")[1])

####################################################################################################
# 7. ä¿å­˜æ¨¡å‹

# æ¸…ç†å†…å­˜
clean_memory()

# ä¿å­˜æ¨¡å‹
model_save_path = "DeepSeek-R1-Medical-COT-Qwen-14B-Mac"
model.save_pretrained(model_save_path)
tokenizer.save_pretrained(model_save_path)

print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {os.path.abspath(model_save_path)}")
print("éªŒè¯å®Œæˆåï¼Œå¯ä»¥ç§»é™¤max_stepsé™åˆ¶å¹¶ä½¿ç”¨å®Œæ•´æ•°æ®é›†è¿›è¡Œè®­ç»ƒ")
```

## äº”ã€æ‰§è¡Œå¾®è°ƒè¿‡ç¨‹

```bash
# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒï¼ˆå¦‚æœå°šæœªæ¿€æ´»ï¼‰
cd ~/Documents/llm-finetune
source peft-env/bin/activate

# å®‰è£…NumPy 1.xç‰ˆæœ¬è§£å†³å…¼å®¹æ€§é—®é¢˜
pip install numpy==1.26.4

# å®‰è£…å†…å­˜ç›‘æ§å·¥å…·(å¯é€‰)
pip install psutil

# è¿è¡Œå¾®è°ƒè„šæœ¬
python r1-lora-peft-mac.py
```

## å…­ã€æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. å†…å­˜é…ç½®æŒ‡å—

| Macå†…å­˜é…ç½® | max_seq_length | batch_size | gradient_steps | LoRA rå€¼ | target_modules |
|------------|----------------|------------|----------------|---------|----------------|
| 32GB | 1024-1536 | 1 | 8-16 | 8 | ä»…attention (q,k,v,o) |
| 64GB | 2048 | 1 | 4-8 | 16 | æ‰€æœ‰7ä¸ªæ¨¡å— |
| 96GB+ | 2048-3072 | 1-2 | 4 | 32 | æ‰€æœ‰7ä¸ªæ¨¡å— |

### 2. è§£å†³å†…å­˜æº¢å‡ºé—®é¢˜

å¦‚æœé‡åˆ°`MPS backend out of memory`é”™è¯¯ï¼Œè¯·å°è¯•ä»¥ä¸‹æ­¥éª¤ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰ï¼š

1. **å‡å°‘åºåˆ—é•¿åº¦**ï¼š
   - å°†`max_seq_length`é™è‡³1024ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
   - è¿™æ˜¯æœ€æœ‰æ•ˆçš„å†…å­˜èŠ‚çœæ–¹å¼

2. **é™ä½LoRAå‚æ•°**ï¼š
   - å°†LoRAçš„`r`å€¼ä»32é™è‡³16æˆ–8
   - å‡å°‘`target_modules`ï¼Œä»…ä¿ç•™`["q_proj", "k_proj", "v_proj", "o_proj"]`

3. **è°ƒæ•´æ‰¹å¤„ç†è®¾ç½®**ï¼š
   - ç¡®ä¿`per_device_train_batch_size=1`
   - å¢åŠ `gradient_accumulation_steps`å€¼ï¼ˆ8æˆ–æ›´é«˜ï¼‰

4. **æ·»åŠ å†…å­˜ä¼˜åŒ–ç¯å¢ƒå˜é‡**ï¼š
   ```bash
   export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0
   export PYTORCH_ENABLE_MPS_FALLBACK=1
   ```

5. **å‡å°‘è®­ç»ƒæ•°æ®**ï¼š
   - ä½¿ç”¨`split="train[0:100]"`ç­‰ç­–ç•¥é™åˆ¶æ ·æœ¬æ•°é‡
   - åœ¨ä»£ç ä¸­é¢‘ç¹è°ƒç”¨`gc.collect()`å’Œ`torch.mps.empty_cache()`

### 3. MPSå…¼å®¹æ€§é—®é¢˜è§£å†³æ–¹æ¡ˆ

Qwen2å’ŒDeepSeek-R1-Distill-Qwenç³»åˆ—æ¨¡å‹åœ¨MPSä¸Šå¯èƒ½é‡åˆ°ä»¥ä¸‹å…¼å®¹æ€§é—®é¢˜ï¼š

1. **å¸¸è§é”™è¯¯ç±»å‹**ï¼š
   - `validateComputeFunctionArguments` ç›¸å…³é”™è¯¯
   - `gather_kernel_1: missing buffer binding at index 2` é”™è¯¯
   - **æ•°æ®ç±»å‹ä¸åŒ¹é…é”™è¯¯**ï¼šä¾‹å¦‚ `'mps.add' op requires the same element type for all operands and results`
   - **use_cacheä¸gradient_checkpointingå†²çª**ï¼š`use_cache=True` ä¸ `gradient_checkpointing=True` ä¸å…¼å®¹
   - å…¶ä»–MPSæ“ä½œæœªå®ç°çš„é”™è¯¯
   - **fp16æ··åˆç²¾åº¦è®­ç»ƒä¸å…¼å®¹**ï¼š`ValueError: fp16 mixed precision requires a GPU (not 'mps')`

2. **è§£å†³ç­–ç•¥**ï¼š
   - **ç»Ÿä¸€æ•°æ®ç±»å‹**ï¼šç¡®ä¿æ‰€æœ‰å¼ é‡éƒ½ä½¿ç”¨ç›¸åŒçš„æ•°æ®ç±»å‹ï¼ˆä½¿ç”¨float32è€Œéfloat16ï¼‰
   - **æ£€æŸ¥æƒé‡å’Œåç½®ç±»å‹åŒ¹é…**ï¼šç¡®ä¿æ¯ä¸ªå±‚çš„æƒé‡å’Œåç½®ä½¿ç”¨ç›¸åŒçš„æ•°æ®ç±»å‹
   - **æ˜¾å¼ç¦ç”¨ç¼“å­˜**ï¼šè®¾ç½®`model.config.use_cache = False`ä»¥é¿å…ä¸æ¢¯åº¦æ£€æŸ¥ç‚¹å†²çª
   - **æ™ºèƒ½ç¦ç”¨gradient_checkpointing**ï¼šåœ¨æ¨ç†å‰ä¸´æ—¶ç¦ç”¨gradient_checkpointingä»¥ä½¿ç”¨KVç¼“å­˜
   - **æ¨ç†å›é€€åˆ°CPU**ï¼šå½“MPSå‡ºç°ä¸å¯æ¢å¤çš„é”™è¯¯æ—¶è‡ªåŠ¨å›é€€åˆ°CPUå¤„ç†
   - **ç¦ç”¨fp16è®­ç»ƒ**ï¼šç¡®ä¿å°†`fp16=False`è®¾ç½®åœ¨`TrainingArguments`ä¸­
   - **æ·»åŠ ç¯å¢ƒå˜é‡**ï¼šä½¿ç”¨`PYTORCH_ENABLE_MPS_FALLBACK=1`å…è®¸æœªå®ç°çš„æ“ä½œå›é€€åˆ°CPU

3. **æ€§èƒ½å½±å“**ï¼š
   - æ¨ç†åœ¨CPUä¸Šé€Ÿåº¦ä¼šæ…¢ä¸€äº›ï¼Œä½†ç»“æœå‡†ç¡®
   - è®­ç»ƒè¿‡ç¨‹ä»èƒ½å……åˆ†åˆ©ç”¨MPSåŠ é€Ÿ
   - åˆç†é…ç½®å‚æ•°å¯ä»¥å‡å°‘æ•°æ®ç±»å‹è½¬æ¢çš„æ€§èƒ½æŸå¤±

### 4. è®­ç»ƒè¿‡ç¨‹ä¸­çš„MPSæ•°æ®ç±»å‹ä¸åŒ¹é…é”™è¯¯ä¿®å¤

å¦‚æœæ‚¨é‡åˆ°ç±»ä¼¼ä»¥ä¸‹é”™è¯¯ï¼š
```
'mps.add' op requires the same element type for all operands and results
%7 = "mps.add"(%6, %arg2) : (tensor<1x688x5120xf16>, tensor<5120xf32>) -> tensor<*xf32>
```

è¿™æ˜¯ç”±äºMPSåç«¯è¦æ±‚æ‰€æœ‰æ“ä½œæ•°å…·æœ‰ç›¸åŒçš„æ•°æ®ç±»å‹ï¼Œä½†æ¨¡å‹ä¸­å­˜åœ¨ä¸åŒç±»å‹çš„å¼ é‡ï¼ˆå¦‚ç¤ºä¾‹ä¸­çš„f16å’Œf32ï¼‰ã€‚æœ¬æŒ‡å—ä¸­å·²ç»å®ç°äº†ä»¥ä¸‹ä¿®å¤ï¼š

1. **ç»Ÿä¸€æ¨¡å‹åŠ è½½æ—¶çš„æ•°æ®ç±»å‹**ï¼š
   ```python
   model = AutoModelForCausalLM.from_pretrained(
       model_path,
       torch_dtype=torch.float32,  # ä½¿ç”¨float32è€Œéfloat16
       device_map="mps",
       # ...
   )
   ```

2. **ç‰¹æ®Šçš„MPSè®­ç»ƒå‡†å¤‡å‡½æ•°**ï¼š
   ```python
   def prepare_model_for_mps_training(model):
       # ç¡®ä¿æ‰€æœ‰æ³¨æ„åŠ›å±‚å’Œå‰é¦ˆå±‚çš„å‚æ•°ä½¿ç”¨ç›¸åŒæ•°æ®ç±»å‹
       for name, module in model.named_modules():
           # æ£€æŸ¥æƒé‡å’Œåç½®æ˜¯å¦ä½¿ç”¨ä¸åŒçš„æ•°æ®ç±»å‹
           # ...
       return model
   ```

3. **ç¦ç”¨KVç¼“å­˜ä¸æ¢¯åº¦æ£€æŸ¥ç‚¹ä¸€èµ·ä½¿ç”¨**ï¼š
   ```python
   model.config.use_cache = False  # é¿å…ä¸æ¢¯åº¦æ£€æŸ¥ç‚¹å†²çª
   ```

4. **ç¡®ä¿æ•°æ®åŠ è½½å™¨è¿”å›ç»Ÿä¸€ç±»å‹çš„å¼ é‡å¹¶åŒ…å«æ ‡ç­¾**ï¼š
   ```python
   def data_collator(features):
       # ...
       # åˆ›å»ºæ ‡ç­¾å¼ é‡
       batch["labels"] = batch["input_ids"].clone()
       
       # ç¡®ä¿æ‰€æœ‰å¼ é‡ä¸ºfloat32ç±»å‹
       for key, value in batch.items():
           if isinstance(value, torch.Tensor) and value.dtype == torch.float16:
               batch[key] = value.to(dtype=torch.float32)
       return batch
   ```

### 5. å…¶ä»–å¸¸è§è®­ç»ƒé”™è¯¯

1. **ç¼ºå°‘æ ‡ç­¾é”™è¯¯**ï¼šå¦‚æœæ‚¨é‡åˆ°ä»¥ä¸‹é”™è¯¯ï¼š
   ```
   ValueError: The model did not return a loss from the inputs, only the following keys: logits.
   ```
   
   è¿™æ„å‘³ç€æ¨¡å‹åœ¨è®­ç»ƒæ—¶æ²¡æœ‰æ”¶åˆ°`labels`å­—æ®µï¼Œæ— æ³•è®¡ç®—æŸå¤±å‡½æ•°ã€‚è§£å†³æ–¹æ³•æ˜¯ä¿®æ”¹æ•°æ®æ•´ç†å‡½æ•°ï¼Œç¡®ä¿å®ƒä¸ºæ¨¡å‹æä¾›è¾“å…¥çš„å‰¯æœ¬ä½œä¸ºæ ‡ç­¾ï¼š
   ```python
   batch["labels"] = batch["input_ids"].clone()
   ```

2. **æ•°æ®é›†åˆ—ä¸åŒ¹é…é”™è¯¯**ï¼šè¿™æ˜¯PEFTæ¨¡å‹çš„ä¸€ä¸ªå¸¸è§é—®é¢˜ï¼Œè§£å†³æ–¹æ³•æ˜¯åœ¨`TrainingArguments`ä¸­è®¾ç½®ï¼š
   ```python
   remove_unused_columns=False
   ```

ä»¥ä¸Šä¿®å¤æªæ–½åº”è¯¥èƒ½å¤Ÿè§£å†³MPSåç«¯ä¸Šçš„å¸¸è§è®­ç»ƒé—®é¢˜ã€‚


---

## é™„ï¼šåŸç‰ˆæ–‡ç« å†…å®¹


å‚è€ƒæ–‡ç« ï¼š[ã€Šå•å¡ RTX 4090 ç”¨ unsloth å’ŒåŒ»å­¦æ•°æ®å¾®è°ƒ DeepSeek-R1-Distill-Qwen-14Bã€‹](https://mp.weixin.qq.com/s?__biz=MzUxMTczMTY1Ng==&mid=2247483742&idx=1&sn=b90af9f3a12f182f5e48859b3b72a40a&scene=21#wechat_redirect)

åœ¨æ–‡ç« [ã€Šå•å¡4090å¾®è°ƒDeepSeek-R1-32Bã€‹](https://mp.weixin.qq.com/s?__biz=MzUxMTczMTY1Ng==&mid=2247483746&idx=1&sn=b7e16a70ffb903b64f28424677fb22c0&chksm=f96e73c5ce19fad3b1b26f56826a3e2470bcfbc8675995c6a47b2b9e466d19ba6ec2c6a32277&cur_album_id=3854852837976768515&scene=189#wechat_redirect)ä¸­ï¼Œæä¾›çš„è®­ç»ƒä»£ç å¦‚ä¸‹ï¼š


```python
import wandb
# ç™»å½• wandb.ai ç”¨äºå®éªŒè·Ÿè¸ª
wandb.login(key="æ”¾ç½®ä½ çš„wandb.aiç½‘ç«™ä¸Šçš„token")
# åˆå§‹åŒ–wandbé¡¹ç›®
run = wandb.init(
    project='Lora-R1-Distill-Qwen on Medical COT Dataset',
    job_type="training",
    anonymous="allow"
)

####################################################################################################
# 1.åŠ è½½æ¨¡å‹

# ä½¿ç”¨ unsloth ä¼˜åŒ–çš„ FastLanguageModel åŠ è½½æ¨¡å‹
from unsloth import FastLanguageModel
max_seq_length = 4096 # æœ€å¤§åºåˆ—é•¿åº¦
dtype = None          # æ•°æ®ç±»å‹ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©
load_in_4bit = True   # ä½¿ç”¨4bité‡åŒ–åŠ è½½æ¨¡å‹ä»¥èŠ‚çœæ˜¾å­˜

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨
model, tokenizer = FastLanguageModel.from_pretrained(
    #model_name = "unsloth/DeepSeek-R1-Distill-Qwen-7B",
    model_name = "/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
    local_files_only=True,  # é¿å…è”ç½‘
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    #token = hf_token, 
)
print(model)

####################################################################################################
# 2. å®šä¹‰æç¤ºæ¨¡æ¿ï¼Œå¹¶åœ¨å¾®è°ƒå‰åšä¸€æ¬¡æ¨ç†

prompt_style = """ä»¥ä¸‹æ˜¯æè¿°ä»»åŠ¡çš„æŒ‡ä»¤ï¼Œä»¥åŠæä¾›æ›´å¤šä¸Šä¸‹æ–‡çš„è¾“å…¥ã€‚
  è¯·å†™å‡ºæ°å½“å®Œæˆè¯¥è¯·æ±‚çš„å›ç­”ã€‚
  åœ¨å›ç­”ä¹‹å‰ï¼Œè¯·ä»”ç»†æ€è€ƒé—®é¢˜ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªé€æ­¥çš„æ€ç»´é“¾ï¼Œä»¥ç¡®ä¿å›ç­”åˆä¹é€»è¾‘ä¸”å‡†ç¡®ã€‚
  ### Instruction:
  ä½ æ˜¯ä¸€ä½åœ¨ä¸´åºŠæ¨ç†ã€è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’æ–¹é¢å…·æœ‰ä¸“ä¸šçŸ¥è¯†çš„åŒ»å­¦ä¸“å®¶ã€‚
  è¯·å›ç­”ä»¥ä¸‹åŒ»å­¦é—®é¢˜ã€‚
  ### Question:
  {}
  ### Response:
  <think>{}"""
train_prompt_style = prompt_style + """
  </think>
  {}"""

# æµ‹è¯•ç”¨åŒ»å­¦é—®é¢˜
question = "ä¸€å70å²çš„ç”·æ€§æ‚£è€…å› èƒ¸ç—›ä¼´å‘•å16å°æ—¶å°±åŒ»ï¼Œå¿ƒç”µå›¾æ˜¾ç¤ºä¸‹å£å¯¼è”å’Œå³èƒ¸å¯¼è”STæ®µæŠ¬é«˜0.1~0.3mVï¼Œç»è¡¥æ¶²åè¡€å‹é™è‡³80/60mmHgï¼Œæ‚£è€…å‡ºç°å‘¼å¸å›°éš¾å’Œä¸èƒ½å¹³å§çš„ç—‡çŠ¶ï¼Œä½“æ£€å‘ç°åŒè‚ºæœ‰å¤§é‡æ°´æ³¡éŸ³ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæœ€æ°å½“çš„è¯ç‰©å¤„ç†æ˜¯ä»€ä¹ˆï¼Ÿ"

# è®¾ç½®æ¨¡å‹ä¸ºæ¨ç†æ¨¡å¼
FastLanguageModel.for_inference(model) 
inputs = tokenizer([prompt_style.format(question, "")], return_tensors="pt").to("cuda")

# ç”Ÿæˆå›ç­”
outputs = model.generate(
    input_ids=inputs.input_ids,
    attention_mask=inputs.attention_mask,
    max_new_tokens=1200,
    use_cache=True,
)
response = tokenizer.batch_decode(outputs)
print("### å¾®è°ƒå‰æ¨¡å‹æ¨ç†ç»“æœï¼š")
print(response[0].split("### Response:")[1])

####################################################################################################
# 3. å¤„ç†æ•°æ®é›†

EOS_TOKEN = tokenizer.eos_token  # æ·»åŠ ç»“æŸç¬¦æ ‡è®°
#æ ¼å¼åŒ–æç¤ºå‡½æ•°,ç”¨äºå¤„ç†æ•°æ®é›†ä¸­çš„ç¤ºä¾‹
def formatting_prompts_func(examples):
    # ä»examplesä¸­æå–é—®é¢˜ã€æ€ç»´é“¾å’Œå›ç­”
    inputs = examples["Question"]      # åŒ»å­¦é—®é¢˜åˆ—è¡¨
    cots = examples["Complex_CoT"]     # æ€ç»´é“¾åˆ—è¡¨ 
    outputs = examples["Response"]     # å›ç­”åˆ—è¡¨
    
    # å­˜å‚¨æ ¼å¼åŒ–åçš„æ–‡æœ¬
    texts = []
    
    # éå†æ¯ä¸ªç¤ºä¾‹,å°†é—®é¢˜ã€æ€ç»´é“¾å’Œå›ç­”ç»„åˆæˆæŒ‡å®šæ ¼å¼
    for input, cot, output in zip(inputs, cots, outputs):
        # ä½¿ç”¨train_prompt_styleæ¨¡æ¿æ ¼å¼åŒ–æ–‡æœ¬,å¹¶æ·»åŠ ç»“æŸç¬¦
        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN
        texts.append(text)
        
    # è¿”å›æ ¼å¼åŒ–åçš„æ–‡æœ¬å­—å…¸
    return {
        "text": texts,
    }

# åŠ è½½æ•°æ®é›†å¹¶åº”ç”¨æ ¼å¼åŒ–
from datasets import load_dataset,load_from_disk
dataset = load_dataset(
    "json",  # æŒ‡å®šæ•°æ®æ ¼å¼ä¸º JSON
    data_files="/datasets/FreedomIntelligence/medical-o1-reasoning-SFT/medical_o1_sft_Chinese.json",
    #split="train[0:500]",  # åªå–å‰ 500 æ¡æ•°æ®
    trust_remote_code=True  # å…¼å®¹ remote code çš„è¡Œä¸º
)

# å¦‚æœè¿”å›çš„æ˜¯ DatasetDictï¼Œåˆ™å–å‡º "train" è¿™ä¸€éƒ¨åˆ†
if isinstance(dataset, dict):  
    dataset = dataset["train"]
    
dataset = dataset.map(formatting_prompts_func, batched = True,)
print(dataset)  # æŸ¥çœ‹æ•°æ®é›†ç»“æ„

####################################################################################################
# 4. é…ç½®è®­ç»ƒå‚æ•°å¯åŠ¨è®­ç»ƒ

model = FastLanguageModel.get_peft_model(
    model, 
    r=32,
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj", 
        "gate_proj", "up_proj", "down_proj",    
    ],
    lora_alpha=16,
    lora_dropout=0,  
    bias="none",  
    use_gradient_checkpointing="unsloth", 
    random_state=8137,
    use_rslora=False,  
    loftq_config=None,
)
print(model)

# é…ç½®è®­ç»ƒå‚æ•°å’Œåˆå§‹åŒ–è®­ç»ƒå™¨
from trl import SFTTrainer  
from transformers import TrainingArguments  
from unsloth import is_bfloat16_supported  

# åˆå§‹åŒ– SFT è®­ç»ƒå™¨
trainer = SFTTrainer(
    model=model,  
    tokenizer=tokenizer,  
    train_dataset=dataset,  
    dataset_text_field="text",  # æ•°æ®é›†å­—æ®µçš„åç§°
    max_seq_length=max_seq_length,  
    dataset_num_proc=2,  # æ•°æ®é›†å¤„ç†çš„å¹¶è¡Œè¿›ç¨‹æ•°ï¼Œæé«˜CPUåˆ©ç”¨ç‡
    args=TrainingArguments(
        per_device_train_batch_size=2, 
        gradient_accumulation_steps=4,   
        warmup_steps=5,  # é¢„çƒ­æ­¥æ•°,é€æ­¥å¢åŠ å­¦ä¹ ç‡
        learning_rate=2e-4,  # å­¦ä¹ ç‡
        lr_scheduler_type="linear",  # çº¿æ€§å­¦ä¹ ç‡è°ƒåº¦å™¨
        # max_steps=200,    # æœ€å¤§è®­ç»ƒæ­¥æ•°ï¼ˆä¸€æ­¥ = å¤„ç†ä¸€ä¸ªbatchçš„æ•°æ®ï¼‰
        fp16=not is_bfloat16_supported(),  # å¦‚æœä¸æ”¯æŒbf16åˆ™ä½¿ç”¨fp16
        bf16=is_bfloat16_supported(),      # å¦‚æœæ”¯æŒåˆ™ä½¿ç”¨bf16
        logging_steps=10,  # æ¯10æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—
        optim="adamw_8bit",  # ä½¿ç”¨8ä½AdamWä¼˜åŒ–å™¨èŠ‚çœæ˜¾å­˜ï¼Œå‡ ä¹ä¸å½±å“è®­ç»ƒæ•ˆæœ
        weight_decay=0.01,   # æƒé‡è¡°å‡ç³»æ•°,ç”¨äºæ­£åˆ™åŒ–ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
        seed=8137,  # éšæœºæ•°ç§å­
        output_dir="outputs",  # ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹å’Œè®­ç»ƒæ—¥å¿—
        run_name="medical-o1-sft-experiment",  # æ˜¾å¼è®¾ç½® wandb è¿è¡Œåç§°ï¼Œé¿å…è­¦å‘Š
    ),
)

# å¼€å§‹è®­ç»ƒ
print(f"trainer.args.max_steps: {trainer.args.max_steps}")
print(f"trainer.args.num_train_epochs: {trainer.args.num_train_epochs}")
trainer.train()
print(f"Total training steps: {trainer.state.max_steps}")
print(f"Total epochs: {trainer.state.epoch}")

####################################################################################################
# 5. å¾®è°ƒåçš„æ¨¡å‹åšä¸€æ¬¡æ¨ç†

FastLanguageModel.for_inference(model)  
inputs = tokenizer([prompt_style.format(question, "")], return_tensors="pt").to("cuda")

# ç”Ÿæˆå›ç­”
outputs = model.generate(
    input_ids=inputs.input_ids, # è¾“å…¥tokençš„idåºåˆ—
    attention_mask=inputs.attention_mask,  # æ³¨æ„åŠ›æ©ç ,ç”¨äºæ ‡è®°æœ‰æ•ˆè¾“å…¥ä½ç½®
    max_new_tokens=1200, # ç”Ÿæˆçš„æœ€å¤§æ–°tokenæ•°é‡
    use_cache=True, # æ˜¯å¦ä½¿ç”¨KVç¼“å­˜åŠ é€Ÿç”Ÿæˆ
)

response = tokenizer.batch_decode(outputs)
print("### å¾®è°ƒåæ¨¡å‹æ¨ç†ç»“æœï¼š")
print(response[0].split("### Response:")[1])

####################################################################################################
# 6. ä¿å­˜æ¨¡å‹

new_model_local = "DeepSeek-R1-Medical-COT-Qwen-32B"
model.save_pretrained(new_model_local) 
tokenizer.save_pretrained(new_model_local)

# ä¿å­˜åˆå¹¶åçš„16bitæ¨¡å‹
model.save_pretrained_merged(new_model_local, tokenizer, save_method = "merged_16bit",)

# ä¿å­˜ä¸º GGUF æ¨¡å‹
# model.save_pretrained_gguf("DeepSeek-R1-Qwen-32B-Medical-COT-GGUF", tokenizer,)
```

## æ€»ç»“ï¼šmacOSç‰ˆæœ¬ä¸åŸç‰ˆä»£ç çš„ä¸»è¦è°ƒæ•´

å¯¹æ¯”åŸå§‹RTX 4090ä¸Šä½¿ç”¨Unslothæ¡†æ¶çš„ä»£ç ä¸é€‚é…macOSçš„ç‰ˆæœ¬ï¼Œä¸»è¦è¿›è¡Œäº†ä»¥ä¸‹è°ƒæ•´ï¼š

### 1. æ¡†æ¶ä¸æ¶æ„è°ƒæ•´
- **æ¡†æ¶æ›¿æ¢**ï¼šå®Œå…¨æ”¾å¼ƒäº†Unslothæ¡†æ¶ï¼Œæ”¹ç”¨æ ‡å‡†çš„Hugging Face PEFTåº“å®ç°LoRAå¾®è°ƒ
- **åç«¯é€‚é…**ï¼šä»CUDAåç«¯è½¬æ¢åˆ°Apple Siliconçš„MPSåç«¯ï¼Œå¹¶é’ˆå¯¹MPSçš„ç‰¹æ€§è¿›è¡Œäº†å¤§é‡å…¼å®¹æ€§ä¿®å¤
- **è®­ç»ƒå™¨å˜æ›´**ï¼šä»Unslothçš„`SFTTrainer`æ›¿æ¢ä¸ºæ ‡å‡†çš„Hugging Face `Trainer`

### 2. å†…å­˜ä¼˜åŒ–ç­–ç•¥
- **æ·»åŠ ä¸»åŠ¨å†…å­˜ç®¡ç†**ï¼šå®šä¹‰äº†`clean_memory()`å‡½æ•°ï¼Œåœ¨å…³é”®èŠ‚ç‚¹ä¸»åŠ¨æ¸…ç†GPUå’ŒCPUå†…å­˜
- **ç¯å¢ƒå˜é‡ä¼˜åŒ–**ï¼šæ·»åŠ äº†å¤šä¸ªMPSç›¸å…³ç¯å¢ƒå˜é‡æ§åˆ¶å†…å­˜ä½¿ç”¨å’Œé”™è¯¯å¤„ç†
- **æ‰¹é‡å¤„ç†å‚æ•°è½¬æ¢**ï¼šå®ç°äº†`safe_prepare_model_for_kbit_training`å‡½æ•°ï¼Œåˆ†æ‰¹å¤„ç†å‚æ•°è½¬æ¢ä»¥é¿å…å†…å­˜å³°å€¼

### 3. è®­ç»ƒå‚æ•°è°ƒæ•´
- **æ‰¹æ¬¡å¤§å°é™ä½**ï¼šä»`per_device_train_batch_size=2`é™ä½åˆ°`1`
- **æ¢¯åº¦ç´¯ç§¯å¢åŠ **ï¼šä»`gradient_accumulation_steps=4`å¢åŠ åˆ°`8`
- **åºåˆ—é•¿åº¦ç¼©çŸ­**ï¼šä»`max_seq_length=4096`é™ä½ä¸º`2048`
- **LoRAå‚æ•°å‡å°**ï¼šä»`r=32`å‡å°åˆ°`r=16`ä»¥èŠ‚çœå†…å­˜
- **æ•°æ®ç²¾åº¦è°ƒæ•´**ï¼šç¦ç”¨åŠç²¾åº¦(`fp16=False`ã€`bf16=False`)ï¼Œç»Ÿä¸€ä½¿ç”¨`float32`é¿å…MPSæ•°æ®ç±»å‹ä¸åŒ¹é…é”™è¯¯
- **ç¦ç”¨å¤šè¿›ç¨‹**ï¼šè®¾ç½®`dataloader_num_workers=0`é¿å…å¤šè¿›ç¨‹å¸¦æ¥çš„é¢å¤–å†…å­˜å¼€é”€

### 4. å…¼å®¹æ€§ä¿®å¤ä¸æ™ºèƒ½å›é€€
- **æ•°æ®ç±»å‹ç»Ÿä¸€**ï¼šå®ç°äº†ç¡®ä¿æ‰€æœ‰å¼ é‡ä½¿ç”¨ç›¸åŒæ•°æ®ç±»å‹(`float32`)çš„ç‰¹æ®Šå¤„ç†
- **æ™ºèƒ½æ¨ç†å›é€€**ï¼šæ·»åŠ äº†`smart_inference`å‡½æ•°ï¼Œåœ¨MPSåç«¯å¤±è´¥æ—¶è‡ªåŠ¨å›é€€åˆ°CPUæ¨ç†
- **æ¢¯åº¦æ£€æŸ¥ç‚¹ä¸KVç¼“å­˜å†²çªè§£å†³**ï¼šåœ¨ä¸åŒé˜¶æ®µé€‚å½“å¯ç”¨/ç¦ç”¨`gradient_checkpointing`å’Œ`use_cache`

### 5. é”™è¯¯å¤„ç†ä¸ç”¨æˆ·ä½“éªŒ
- **è¯¦ç»†é”™è¯¯åˆ†æ**ï¼šå¯¹å¸¸è§é”™è¯¯ç±»å‹æä¾›å…·ä½“çš„ä¿®å¤å»ºè®®
- **åˆ†çº§é…ç½®æŒ‡å—**ï¼šæ ¹æ®ä¸åŒMacå†…å­˜é…ç½®æä¾›ç›¸åº”çš„å‚æ•°è°ƒæ•´å»ºè®®
- **è·¯å¾„å¤„ç†æ”¹è¿›**ï¼šå¼ºè°ƒå¹¶è§£å†³äº†Pythonè„šæœ¬ä¸­`~/`è·¯å¾„ä¸ä¼šè‡ªåŠ¨å±•å¼€çš„é—®é¢˜

è¿™äº›è°ƒæ•´ä¸ä»…è§£å†³äº†Apple Siliconæ¶æ„ä¸MPSåç«¯çš„å…¼å®¹æ€§é—®é¢˜ï¼Œè¿˜é€šè¿‡ç²¾å¿ƒä¼˜åŒ–çš„å†…å­˜ç®¡ç†å’Œå‚æ•°è°ƒæ•´ï¼Œä½¿å¾—åœ¨å†…å­˜æœ‰é™çš„Macè®¾å¤‡ä¸Šä¹Ÿèƒ½ç¨³å®šåœ°å¾®è°ƒ14Bå‚æ•°çº§åˆ«çš„å¤§è¯­è¨€æ¨¡å‹ã€‚



## Macè®¾å¤‡å¾®è°ƒå¤§è¯­è¨€æ¨¡å‹çš„å…¶ä»–å‚è€ƒä¿¡æ¯

### éCUDAç¯å¢ƒä¸‹çš„å¤§æ¨¡å‹å¾®è°ƒå®è·µ

[æœ‰äº‘æŠ€æœ¯å›¢é˜Ÿçš„å®è·µ](https://mp.weixin.qq.com/s?__biz=MjM5NjUxNDIwNw==&mid=2654069282&idx=1&sn=ac26cc4dc52b0d63c11cfee95edd2187&chksm=bc35d7c842283b695ab733fa22829780c7e39ca62ae460fa4fd2cae2594bf175a0b4291eeb4e#rd)å±•ç¤ºäº†å¦‚ä½•åœ¨Mac Mç³»åˆ—èŠ¯ç‰‡ä¸Šä½¿ç”¨LoRAæŠ€æœ¯å¯¹DeepSeek Coderæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä¸ºä½¿ç”¨ Mac çš„å¼€å‘è€…æä¾›å‚è€ƒã€‚

#### 1. é¡¹ç›®æ¦‚è¿°

**ç›®æ ‡**ï¼š
- ä½¿ç”¨LoRAæŠ€æœ¯å¾®è°ƒDeepSeek Coder 7Bæ¨¡å‹
- ä¼˜åŒ–æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šçš„è¡¨ç°
- åœ¨Mac Mç³»åˆ—èŠ¯ç‰‡ä¸Šå®ç°å¾®è°ƒè®­ç»ƒå’Œæ¨ç†

**æŠ€æœ¯æ ˆ**ï¼š
- åŸºç¡€æ¨¡å‹ï¼šdeepseek-ai/deepseek-coder-7b-base
- è®­ç»ƒæ¡†æ¶ï¼šPyTorch + Transformers + PEFT
- ç¡¬ä»¶å¹³å°ï¼šMac Mç³»åˆ—èŠ¯ç‰‡(MPSåç«¯)

#### 2. å®ç°æµç¨‹

**æ•°æ®å‡†å¤‡**ï¼š
```python
training_data = [
    {
        "instruction": "å®ç°ä¸€ä¸ªäºŒåˆ†æŸ¥æ‰¾ç®—æ³•",
        "input": "",
        "output": """def binary_search(arr, target):
    left, right = 0, len(arr) - 1
    while left <= right:
        mid = (left + right) // 2
        if arr[mid] == target:
            return mid
        elif arr[mid] < target:
            left = mid + 1
        else:
            right = mid - 1
    return -1"""
    },
    {
        "instruction": "ç¼–å†™ä¸€ä¸ªå¿«é€Ÿæ’åºå‡½æ•°",
        "input": "",
        "output": """def quicksort(arr):
    if len(arr) <= 1:
        return arr
    pivot = arr[len(arr) // 2]
    left = [x for x in arr if x < pivot]
    middle = [x for x in arr if x == pivot]
    right = [x for x in arr if x > pivot]
    return quicksort(left) + middle + quicksort(right)"""
    }
]
```

**LoRAé…ç½®**ï¼š
```python
LORA_CONFIG = LoraConfig(
    task_type=TaskType.CAUSAL_LM,      # ä»»åŠ¡ç±»å‹:å› æœè¯­è¨€æ¨¡å‹
    r=8,                               # LoRAçŸ©é˜µçš„ç§©,æ§åˆ¶å¯è®­ç»ƒå‚æ•°é‡
    lora_alpha=32,                     # ç¼©æ”¾å› å­,æ§åˆ¶LoRAæ›´æ–°çš„é‡è¦æ€§
    lora_dropout=0.1,                  # Dropoutæ¯”ä¾‹,é˜²æ­¢è¿‡æ‹Ÿåˆ
    target_modules=[                   # éœ€è¦è®­ç»ƒçš„æ¨¡å—
        "q_proj",                      # æŸ¥è¯¢çŸ©é˜µæŠ•å½±å±‚
        "v_proj",                      # å€¼çŸ©é˜µæŠ•å½±å±‚  
        "k_proj",                      # é”®çŸ©é˜µæŠ•å½±å±‚
        "o_proj",                      # è¾“å‡ºæŠ•å½±å±‚
    ],
    bias="none",                       # æ˜¯å¦è®­ç»ƒåç½®é¡¹
    modules_to_save=None               # éœ€è¦å®Œæ•´ä¿å­˜çš„æ¨¡å—
)
```

**è®­ç»ƒå‚æ•°ä¼˜åŒ–**ï¼š
```python
training_args = TrainingArguments(
    output_dir="./deepseek-finetuned",
    per_device_train_batch_size=1,     # å—é™äºMç³»åˆ—èŠ¯ç‰‡å†…å­˜
    gradient_accumulation_steps=64,    # ç´¯ç§¯æ¢¯åº¦ä»¥æ¨¡æ‹Ÿå¤§æ‰¹é‡
    learning_rate=5e-5,
    num_train_epochs=3,
    gradient_checkpointing=True,       # å‡å°‘å†…å­˜ä½¿ç”¨
    max_grad_norm=0.3,                 # æ¢¯åº¦è£å‰ª,é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
    warmup_steps=50                    # é¢„çƒ­æ­¥æ•°
)
```

#### 3. Mac Mç³»åˆ—é€‚é…æŠ€å·§

**å†…å­˜ç®¡ç†**ï¼š
```python
# è®¾ç½®MPSå†…å­˜æ°´ä½
os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
os.environ['PYTORCH_MPS_LOW_WATERMARK_RATIO'] = '0.0'

# å®šæœŸæ¸…ç†å†…å­˜
if torch.backends.mps.is_available():
    torch.mps.empty_cache()
```

**æ€§èƒ½ä¼˜åŒ–ç­–ç•¥**ï¼š

1. **æ¨¡å‹ä¼˜åŒ–**ï¼š
   - KV Cacheä¼˜åŒ–ï¼šå¯ç”¨æ³¨æ„åŠ›ç¼“å­˜æœºåˆ¶,ä¼˜åŒ–ç¼“å­˜æ›´æ–°ç­–ç•¥
   - è®¡ç®—å›¾ä¼˜åŒ–ï¼šä½¿ç”¨torch.inferencemode(),å¯ç”¨é™æ€å›¾ä¼˜åŒ–
   - ç”Ÿæˆå‚æ•°è°ƒä¼˜ï¼šè‡ªé€‚åº”batch size,ä¼˜åŒ–é‡‡æ ·ç­–ç•¥

2. **å†…å­˜ä¼˜åŒ–**ï¼š
   - æ‰¹å¤„ç†ç­–ç•¥ï¼šåŠ¨æ€batch sizeè°ƒæ•´,æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ä¼˜åŒ–
   - æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼šé€‰æ‹©æ€§æ¿€æ´»æ£€æŸ¥ç‚¹,å¹³è¡¡å†…å­˜ä¸é€Ÿåº¦
   - æ˜¾å­˜ç®¡ç†ï¼šåŠæ—¶é‡Šæ”¾æ— ç”¨å¼ é‡,ä½¿ç”¨CPU offload,å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ

3. **æ•°æ®åŠ è½½ä¼˜åŒ–**ï¼š
   - é¢„å¤„ç†åŠ é€Ÿï¼šå¤šè¿›ç¨‹æ•°æ®åŠ è½½,æ•°æ®é¢„å–ä¸ç¼“å­˜
   - å†…å­˜ç®¡ç†ï¼šä½¿ç”¨å†…å­˜æ˜ å°„,æ¸è¿›å¼æ•°æ®åŠ è½½
   - IOä¼˜åŒ–ï¼šä½¿ç”¨å¼‚æ­¥åŠ è½½,ä¼˜åŒ–æ•°æ®æ ¼å¼

4. **è®­ç»ƒæµç¨‹ä¼˜åŒ–**ï¼š
   - ç›‘æ§ä¸è°ƒæ•´ï¼šå®æ—¶æ€§èƒ½åˆ†æ,è‡ªåŠ¨åŒ–å‚æ•°è°ƒä¼˜
   - å®¹é”™æœºåˆ¶ï¼šæ£€æŸ¥ç‚¹ä¿å­˜,è®­ç»ƒçŠ¶æ€æ¢å¤

#### 4. æ¨ç†ä¼˜åŒ–é…ç½®

```python
generation_config = {
    "max_length": 2048,               # ç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦
    "temperature": 0.7,               # é‡‡æ ·æ¸©åº¦,æ§åˆ¶ç”Ÿæˆçš„éšæœºæ€§
    "top_p": 0.95,                    # æ ¸é‡‡æ ·é˜ˆå€¼,æ§åˆ¶è¯è¡¨é‡‡æ ·èŒƒå›´
    "top_k": 50,                      # ä¿ç•™æ¦‚ç‡æœ€é«˜çš„kä¸ªtoken
    "do_sample": True,                # æ˜¯å¦ä½¿ç”¨é‡‡æ ·ç­–ç•¥
    "num_beams": 1,                   # beam searchçš„æŸå®½
    "repetition_penalty": 1.1,        # é‡å¤æƒ©ç½šå› å­,é¿å…é‡å¤ç”Ÿæˆ
    "early_stopping": True,           # æ˜¯å¦æå‰åœæ­¢ç”Ÿæˆ
    "pad_token_id": tokenizer.pad_token_id,  # padding tokençš„ID
    "eos_token_id": tokenizer.eos_token_id,  # ç»“æŸç¬¦tokençš„ID
}
```

#### 5. ä¸»è¦æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆ

**MPSåç«¯å†…å­˜ç®¡ç†**ï¼š
- å†…å­˜æ³„æ¼é—®é¢˜ï¼šMPSåç«¯ç¼“å­˜ç§¯ç´¯ã€å¼ é‡ç¢ç‰‡åŒ–ã€è‡ªåŠ¨å›æ”¶ä¸åŠæ—¶
- è§£å†³æ–¹æ¡ˆï¼šå®šæœŸæ‰‹åŠ¨æ¸…ç†ç¼“å­˜ã€ç›‘æ§å†…å­˜ä½¿ç”¨ã€ä¼˜åŒ–å¼ é‡ç”Ÿå‘½å‘¨æœŸ

**æ¶ˆè´¹çº§ç¡¬ä»¶ä¼˜åŒ–**ï¼š
- è®¡ç®—èƒ½åŠ›é™åˆ¶ï¼šå•GPUæ˜¾å­˜ä¸è¶³ã€è®¡ç®—é€Ÿåº¦ç“¶é¢ˆ
- è§£å†³æ–¹æ¡ˆï¼šæ¨¡å‹é‡åŒ–ã€æ¢¯åº¦ç´¯ç§¯ã€ä¼˜åŒ–å™¨å†…å­˜ä¼˜åŒ–

**è®­ç»ƒå’Œæ¨ç†æ€§èƒ½å¹³è¡¡**ï¼š
- è®­ç»ƒæ•ˆç‡ï¼šæ‰¹å¤„ç†å¤§å°å—é™ã€æ¢¯åº¦ç´¯ç§¯å¼€é”€å¤§
- æ¨ç†å»¶è¿Ÿï¼šé¦–æ¬¡æ¨ç†å»¶è¿Ÿé«˜ã€å“åº”æ—¶é—´ä¸ç¨³å®š
- è§£å†³æ–¹æ¡ˆï¼šåŠ¨æ€æ‰¹å¤„ç†ã€KVç¼“å­˜ä¼˜åŒ–ã€é¢„çƒ­æ¨¡å‹

#### 6. å‚è€ƒèµ„æº

- DeepSeek Coderå®˜æ–¹æ–‡æ¡£
- PEFTåº“æ–‡æ¡£
- PyTorch MPSæ–‡æ¡£
- LoRAè®ºæ–‡


### åŸºäºApple MLXæ¡†æ¶çš„å¤§æ¨¡å‹å¾®è°ƒ

[Apple MLXæ¡†æ¶](https://github.com/ml-explore/mlx)æ˜¯è‹¹æœå®˜æ–¹å¼€æºçš„é’ˆå¯¹Apple SiliconèŠ¯ç‰‡ä¼˜åŒ–çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œä¸ºå¼€å‘è€…æä¾›äº†åœ¨Macç­‰è®¾å¤‡ä¸Šç›´æ¥è¿›è¡Œæ¨¡å‹è®­ç»ƒå’Œéƒ¨ç½²çš„èƒ½åŠ›ã€‚[æ–‡ç« ](https://mp.weixin.qq.com/s?__biz=MzA5NDc3ODQxNQ==&mid=2247484394&idx=1&sn=aeeb89f13a5269cbda8ca6f31d198b89&chksm=915cb4f81cab6148fda2f458f969edec253f789c4279ba67cd946f6d0fdbc3eb2a93f32c389a#rd)ä»‹ç»åŸºäºMLXæ¡†æ¶åœ¨M1è®¾å¤‡ä¸Šå¾®è°ƒMistral-7Bæ¨¡å‹çš„å®è·µè¿‡ç¨‹ã€‚

#### 1. MLXæ¡†æ¶ç‰¹æ€§

- **ç†Ÿæ‚‰çš„API**ï¼šæä¾›ç±»ä¼¼NumPyçš„Python APIå’Œç±»ä¼¼PyTorchçš„é«˜çº§API
- **å¯ç»„åˆå‡½æ•°è½¬æ¢**ï¼šæ”¯æŒè‡ªåŠ¨å¾®åˆ†ã€è‡ªåŠ¨çŸ¢é‡åŒ–å’Œè®¡ç®—å›¾ä¼˜åŒ–
- **æƒ°æ€§è®¡ç®—**ï¼šåªåœ¨éœ€è¦æ—¶æ‰å…·ä½“åŒ–æ•°ç»„ï¼Œæé«˜è®¡ç®—æ•ˆç‡
- **åŠ¨æ€å›¾æ„å»º**ï¼šæ— éœ€é‡ç¼–è¯‘å³å¯æ”¯æŒå½¢çŠ¶å˜åŒ–ï¼Œç®€åŒ–è°ƒè¯•
- **å¤šè®¾å¤‡æ”¯æŒ**ï¼šå¯åœ¨CPUå’ŒGPUä¸Šè¿è¡Œï¼Œå……åˆ†åˆ©ç”¨ç¡¬ä»¶
- **ç»Ÿä¸€å†…å­˜æ¨¡å‹**ï¼šæ•°ç»„ä½äºå…±äº«å†…å­˜ï¼Œæ— éœ€è®¾å¤‡é—´æ•°æ®ç§»åŠ¨

#### 2. å¾®è°ƒç¯å¢ƒå‡†å¤‡

```bash
# è·å–MLXç¤ºä¾‹ä»£ç 
git clone https://github.com/ml-explore/mlx-examples
cd mlx-examples/lora

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# ä¸‹è½½Mistral-7Bæ¨¡å‹
curl -O https://files.mistral-7b-v0-1.mistral.ai/mistral-7B-v0.1.tar
tar -xf mistral-7B-v0.1.tar

# è½¬æ¢æ¨¡å‹æ ¼å¼ä¸ºMLXæ ¼å¼
python convert.py \
  --torch-model mistral-7B-v0.1 \
  --mlx-model mistral-7b-v0.1-mlx
```

#### 3. è®­ç»ƒæ•°æ®æ ¼å¼

MLXç¤ºä¾‹ä¸­ä½¿ç”¨WikiSQLæ•°æ®é›†è¿›è¡Œæ–‡æœ¬åˆ°SQLçš„è½¬æ¢è®­ç»ƒï¼Œæ•°æ®æ ¼å¼å¦‚ä¸‹ï¼š

```json
{
    "text": "table: 1-1000181-1\ncolumns: State/territory, Text/background colour, Format, Current slogan, Current series, Notes\nQ: Tell me what the notes are for South Australia \nA: SELECT Notes FROM 1-1000181-1 WHERE Current slogan = 'SOUTH AUSTRALIA'"
}
```

æ•°æ®é›†ç»“æ„ï¼š
- `table`: è¡¨åç§°
- `columns`: åˆ—åç§°åˆ—è¡¨
- `Q`: ç”¨æˆ·è‡ªç„¶è¯­è¨€é—®é¢˜
- `A`: å¯¹åº”çš„SQLæŸ¥è¯¢è¯­å¥

#### 4. å†…å­˜ä¼˜åŒ–ä¸å‚æ•°è°ƒæ•´

åœ¨M1è®¾å¤‡ä¸Šè¿›è¡Œå¾®è°ƒæ—¶ï¼Œå†…å­˜ç®¡ç†æ˜¯å…³é”®æŒ‘æˆ˜ã€‚ä»¥ä¸‹æ˜¯ä¼˜åŒ–å‚æ•°ï¼š

```bash
# ä¼˜åŒ–å†…å­˜ä½¿ç”¨çš„è®­ç»ƒå‘½ä»¤
python lora.py \
   --model mistral-7b-v0.1-mlx \
   --train \
   --batch-size 1 \          # é™ä½æ‰¹æ¬¡å¤§å°å‡å°‘å†…å­˜å ç”¨
   --lora-layers 4           # å‡å°‘å¾®è°ƒå±‚æ•°
```

**å…³é”®å‚æ•°è§£æ**ï¼š
- `--batch-size`ï¼šé»˜è®¤ä¸º4ï¼Œé™ä½è‡³1æˆ–2å¯æ˜¾è‘—å‡å°‘å†…å­˜æ¶ˆè€—
- `--lora-layers`ï¼šé»˜è®¤ä¸º16ï¼Œå¯é™è‡³8æˆ–4å‡å°‘åå‘ä¼ æ’­å†…å­˜ï¼Œä½†å¯èƒ½å½±å“å¾®è°ƒè´¨é‡
- æ•°æ®é›†é•¿åº¦ï¼šè¾ƒé•¿æ ·æœ¬éœ€è¦æ›´å¤šå†…å­˜ï¼Œå¯è€ƒè™‘åˆ†è§£ä¸ºæ›´å°åºåˆ—

#### 5. æ€§èƒ½ä¸è®­ç»ƒè¿‡ç¨‹

åœ¨32GBçš„M1 Macä¸Šï¼Œä½¿ç”¨ä¼˜åŒ–å‚æ•°åçš„è®­ç»ƒæ€§èƒ½ï¼š
- å¤„ç†é€Ÿåº¦ï¼šçº¦110 tokens/ç§’
- æŸå¤±å˜åŒ–ï¼šä»åˆå§‹2.265é™è‡³1.325ï¼ˆ800æ¬¡è¿­ä»£åï¼‰

**ä¸åŒå‚æ•°é…ç½®çš„æŸå¤±å˜åŒ–å¯¹æ¯”**ï¼š

| é…ç½® | è¿­ä»£æ¬¡æ•° | Loss |
|------|---------|------|
| batch=1, lora=4 | 800 | 1.325 |
| batch=2, lora=8 | 800 | 1.213 |
| batch=2, lora=8, æ›´å¤§æ•°æ®é›† | 800 | 1.360 |


#### 6. æ¨¡å‹è¯„ä¼°ä¸è¾“å‡ºç¤ºä¾‹

å¾®è°ƒåçš„æ¨¡å‹ç”¨äºè‡ªç„¶è¯­è¨€åˆ°SQLçš„è½¬æ¢ä»»åŠ¡ï¼š

```bash
python lora.py --model mistral-7b-v0.1-mlx \
               --adapter-file adapters_2_8.npz \
               --num-tokens 50 \
               --prompt "table: 1-10015132-16
columns: Player, No., Nationality, Position, Years in Toronto, School/Club Team
Q: What is terrence ross' nationality
A: "
```

**æ¨¡å‹è¾“å‡º**ï¼š
```
A: SELECT Nationality FROM 1-10015132-16 WHERE Player = 'Terrence Ross'
```

#### 7. ç»éªŒæ€»ç»“ä¸å»ºè®®

1. **å†…å­˜ç®¡ç†ç­–ç•¥**ï¼š
   - é™ä½æ‰¹æ¬¡å¤§å°æ˜¯æœ€ç›´æ¥æœ‰æ•ˆçš„å†…å­˜å‡å°‘æ–¹æ³•
   - é™ä½LoRAå±‚æ•°å¯å‡å°‘å†…å­˜ï¼Œä½†ä¼šå½±å“æ¨¡å‹è´¨é‡
   - 32GBå†…å­˜è®¾å¤‡å»ºè®®æœ€å¤§ä½¿ç”¨8ä¸ªLoRAå±‚

2. **æ•°æ®é›†è€ƒé‡**ï¼š
   - è®­ç»ƒé›†å¤§å°å¯¹ç»“æœå½±å“æ˜¾è‘—
   - å¯¹äºå¤æ‚ä»»åŠ¡ï¼Œ1000æ¡æ ·æœ¬é€šå¸¸ä¸è¶³ï¼Œå»ºè®®ä½¿ç”¨5000-10000æ¡

3. **é€‚ç”¨åœºæ™¯**ï¼š
   - RAGç³»ç»Ÿè‡ªå®šä¹‰ï¼šé’ˆå¯¹ç‰¹å®šé¢†åŸŸçŸ¥è¯†å¾®è°ƒ
   - å°å‹ä¸“ä¸šæ¨¡å‹ï¼šå¦‚æ–‡æœ¬åˆ°SQLè½¬æ¢å™¨
   - å¼€å‘å’Œæµ‹è¯•ï¼šæ— éœ€é«˜ç«¯GPUå³å¯è¿›è¡ŒåŸå‹å¼€å‘

#### 8. æŠ€æœ¯ä»·å€¼ä¸æ„ä¹‰

- **ç”Ÿæ€å¤šæ ·åŒ–**ï¼šä¸ºéNVIDIAè®¾å¤‡æä¾›å¤§æ¨¡å‹å¾®è°ƒèƒ½åŠ›
- **å¼€å‘é—¨æ§›é™ä½**ï¼šé™ä½ç¡¬ä»¶è¦æ±‚ï¼Œä½¿æ›´å¤šå¼€å‘è€…èƒ½å‚ä¸å¤§æ¨¡å‹å¼€å‘
- **è‹¹æœè®¾å¤‡ä¼˜åŠ¿**ï¼šå……åˆ†åˆ©ç”¨Apple Siliconç»Ÿä¸€å†…å­˜æ¶æ„
- **åº”ç”¨åœºæ™¯æ‰©å±•**ï¼šæ”¯æŒåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šè¿›è¡Œä¸ªæ€§åŒ–æ¨¡å‹é€‚é…

MLXæ¡†æ¶ä¸ºAppleè®¾å¤‡æä¾›äº†ä¸€æ¡ä¸ä¾èµ–NVIDIA CUDAçš„å¤§æ¨¡å‹å¾®è°ƒè·¯å¾„ï¼Œè™½ç„¶åœ¨å¤„ç†å¤§è§„æ¨¡è®­ç»ƒæ—¶ä»æœ‰å±€é™ï¼Œä½†å¯¹äºç‰¹å®šä»»åŠ¡çš„é€‚åº”æ€§å¾®è°ƒå·²ç»å±•ç°å‡ºå®ç”¨ä»·å€¼ï¼Œæœªæ¥éšç€æ¡†æ¶ä¼˜åŒ–å’ŒAppleç¡¬ä»¶å‡çº§ï¼Œè¿™ä¸€æ–¹æ¡ˆçš„å¯è¡Œæ€§å°†è¿›ä¸€æ­¥å¢å¼ºã€‚

#### 9. å‚è€ƒèµ„æº

- [MLX GitHubä»“åº“](https://github.com/ml-explore/mlx)
- [MLX LoRAå¾®è°ƒç¤ºä¾‹](https://github.com/ml-explore/mlx-examples/tree/main/lora)
- [å…«ä¸€èœåˆ€ï¼šåŸºäºApple MLXæ¡†æ¶çš„M1è®¾å¤‡ä¸Šå¤§æ¨¡å‹å¾®è°ƒå®è·µ](https://mp.weixin.qq.com/s?__biz=MzA5NDc3ODQxNQ==&mid=2247484394&idx=1&sn=aeeb89f13a5269cbda8ca6f31d198b89&chksm=915cb4f81cab6148fda2f458f969edec253f789c4279ba67cd946f6d0fdbc3eb2a93f32c389a#rd)


### Appleè®¾å¤‡æ·±åº¦å­¦ä¹ æ¡†æ¶

#### MPSä¸MLXæ¡†æ¶å¯¹æ¯”

MPSåç«¯å’ŒMLXæ¡†æ¶è™½ç„¶éƒ½åˆ©ç”¨Appleç¡¬ä»¶èƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨æ¶æ„å’Œè®¾è®¡ç†å¿µä¸Šæœ‰æ˜æ˜¾åŒºåˆ«ï¼š

##### åº•å±‚ç¡¬ä»¶è°ƒç”¨
- **å…±åŒç‚¹**ï¼šäºŒè€…éƒ½ä½¿ç”¨è‹¹æœçš„Metalå›¾å½¢APIæ¥è®¿é—®è®¡ç®—èƒ½åŠ›
- **å…±åŒç‚¹**ï¼šéƒ½å……åˆ†åˆ©ç”¨äº†Apple SiliconèŠ¯ç‰‡çš„ç»Ÿä¸€å†…å­˜æ¶æ„

##### æ¶æ„å±‚æ¬¡
- **MPS (Metal Performance Shaders)**ï¼š
  - æ˜¯ä¸€ä¸ªé€‚é…å±‚ï¼Œè®©PyTorchç­‰ç°æœ‰æ¡†æ¶èƒ½åœ¨è‹¹æœç¡¬ä»¶ä¸Šè¿è¡Œ
  - æœ¬è´¨ä¸Šæ˜¯å°†ç°æœ‰æ¡†æ¶çš„è¿ç®—ç¿»è¯‘æˆMetalæŒ‡ä»¤
  - ç±»ä¼¼"ç¿»è¯‘å™¨"è§’è‰²ï¼Œå­˜åœ¨ä¸€å®šçš„è½¬æ¢å¼€é”€

- **MLXæ¡†æ¶**ï¼š
  - è‹¹æœä»å¤´å¼€å‘çš„æ¡†æ¶ï¼Œç›´æ¥ä¸ºApple Siliconæ¶æ„ä¼˜åŒ–
  - æ— éœ€"ç¿»è¯‘"å±‚ï¼Œç›´æ¥ç”Ÿæˆé’ˆå¯¹è‹¹æœç¡¬ä»¶çš„æœ€ä¼˜æŒ‡ä»¤
  - æ›´æ·±åº¦æ•´åˆäº†è‹¹æœç¡¬ä»¶çš„ç‰¹æ€§ï¼ˆç‰¹åˆ«æ˜¯ç»Ÿä¸€å†…å­˜ï¼‰

##### æŠ€æœ¯æ€§èƒ½å¯¹æ¯”

```
æ€§èƒ½ä¼˜åŒ–å±‚æ¬¡ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     åº”ç”¨ä»£ç  (Python)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  PyTorch  â”‚    MLX      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤             â”‚
â”‚ MPSåç«¯   â”‚             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚        Metal API        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    Apple Siliconç¡¬ä»¶    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- **å†…å­˜æ•ˆç‡**ï¼šMLXé€šå¸¸æ¯”PyTorch+MPSæœ‰æ›´é«˜çš„å†…å­˜åˆ©ç”¨ç‡
- **è®¡ç®—æ€§èƒ½**ï¼šMLXåœ¨ç›¸åŒç¡¬ä»¶ä¸Šé€šå¸¸èƒ½æä¾›æ›´å¥½çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨çŸ©é˜µè¿ç®—ä¸Š
- **å¯åŠ¨æ—¶é—´**ï¼šMLXé€šå¸¸æœ‰æ›´å¿«çš„å¯åŠ¨å’Œç¼–è¯‘æ—¶é—´

#### Apple Metalè®¡ç®—èµ„æºç®¡ç†

Metalä¸ä»…é™äºGPUï¼Œå®ƒæ˜¯Appleè®¾è®¡çš„åº•å±‚å›¾å½¢å’Œè®¡ç®—APIï¼Œèƒ½å¤Ÿè®¿é—®Apple SiliconèŠ¯ç‰‡ä¸Šçš„å¤šç§è®¡ç®—èµ„æºï¼š

##### Metalçš„å…¨é¢è®¡ç®—èƒ½åŠ›
- **CPU**ï¼šåŒ…æ‹¬æ€§èƒ½æ ¸å¿ƒå’Œèƒ½æ•ˆæ ¸å¿ƒ
- **GPU**ï¼šé›†æˆçš„å›¾å½¢å¤„ç†å•å…ƒ
- **ç¥ç»å¼•æ“(Neural Engine)**ï¼šä¸“ç”¨äºæœºå™¨å­¦ä¹ åŠ é€Ÿçš„ç¡¬ä»¶
- **AMX(Apple Matrix accelerators)**ï¼šçŸ©é˜µè®¡ç®—åŠ é€Ÿå™¨
- **åª’ä½“å¼•æ“**ï¼šè§†é¢‘ç¼–è§£ç ä¸“ç”¨å¤„ç†å•å…ƒ

##### è‡ªåŠ¨èµ„æºåˆ†é…èƒ½åŠ›

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           åº”ç”¨ç¨‹åº (MLX/PyTorchç­‰)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Metalæ¡†æ¶                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“           â†“            â†“         â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   CPU   â”‚ â”‚   GPU   â”‚ â”‚ç¥ç»å¼•æ“  â”‚ â”‚  AMX    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Metalçš„ä»»åŠ¡åˆ†é…ç­–ç•¥ï¼š

1. **æ™ºèƒ½è°ƒåº¦**ï¼šæ ¹æ®å·¥ä½œè´Ÿè½½ç‰¹æ€§è‡ªåŠ¨é€‰æ‹©é€‚å½“çš„è®¡ç®—å•å…ƒ
2. **å¼‚æ„è®¡ç®—**ï¼šå¯ä»¥åŒæ—¶åˆ©ç”¨å¤šç§è®¡ç®—èµ„æºååŒå·¥ä½œ
3. **ä¸“ç”¨åŠ é€Ÿ**ï¼šç‰¹å®šè¿ç®—ä¼šè‡ªåŠ¨è·¯ç”±åˆ°æœ€é€‚åˆçš„ç¡¬ä»¶å•å…ƒ

##### æ¡†æ¶å·®å¼‚åœ¨èµ„æºåˆ©ç”¨ä¸Š

- **MPS(Metal Performance Shaders)**ï¼š
  - ä¸»è¦ä¼˜åŒ–GPUè®¡ç®—ï¼Œå¯¹å…¶ä»–è®¡ç®—å•å…ƒæ”¯æŒæœ‰é™
  - é€šè¿‡Metalè®¿é—®ç¡¬ä»¶ï¼Œä½†ä¼˜åŒ–å±‚æ¬¡è¾ƒæµ…

- **MLX**ï¼š
  - æ·±åº¦æ•´åˆæ‰€æœ‰è®¡ç®—èµ„æºï¼ŒåŒ…æ‹¬CPUã€GPUã€ç¥ç»å¼•æ“
  - æ›´æ™ºèƒ½åœ°åˆ†é…ä¸åŒç±»å‹çš„è®¡ç®—ä»»åŠ¡åˆ°æœ€é€‚åˆçš„ç¡¬ä»¶
  - ä¸“ä¸ºæœºå™¨å­¦ä¹ ä¼˜åŒ–ï¼Œäº†è§£ä¸åŒè®¡ç®—æ¨¡å¼çš„ç‰¹ç‚¹

åœ¨è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹æ—¶ï¼ŒMetalèƒ½å¤Ÿè‡ªåŠ¨å°†ä¸åŒç±»å‹çš„è®¡ç®—ä»»åŠ¡åˆ†é…ç»™æœ€é€‚åˆçš„ç¡¬ä»¶å•å…ƒï¼š
- çŸ©é˜µä¹˜æ³•å¯èƒ½è·¯ç”±åˆ°GPUæˆ–AMX
- æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥åˆ©ç”¨GPUçš„å¹¶è¡Œèƒ½åŠ›
- æ¿€æ´»å‡½æ•°å¯èƒ½åœ¨CPUä¸Šæ‰§è¡Œ
- ç‰¹å®šMLç®—å­å¯ä»¥åˆ©ç”¨ç¥ç»å¼•æ“åŠ é€Ÿ

è¿™ç§å…¨é¢çš„è®¡ç®—èµ„æºè°ƒåº¦æ˜¯Appleè®¾å¤‡ä¸Šèƒ½å¤Ÿè¿›è¡Œå¤§æ¨¡å‹å¾®è°ƒçš„å…³é”®æŠ€æœ¯åŸºç¡€ï¼Œä¹Ÿæ˜¯ä¸ºä»€ä¹ˆå³ä½¿æ²¡æœ‰é«˜ç«¯NVIDIA GPUï¼ŒAppleè®¾å¤‡ä»èƒ½æœ‰æ•ˆæ”¯æŒæœºå™¨å­¦ä¹ å·¥ä½œè´Ÿè½½çš„æ ¹æœ¬åŸå› ã€‚

### Macè®¾å¤‡ä¸Šå¾®è°ƒå¤§è¯­è¨€æ¨¡å‹çš„é€šç”¨æ³¨æ„äº‹é¡¹

é€šè¿‡åˆ†æä¸Šè¿°ä¸¤ç¯‡å…³äºMacè®¾å¤‡ä¸Šå¾®è°ƒå¤§æ¨¡å‹çš„å®è·µæ–‡ç« (DeepSeek+MPSä¸MLXæ¡†æ¶)ï¼Œå¯ä»¥æ€»ç»“å‡ºä»¥ä¸‹é€šç”¨çš„å…³é”®æ³¨æ„äº‹é¡¹ï¼š

#### 1. å†…å­˜ç®¡ç†æ˜¯æ ¸å¿ƒæŒ‘æˆ˜

- **å†…å­˜æ°´ä½æ§åˆ¶**ï¼šè®¾ç½®`PYTORCH_MPS_HIGH_WATERMARK_RATIO`å’Œ`PYTORCH_MPS_LOW_WATERMARK_RATIO`å‚æ•°æ§åˆ¶å†…å­˜ä½¿ç”¨ä¸Šé™
- **æ‰‹åŠ¨æ¸…ç†ç¼“å­˜**ï¼šå®šæœŸä½¿ç”¨`torch.mps.empty_cache()`æˆ–MLXç­‰æ•ˆå‘½ä»¤æ¸…ç†GPUå†…å­˜
- **å†…å­˜æ³„æ¼ç›‘æ§**ï¼šç‰¹åˆ«å…³æ³¨MPSåç«¯ç¼“å­˜ç§¯ç´¯ã€å¼ é‡ç¢ç‰‡åŒ–é—®é¢˜ï¼Œé•¿æ—¶é—´è®­ç»ƒæ—¶å°¤ä¸ºé‡è¦

#### 2. è®­ç»ƒå‚æ•°ä¼˜åŒ–

- **æ‰¹æ¬¡å¤§å°é™ä½**ï¼šå°†batch sizeé™è‡³1ï¼Œè¿™æ˜¯æœ€ç›´æ¥æœ‰æ•ˆçš„å†…å­˜èŠ‚çœæ–¹æ³•
- **æ¢¯åº¦ç´¯ç§¯**ï¼šä½¿ç”¨åˆç†çš„`gradient_accumulation_steps`(å¦‚8-16)æ¥æ¨¡æ‹Ÿå¤§æ‰¹é‡è®­ç»ƒæ•ˆæœï¼Œä½†ä¸å¿…è¿‡é«˜(å¦‚64)ï¼Œè¿‡é«˜ä¼šå½±å“æ”¶æ•›é€Ÿåº¦å’Œè®­ç»ƒæ•ˆç‡
- **å¾®è°ƒå±‚æ•°é™åˆ¶**ï¼š
  - é’ˆå¯¹ç‰¹å®šå…³é”®å±‚(q_proj, v_proj, k_proj, o_proj, gate_proj, up_proj, down_proj)è¿›è¡ŒLoRAå¾®è°ƒ
  - 32GBå†…å­˜è®¾å¤‡å¯ä»¥å°è¯•ä½¿ç”¨æ›´å¤šå±‚å’Œæ›´å¤§rå€¼(å¦‚r=16)ï¼Œ16GBè®¾å¤‡å»ºè®®4å±‚æˆ–æ›´å°‘ä¸”rå€¼æ›´å°(å¦‚r=8)

#### 3. æŠ€æœ¯ç­–ç•¥å…±è¯†

- **æ¢¯åº¦æ£€æŸ¥ç‚¹**ï¼šé‡‡ç”¨`gradient_checkpointing=True`ä»¥å¹³è¡¡æ€§èƒ½å’Œå†…å­˜å ç”¨
- **ä¸“æ³¨äºLoRA**ï¼šLoRAæŠ€æœ¯æ˜¯ç›®å‰Macè®¾å¤‡ä¸Šå¾®è°ƒå¤§æ¨¡å‹çš„æœ€ä½³é€‰æ‹©ï¼Œæ·»åŠ å‚æ•°å°‘ã€å†…å­˜å ç”¨ä½
- **æ•°æ®ç±»å‹ç®¡ç†**ï¼šåœ¨MPSåç«¯ä¸Š**å¿…é¡»ç¦ç”¨fp16å’Œbf16**ï¼Œç»Ÿä¸€ä½¿ç”¨float32ä»¥é¿å…æ•°æ®ç±»å‹ä¸åŒ¹é…é”™è¯¯ï¼Œè¿™ç‚¹ä¸CUDAç¯å¢ƒä¸åŒ
- **æ•°æ®é›†è§„æ¨¡**ï¼šMacè®¾å¤‡ä¸Šå¾®è°ƒéœ€è°¨æ…æ§åˆ¶æ•°æ®é›†å¤§å°ï¼Œå»ºè®®æ ¹æ®ä»»åŠ¡å¤æ‚åº¦é€‰æ‹©é€‚å½“æ•°é‡ï¼Œåˆå§‹éªŒè¯æ—¶å¯ä»¥åªä½¿ç”¨å°‘é‡æ ·æœ¬(å¦‚200æ¡)

#### 4. æ€§èƒ½ä¸è´¨é‡å¹³è¡¡

- **è®­ç»ƒä¸æ¨ç†æƒè¡¡**ï¼šéœ€è¦åœ¨è®­ç»ƒæ•ˆç‡ä¸æ¨¡å‹æ•ˆæœé—´åšå‡ºæƒè¡¡ï¼Œå‚æ•°è¶Šå°è®­ç»ƒè¶Šå¿«ä½†æ•ˆæœå¯èƒ½è¾ƒå·®
- **è®­ç»ƒæ—¶é—´é¢„æœŸ**ï¼šç›¸æ¯”NVIDIA GPUï¼Œéœ€è¦æ¥å—æ›´é•¿çš„è®­ç»ƒæ—¶é—´
- **æ¨¡å‹è§„æ¨¡é€‰æ‹©**ï¼š
  - 16GBå†…å­˜è®¾å¤‡ï¼šæœ€å¤§å»ºè®®7Bæ¨¡å‹
  - 32GBå†…å­˜è®¾å¤‡ï¼šå¯ä»¥å¾®è°ƒ14Bçº§åˆ«æ¨¡å‹ï¼Œä½†éœ€è°¨æ…è°ƒæ•´å‚æ•°
  - 64GBåŠä»¥ä¸Šå†…å­˜è®¾å¤‡ï¼šå¯æ›´ç¨³å®šåœ°å¾®è°ƒ14Bä»¥ä¸Šæ¨¡å‹

#### 5. æ¡†æ¶é€‰æ‹©è€ƒé‡

- **PyTorch+PEFT**ï¼šæœ¬æ–‡å®è·µé‡‡ç”¨çš„æ–¹æ¡ˆï¼Œå…¼å®¹æ€§æ›´å¥½ï¼Œç”Ÿæ€æ›´ä¸°å¯Œï¼Œä½†éœ€è¦é’ˆå¯¹MPSåç«¯è¿›è¡Œå¤šé¡¹å…¼å®¹æ€§ä¿®å¤
- **MLXæ¡†æ¶**ï¼šè‹¹æœä¸“é—¨å¼€å‘çš„æ›¿ä»£æ–¹æ¡ˆï¼Œé’ˆå¯¹Apple Siliconä¼˜åŒ–ï¼Œæ€§èƒ½å¯èƒ½æ›´å¥½ï¼Œä½†ç”Ÿæ€è¾ƒæ–°ï¼Œå·¥å…·é“¾ä¸å¤Ÿä¸°å¯Œ
- **ç»Ÿä¸€å†…å­˜ä¼˜åŠ¿**ï¼šä¸¤ç§æ–¹æ³•éƒ½èƒ½å……åˆ†åˆ©ç”¨Apple SiliconèŠ¯ç‰‡çš„ç»Ÿä¸€å†…å­˜æ¶æ„

#### 6. å®ç”¨å»ºè®®

- **é¢„çƒ­é˜¶æ®µ**ï¼šä¸ºæ¨¡å‹è®¾ç½®é€‚å½“çš„é¢„çƒ­æ­¥æ•°(5-10æ­¥)ä»¥ç¨³å®šåˆå§‹è®­ç»ƒ
- **KVç¼“å­˜ä¸æ¢¯åº¦æ£€æŸ¥ç‚¹å†²çª**ï¼šåœ¨è®­ç»ƒæ—¶å¿…é¡»è®¾ç½®`model.config.use_cache = False`ä»¥é¿å…ä¸æ¢¯åº¦æ£€æŸ¥ç‚¹å†²çª
- **æ™ºèƒ½æ¨ç†å›é€€**ï¼šå®ç°æ™ºèƒ½æ¨ç†å‡½æ•°ï¼Œåœ¨MPSåç«¯å¤±è´¥æ—¶è‡ªåŠ¨å›é€€åˆ°CPU
- **åŠŸè€—ç®¡ç†**ï¼šæ³¨æ„Macè®¾å¤‡åœ¨é•¿æ—¶é—´è®­ç»ƒä¸­çš„æ•£çƒ­é—®é¢˜ï¼Œå¯èƒ½éœ€è¦å¤–éƒ¨æ•£çƒ­æ”¯æŒ
- **åˆ†æ‰¹è®­ç»ƒ**ï¼šå°†é•¿æ—¶é—´è®­ç»ƒåˆ†ä¸ºå¤šä¸ªçŸ­è®­ç»ƒé˜¶æ®µï¼Œå®šæœŸä¿å­˜æ£€æŸ¥ç‚¹ï¼Œé¿å…å› å†…å­˜é—®é¢˜è®­ç»ƒå¤±è´¥
- **è·¯å¾„å¤„ç†æ­£ç¡®æ€§**ï¼šåœ¨Pythonè„šæœ¬ä¸­ä½¿ç”¨`os.path.expanduser()`å±•å¼€åŒ…å«`~`çš„è·¯å¾„ï¼Œé¿å…è·¯å¾„é”™è¯¯

Macè®¾å¤‡ä¸Šçš„å¤§æ¨¡å‹å¾®è°ƒè™½ç„¶é¢ä¸´èµ„æºé™åˆ¶ï¼Œä½†é€šè¿‡åˆç†çš„å‚æ•°è®¾ç½®å’Œä¼˜åŒ–ç­–ç•¥ï¼Œå®Œå…¨å¯ä»¥å®ç°æœ‰æ•ˆçš„å¾®è°ƒï¼Œå°¤å…¶é€‚åˆåŸå‹å¼€å‘ã€æ¦‚å¿µéªŒè¯å’Œç‰¹å®šé¢†åŸŸçš„å°è§„æ¨¡å®šåˆ¶åŒ–éœ€æ±‚ã€‚

