# åœ¨macOS (Apple Silicon)ä¸Šå¾®è°ƒDeepSeek-R1-Distill-Qwen-14BæŒ‡å—

æœ¬æŒ‡å—å°†å¸®åŠ©æ‚¨åœ¨macOSï¼ˆApple Siliconï¼‰è®¾å¤‡ä¸Šä½¿ç”¨Hugging Face PEFTåº“å¾®è°ƒDeepSeek-R1-Distill-Qwen-14Bæ¨¡å‹ã€‚åŸæŒ‡å—åŸºäºä½¿ç”¨Unslothæ¡†æ¶åœ¨RTX 4090ä¸Šå¾®è°ƒï¼Œä½†ç”±äºUnslothç›®å‰ä¸æ”¯æŒApple Siliconçš„MPSåç«¯ï¼Œæœ¬æŒ‡å—å·²é’ˆå¯¹Apple Siliconç¯å¢ƒåšäº†å…¨é¢è°ƒæ•´ï¼Œæ”¹ç”¨æ ‡å‡†PEFTåº“å®ç°ã€‚

## ä¸€ã€ç¡¬ä»¶è¦æ±‚ä¸å‡†å¤‡

### æ¨èé…ç½®
- æœºå‹ï¼šM2 Pro/Maxæˆ–æ›´é«˜é…ç½®çš„Mac
- å†…å­˜ï¼šè‡³å°‘32GBï¼Œå»ºè®®64GBä»¥ä¸Š
- å­˜å‚¨ï¼šè‡³å°‘100GBå¯ç”¨ç©ºé—´
- macOSç‰ˆæœ¬ï¼šVentura (13.0)æˆ–æ›´æ–°ç‰ˆæœ¬

> **æ³¨æ„**ï¼š14Bå‚æ•°æ¨¡å‹åœ¨macOSä¸Šæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œè¯·ç¡®ä¿æ‚¨æœ‰è¶³å¤Ÿå†…å­˜å¹¶åšå¥½æ€§èƒ½é¢„æœŸã€‚æ­¤å¤–ï¼ŒDeepSeek-R1-Distill-Qwenç³»åˆ—æ¨¡å‹çš„æŸäº›æ“ä½œå¯èƒ½ä¸MPSåç«¯å­˜åœ¨å…¼å®¹æ€§é—®é¢˜ï¼Œä¸‹æ–‡æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚

> **âš ï¸ ç‰¹åˆ«æé†’**ï¼šMPSåç«¯åœ¨Hugging Faceçš„Accelerateåº“ä¸­**ä¸è¢«è¯†åˆ«ä¸ºæ”¯æŒfp16æ··åˆç²¾åº¦è®­ç»ƒ**çš„è®¾å¤‡ã€‚æœ¬æŒ‡å—å·²é’ˆå¯¹æ­¤é—®é¢˜æä¾›äº†è§£å†³æ–¹æ¡ˆï¼Œç¡®ä¿å°†è®­ç»ƒå‚æ•°ä¸­çš„`fp16=False`ã€‚

## äºŒã€ç¯å¢ƒé…ç½®

### 1. å®‰è£…å¿…è¦çš„è½¯ä»¶

```bash
# å®‰è£…Homebrewï¼ˆå¦‚æœå°šæœªå®‰è£…ï¼‰
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

# å®‰è£…Pythonç›¸å…³å·¥å…·
brew install python@3.11 cmake

# å®‰è£…git-lfsï¼ˆç”¨äºæ¨¡å‹ä¸‹è½½ï¼‰
brew install git-lfs
git lfs install
```

### 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

```bash
# åˆ›å»ºé¡¹ç›®ç›®å½•
mkdir -p ~/Documents/llm-finetune
cd ~/Documents/llm-finetune

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3 -m venv peft-env
source peft-env/bin/activate
```

### 3. å®‰è£…ä¾èµ–

```bash
# ç¡®ä¿pipæ˜¯æœ€æ–°ç‰ˆæœ¬
pip install --upgrade pip

# å®‰è£…PyTorch with MPSæ”¯æŒ
pip install torch torchvision torchaudio

# å®‰è£…NumPy 1.xç‰ˆæœ¬è§£å†³å…¼å®¹æ€§é—®é¢˜
pip install numpy==1.26.4

# å®‰è£…æ ¸å¿ƒä¾èµ–åº“
pip install transformers datasets peft accelerate huggingface_hub wandb psutil
```

#### PyTorchå®‰è£…è¯¦è§£

ä¸Šè¿°å‘½ä»¤ `pip install torch torchvision torchaudio` ä¸€æ¬¡æ€§å®‰è£…äº†ä¸‰ä¸ªç›¸å…³çš„PythonåŒ…ï¼š

1. **torch**ï¼š
   - PyTorchçš„æ ¸å¿ƒåº“
   - ç”±Meta(åŸFacebook)å¼€å‘çš„æµè¡Œæ·±åº¦å­¦ä¹ æ¡†æ¶
   - æä¾›å¼ é‡è®¡ç®—å’ŒåŠ¨æ€ç¥ç»ç½‘ç»œåŠŸèƒ½

2. **torchvision**ï¼š
   - PyTorchçš„è®¡ç®—æœºè§†è§‰æ‰©å±•åº“
   - åŒ…å«å¸¸ç”¨çš„æ•°æ®é›†ã€æ¨¡å‹æ¶æ„å’Œå›¾åƒå¤„ç†å·¥å…·

3. **torchaudio**ï¼š
   - PyTorchçš„éŸ³é¢‘å¤„ç†æ‰©å±•åº“
   - æä¾›éŸ³é¢‘åŠ è½½ã€å¤„ç†å’Œç‰¹å¾æå–åŠŸèƒ½

**åœ¨Apple Siliconä¸Š**ï¼Œæ­¤å‘½ä»¤ä¼šè‡ªåŠ¨å®‰è£…æ”¯æŒMPS (Metal Performance Shaders)çš„PyTorchç‰ˆæœ¬ï¼Œä½¿å¾—æ·±åº¦å­¦ä¹ ä»»åŠ¡èƒ½å¤Ÿåˆ©ç”¨Mç³»åˆ—èŠ¯ç‰‡çš„GPUåŠ é€Ÿã€‚

> âš ï¸ **é‡è¦è¯´æ˜**ï¼šæœ¬æŒ‡å—ç›®å‰ä½¿ç”¨æ ‡å‡†çš„Hugging Face PEFTåº“è¿›è¡ŒLoRAå¾®è°ƒï¼Œè€ŒéUnslothã€‚è¿™æ˜¯å› ä¸ºç»è¿‡æµ‹è¯•ï¼Œ**Unslothç›®å‰ä¸å®Œå…¨æ”¯æŒApple Siliconçš„MPSåç«¯**ã€‚

### 4. åŸå§‹æ–¹æ³•ï¼šUnslothå®‰è£…ï¼ˆå‚è€ƒï¼Œä¸é€‚ç”¨äºApple Siliconï¼‰

<details>
<summary>ğŸ‘‰ å±•å¼€æŸ¥çœ‹åŸå§‹Unslothå®‰è£…æ–¹æ³•ï¼ˆä»…ä¾›å‚è€ƒï¼Œä¸æ¨èä½¿ç”¨ï¼‰</summary>

```bash
# å…‹éš†Unsloth
git clone https://github.com/unslothai/unsloth.git
cd unsloth

# å®‰è£…Unslothï¼ˆApple Siliconä¼˜åŒ–ç‰ˆæœ¬ï¼‰
# æ³¨æ„ï¼šåœ¨zshä¸­éœ€è¦ç”¨å¼•å·é¿å…æ–¹æ‹¬å·è¢«è§£æä¸ºé€šé…ç¬¦
pip install ".[mps]"

# å¦‚æœå‡ºç° "zsh: no matches found: .[mps]" é”™è¯¯ï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ä¹‹ä¸€ï¼š
# pip install ".[mps]"    # ä½¿ç”¨å¼•å·
# pip install .\[mps\]    # ä½¿ç”¨è½¬ä¹‰ç¬¦
# pip install -e . --extra-index-url="mps"  # ä½¿ç”¨-eé€‰é¡¹
```

#### å…³äºUnslothå®‰è£…æ–¹å¼çš„è¯´æ˜

ä¸Šè¿°å®‰è£…æ–¹å¼ä½¿ç”¨äº†æºç å®‰è£…çš„æ–¹æ³•ï¼Œè€Œä¸æ˜¯ç›´æ¥é€šè¿‡pipå®‰è£…ã€‚è¿™æ ·åšçš„åŸå› æœ‰ï¼š

1. **è·å–æœ€æ–°MPSæ”¯æŒ**ï¼š
   - ç›´æ¥ä»GitHubè·å–æœ€æ–°çš„æºç ï¼ŒåŒ…å«å¯¹Apple Siliconçš„æœ€æ–°ä¼˜åŒ–
   - å¯ä»¥è®¿é—®å¯èƒ½å°šæœªåœ¨PyPIæ­£å¼å‘å¸ƒçš„åŠŸèƒ½

2. **å¯ç¼–è¾‘æ¨¡å¼**ï¼š
   - å®‰è£…åœ¨"å¯ç¼–è¾‘æ¨¡å¼"ï¼Œå…è®¸åœ¨éœ€è¦æ—¶ä¿®æ”¹æºç 
   - å¯¹äºè°ƒè¯•å’Œè‡ªå®šä¹‰å¾®è°ƒæµç¨‹å¾ˆæœ‰å¸®åŠ©

3. **ç‰ˆæœ¬çµæ´»æ€§**ï¼š
   - å¯ä»¥é€šè¿‡Gitåˆ‡æ¢åˆ°ç‰¹å®šç‰ˆæœ¬æˆ–åˆ†æ”¯

**æ›¿ä»£æ–¹æ¡ˆ**ï¼šæ‚¨ä¹Ÿå¯ä»¥ç›´æ¥é€šè¿‡pipå®‰è£…ï¼š
```bash
# ç›´æ¥ä»PyPIå®‰è£…ï¼ˆç®€å•ä½†å¯èƒ½ä¸æ˜¯æœ€æ–°ç‰ˆæœ¬ï¼‰
pip install unsloth[mps]
```

**âš ï¸ æç¤ºï¼š** ç»æµ‹è¯•ï¼Œè™½ç„¶Unslothå¯ä»¥åœ¨MacOSä¸Šå®‰è£…ï¼Œä½†å®é™…ä½¿ç”¨æ—¶ä¼šé‡åˆ°å…¼å®¹æ€§é—®é¢˜ï¼Œå› æ­¤æœ¬æŒ‡å—é‡‡ç”¨æ ‡å‡†PEFTåº“å®ç°LoRAå¾®è°ƒã€‚
</details>

### 5. å¸¸è§ç¯å¢ƒé—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

#### Pythonç‰ˆæœ¬å†²çªé—®é¢˜

å¦‚æœæ‚¨åœ¨å®‰è£…Python 3.11æ—¶é‡åˆ°ç±»ä¼¼ä»¥ä¸‹é”™è¯¯ï¼š

```
Error: The `brew link` step did not complete successfully
Could not symlink bin/2to3-3.11
Target /usr/local/bin/2to3-3.11 already exists.
```

è¿™è¡¨æ˜ç³»ç»Ÿä¸­å­˜åœ¨å¤šä¸ªPythonç‰ˆæœ¬å†²çªã€‚è§£å†³æ–¹æ³•ï¼š

1. **ä½¿ç”¨ç‰¹å®šç‰ˆæœ¬è·¯å¾„**ï¼š
   ```bash
   # ä½¿ç”¨Homebrewå®‰è£…çš„Python 3.11çš„å®Œæ•´è·¯å¾„åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
   /usr/local/Cellar/python@3.11/3.11.11/bin/python3.11 -m venv unsloth-env
   ```

2. **å¼ºåˆ¶åˆ›å»ºé“¾æ¥**ï¼ˆå¯é€‰ï¼‰ï¼š
   ```bash
   brew link --overwrite python@3.11
   ```

3. **éªŒè¯æ­£åœ¨ä½¿ç”¨çš„Pythonç‰ˆæœ¬**ï¼š
   ```bash
   python --version  # åº”æ˜¾ç¤º3.11.x
   ```

#### è™šæ‹Ÿç¯å¢ƒä½¿ç”¨é¡»çŸ¥

è¯·æ³¨æ„ï¼Œæ¿€æ´»è™šæ‹Ÿç¯å¢ƒï¼ˆ`source unsloth-env/bin/activate`ï¼‰**ä»…å¯¹å½“å‰ç»ˆç«¯ä¼šè¯æœ‰æ•ˆ**ï¼š

- å…³é—­ç»ˆç«¯çª—å£åï¼Œæ¿€æ´»çŠ¶æ€ä¼šä¸¢å¤±
- æ–°å¼€çš„ç»ˆç«¯çª—å£éœ€è¦é‡æ–°æ¿€æ´»ç¯å¢ƒ
- å¤šä¸ªç»ˆç«¯çª—å£éœ€è¦åˆ†åˆ«æ¿€æ´»

**å®ç”¨å»ºè®®**ï¼š

1. **åˆ›å»ºåˆ«å**ï¼šåœ¨`.zshrc`ä¸­æ·»åŠ ï¼š
   ```bash
   echo 'alias activate-unsloth="cd ~/Documents/unsloth-finetune && source unsloth-env/bin/activate"' >> ~/.zshrc
   ```

2. **åˆ›å»ºå¯åŠ¨è„šæœ¬**ï¼š
   ```bash
   echo '#!/bin/bash
   cd ~/Documents/unsloth-finetune
   source unsloth-env/bin/activate
   exec "$@"' > ~/start-unsloth.sh
   chmod +x ~/start-unsloth.sh
   ```
   ä½¿ç”¨ï¼š`~/start-unsloth.sh python your_script.py`

#### PyTorchä¸‹è½½è¶…æ—¶é—®é¢˜

å¦‚æœæ‚¨åœ¨å®‰è£…PyTorchæ—¶é‡åˆ°ç±»ä¼¼ä»¥ä¸‹é”™è¯¯:

```
HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.
```

è¿™é€šå¸¸æ˜¯å› ä¸ºPyTorchåŒ…ä½“ç§¯è¾ƒå¤§(çº¦150MB)ï¼Œåœ¨ç½‘ç»œçŠ¶å†µä¸ä½³æ—¶å®¹æ˜“ä¸‹è½½è¶…æ—¶ã€‚è§£å†³æ–¹æ³•:

1. **ä½¿ç”¨é•œåƒæºå®‰è£…**:
   ```bash
   pip install torch torchvision torchaudio -i https://pypi.tuna.tsinghua.edu.cn/simple
   ```

2. **å¢åŠ pipè¶…æ—¶æ—¶é—´**:
   ```bash
   pip --default-timeout=1000 install torch torchvision torchaudio
   ```

3. **åˆ†æ­¥ä¸‹è½½**:
   ```bash
   # å…ˆåªå®‰è£…torchæ ¸å¿ƒåº“
   pip install torch -i https://pypi.tuna.tsinghua.edu.cn/simple
   # æˆåŠŸåå†å®‰è£…å…¶ä»–ç»„ä»¶
   pip install torchvision torchaudio -i https://pypi.tuna.tsinghua.edu.cn/simple
   ```

4. **ç¡®è®¤Apple Siliconå…¼å®¹æ€§**:
   å¯¹äºMç³»åˆ—èŠ¯ç‰‡ï¼Œç¡®ä¿ä¸‹è½½çš„æ˜¯ARMæ¶æ„çš„åŒ…:
   ```bash
   pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu
   ```

## ä¸‰ã€ä¸‹è½½æ¨¡å‹ä¸æ•°æ®

### 1. è®¾ç½®HuggingFaceç¼“å­˜ç›®å½•

```bash
# åˆ›å»ºç¼“å­˜ç›®å½•
mkdir -p ~/Documents/models
mkdir -p ~/Documents/datasets

# è®¾ç½®ç¯å¢ƒå˜é‡
export HF_HOME=~/Documents/models
export TRANSFORMERS_CACHE=~/Documents/models
export HUGGINGFACE_HUB_CACHE=~/Documents/models
```

### 2. ä¸‹è½½æ¨¡å‹

#### å‡†å¤‡å·¥ä½œï¼šå®‰è£…å¿…è¦çš„åº“

```bash
# æ ¹æ®æ‚¨é€‰æ‹©çš„ä¸‹è½½æ–¹å¼ï¼Œå®‰è£…å¯¹åº”çš„åº“
# æ–¹å¼ä¸€ï¼šModelScopeï¼ˆéœ€è¦å®‰è£…modelscopeåº“ï¼‰
pip install modelscope

# æ–¹å¼äºŒï¼šHuggingFace APIï¼ˆéœ€è¦å®‰è£…huggingface_hubï¼‰
pip install huggingface_hub
```

> **âš ï¸ é‡è¦è­¦å‘Š**ï¼šåœ¨Pythonè„šæœ¬ä¸­ï¼Œ`~/` è¿™æ ·çš„è·¯å¾„ç®€å†™**ä¸ä¼š**è¢«è‡ªåŠ¨å±•å¼€ä¸ºç”¨æˆ·ä¸»ç›®å½•ï¼å¿…é¡»ä½¿ç”¨ç»å¯¹è·¯å¾„æˆ–`os.path.expanduser()`å‡½æ•°è¿›è¡Œè½¬æ¢ï¼Œå¦åˆ™ä¼šå¯¼è‡´è·¯å¾„é”™è¯¯ï¼

é€‰æ‹©ä»¥ä¸‹ä¸¤ç§æ–¹å¼ä¹‹ä¸€ï¼š

#### ä½¿ç”¨ModelScopeï¼ˆå¦‚æœéœ€è¦é•œåƒåŠ é€Ÿï¼‰
```python
# åˆ›å»ºå¹¶è¿è¡ŒPythonè„šæœ¬
cat > download_model.py << 'EOF'
import os
from modelscope import snapshot_download

# æ­£ç¡®: ä½¿ç”¨os.path.expanduserå±•å¼€ç”¨æˆ·ä¸»ç›®å½•
model_cache_dir = os.path.expanduser('~/Documents/models')

# ä¸‹è½½æ¨¡å‹åˆ°å±•å¼€åçš„è·¯å¾„
snapshot_download('deepseek-ai/DeepSeek-R1-Distill-Qwen-14B', cache_dir=model_cache_dir)

# é”™è¯¯ç¤ºä¾‹ - ä¸è¦è¿™æ ·åš: âŒ
# snapshot_download('deepseek-ai/DeepSeek-R1-Distill-Qwen-14B', cache_dir='~/Documents/models')
EOF

python download_model.py
```

#### æˆ–ä½¿ç”¨HuggingFace API
```bash
# è®¾ç½®é•œåƒï¼ˆå¯é€‰ï¼Œå¦‚æœä¸‹è½½æ…¢ï¼‰
export HF_ENDPOINT=https://hf-mirror.com

# ä¸‹è½½æ¨¡å‹ - ç”¨ç»å¯¹è·¯å¾„æˆ–os.path.expanduser()
python -c "import os; from huggingface_hub import snapshot_download; snapshot_download('deepseek-ai/DeepSeek-R1-Distill-Qwen-14B', cache_dir=os.path.expanduser('~/Documents/models'))"
```

### 3. ä¸‹è½½åŒ»å­¦æ•°æ®é›†

```bash
# è®¾ç½®é•œåƒï¼ˆå¯é€‰ï¼‰
export HF_ENDPOINT=https://hf-mirror.com

# ä¸‹è½½æ•°æ®é›†
huggingface-cli download --resume-download --repo-type dataset FreedomIntelligence/medical-o1-reasoning-SFT --local-dir ~/Documents/datasets/medical-o1-reasoning-SFT
```

### 4. éªŒè¯æ•°æ®é›†

```bash
# æŸ¥çœ‹æ•°æ®é›†çš„æœ€åä¸€æ¡è®°å½•
tail -6 ~/Documents/datasets/medical-o1-reasoning-SFT/medical_o1_sft_Chinese.json
```


## å››ã€å¾®è°ƒè„šæœ¬é…ç½®

**Unsloth ç›®å‰ä¸æ”¯æŒ Apple Silicon çš„ MPS åç«¯**ã€‚æˆ‘ä»¬éœ€è¦ä½¿ç”¨æ ‡å‡†çš„ Hugging Face PEFT (Parameter-Efficient Fine-Tuning) åº“æ¥è¿›è¡Œ LoRA å¾®è°ƒã€‚

> **âš ï¸æç¤º**ï¼šåœ¨Pythonè„šæœ¬ä¸­å¤„ç†æ–‡ä»¶è·¯å¾„åº”ä½¿ç”¨`os.path.expanduser()`å‡½æ•°å¤„ç†åŒ…å«`~`çš„è·¯å¾„ï¼Œå¦‚æœ¬ç¤ºä¾‹ä¸­æ‰€ç¤ºã€‚æ­¤ç±»è·¯å¾„é”™è¯¯æ˜¯å¯¼è‡´æ¨¡å‹åŠ è½½å¤±è´¥çš„æœ€å¸¸è§åŸå› ä¹‹ä¸€ã€‚


é¦–å…ˆï¼Œå®‰è£…å¿…è¦çš„åº“ï¼š

```bash
# å®‰è£…æ ¸å¿ƒåº“ 
pip install transformers datasets peft accelerate wandb

# æ³¨æ„ï¼šNumPyç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜ - ç¡®ä¿å®‰è£…1.xç³»åˆ—ç‰ˆæœ¬ä»¥é¿å…å…¼å®¹æ€§é”™è¯¯
pip install numpy==1.26.4
```


å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„è„šæœ¬åšä¸€æ¬¡åœ¨å¾®è°ƒå‰çš„æ¨ç†ï¼š


```python
import torch
import time
import os
import gc
import threading
from transformers import AutoModelForCausalLM, AutoTokenizer

print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"MPSå¯ç”¨: {torch.backends.mps.is_available()}")

# ä¼˜åŒ–MPSæ€§èƒ½çš„ç¯å¢ƒå˜é‡
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"  
os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"  
os.environ["PYTORCH_MPS_LOW_WATERMARK_RATIO"] = "0.0"   

# æ¸…ç†å†…å­˜
gc.collect()
torch.mps.empty_cache()

# æ¨¡å‹è·¯å¾„
model_path = "/Users/yangjiefeng/Documents/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B"

print("====== é˜¶æ®µ1: åŠ è½½æ¨¡å‹ ======")
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token

print("å¼€å§‹åŠ è½½æ¨¡å‹...")
start_time = time.time()

model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float16,  # ä½¿ç”¨float16å‡å°‘å†…å­˜å ç”¨
    device_map="mps",  # ç›´æ¥æŒ‡å®šä½¿ç”¨MPS
    trust_remote_code=True,
    low_cpu_mem_usage=True,
)

print(f"æ¨¡å‹åŠ è½½ç”¨æ—¶: {time.time() - start_time:.2f}ç§’")
print(f"æ¨¡å‹ç±»å‹: {model.__class__.__name__}")

# æ‰“å°æ¨¡å‹å‚æ•°æ‰€åœ¨è®¾å¤‡
devices = {}
for name, param in model.named_parameters():
    device = param.device
    if device not in devices:
        devices[device] = 0
    devices[device] += param.numel()

print("æ¨¡å‹å‚æ•°åˆ†å¸ƒ:")
total_params = sum(devices.values())
for device, count in devices.items():
    print(f"  {device}: {count/1e9:.2f}B å‚æ•° ({count/total_params*100:.1f}%)")

print("====== é˜¶æ®µ2: å‡†å¤‡æ¨ç† ======")
model.eval()
model.config.use_cache = True
torch.set_grad_enabled(False)

# æ¸…ç†å†…å­˜
gc.collect()
torch.mps.empty_cache()

print("====== é˜¶æ®µ3: æ‰§è¡Œæ¨ç† ======")
question = "ä¸€å70å²çš„ç”·æ€§æ‚£è€…å› èƒ¸ç—›ä¼´å‘•å16å°æ—¶å°±åŒ»ï¼Œå¿ƒç”µå›¾æ˜¾ç¤ºä¸‹å£å¯¼è”å’Œå³èƒ¸å¯¼è”STæ®µæŠ¬é«˜0.1~0.3mVï¼Œç»è¡¥æ¶²åè¡€å‹é™è‡³80/60mmHgï¼Œæ‚£è€…å‡ºç°å‘¼å¸å›°éš¾å’Œä¸èƒ½å¹³å§çš„ç—‡çŠ¶ï¼Œä½“æ£€å‘ç°åŒè‚ºæœ‰å¤§é‡æ°´æ³¡éŸ³ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæœ€æ°å½“çš„è¯ç‰©å¤„ç†æ˜¯ä»€ä¹ˆï¼Ÿ"

prompt = f"""ä»¥ä¸‹æ˜¯æè¿°ä»»åŠ¡çš„æŒ‡ä»¤ï¼Œä»¥åŠæä¾›æ›´å¤šä¸Šä¸‹æ–‡çš„è¾“å…¥ã€‚
è¯·å†™å‡ºæ°å½“å®Œæˆè¯¥è¯·æ±‚çš„å›ç­”ã€‚
åœ¨å›ç­”ä¹‹å‰ï¼Œè¯·ä»”ç»†æ€è€ƒé—®é¢˜ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªé€æ­¥çš„æ€ç»´é“¾ï¼Œä»¥ç¡®ä¿å›ç­”åˆä¹é€»è¾‘ä¸”å‡†ç¡®ã€‚
### Instruction:
ä½ æ˜¯ä¸€ä½åœ¨ä¸´åºŠæ¨ç†ã€è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’æ–¹é¢å…·æœ‰ä¸“ä¸šçŸ¥è¯†çš„åŒ»å­¦ä¸“å®¶ã€‚
è¯·å›ç­”ä»¥ä¸‹åŒ»å­¦é—®é¢˜ã€‚
### Question:
{question}
### Response:
"""

print("å‡†å¤‡è¾“å…¥...")
inputs = tokenizer(prompt, return_tensors="pt")

main_device = next(model.parameters()).device
print(f"æ¨¡å‹ä¸»è¦åœ¨è®¾å¤‡: {main_device}")
inputs = {k: v.to(main_device) for k, v in inputs.items()}

# å…¨å±€å˜é‡ï¼Œç”¨äºè·Ÿè¸ªç”Ÿæˆæ˜¯å¦å®Œæˆ
generation_completed = False

def print_progress():
    """æ¯10ç§’æ‰“å°ä¸€æ¬¡è¿›åº¦æç¤º"""
    if not generation_completed:
        elapsed = time.time() - start_time
        print(f"ç”Ÿæˆä¸­... å·²ç»è¿è¡Œ {elapsed:.1f} ç§’")
        threading.Timer(10.0, print_progress).start()

print("å¼€å§‹ç”Ÿæˆ...")
print("æ³¨æ„ï¼šç”Ÿæˆè¿‡ç¨‹ä¸­å°†æ¯10ç§’æ˜¾ç¤ºä¸€æ¬¡è¿è¡Œæ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
start_time = time.time()

# å¯åŠ¨è¿›åº¦æ‰“å°çº¿ç¨‹
threading.Timer(10.0, print_progress).start()

try:
    # æ‰§è¡Œç”Ÿæˆ - ä¸ä½¿ç”¨callbackå‚æ•°
    outputs = model.generate(
        **inputs,
        max_new_tokens=1200,
        temperature=0.7,
        do_sample=True,
        use_cache=True,
    )
    
    # æ ‡è®°ç”Ÿæˆå·²å®Œæˆ
    generation_completed = True
    
    # è®¡ç®—ç”Ÿæˆé€Ÿåº¦
    generation_time = time.time() - start_time
    tokens_generated = outputs.shape[1] - inputs["input_ids"].shape[1]
    tokens_per_second = tokens_generated / generation_time
    
    print(f"\nç”Ÿæˆå®Œæˆ!")
    print(f"ç”Ÿæˆäº† {tokens_generated} ä¸ªtokensï¼Œç”¨æ—¶ {generation_time:.2f} ç§’")
    print(f"ç”Ÿæˆé€Ÿåº¦: {tokens_per_second:.2f} tokens/ç§’")
    
    # è§£ç è¾“å‡º
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    print("\n====== ç”Ÿæˆç»“æœ ======\n")
    print(generated_text)
    
    # å°è¯•åˆ†å‰²å‡ºå›ç­”éƒ¨åˆ†
    if "### Response:" in generated_text:
        answer = generated_text.split("### Response:")[1]
        print("\n====== æå–çš„å›ç­” ======\n")
        print(answer)
    
except Exception as e:
    print(f"ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
    import traceback
    traceback.print_exc()

# æ ‡è®°ç”Ÿæˆå·²å®Œæˆï¼Œåœæ­¢è¿›åº¦æç¤º
generation_completed = True

# æ¸…ç†èµ„æº
print("\n====== æ¸…ç†èµ„æº ======")
del model
del tokenizer
gc.collect()
torch.mps.empty_cache()
print("æ¨ç†å®Œæˆï¼Œèµ„æºå·²é‡Šæ”¾")
```

---


åˆ›å»ºå¾®è°ƒè„šæœ¬ï¼š

```bash
cd ~/Documents/unsloth-finetune
touch r1-lora-peft-mac.py
```

ç¼–è¾‘`r1-lora-peft-mac.py`ï¼Œå¡«å…¥ä»¥ä¸‹ä»£ç ï¼š

```python
import os
import json
import torch
import wandb
import gc  # ç”¨äºä¸»åŠ¨åƒåœ¾æ”¶é›†
from pathlib import Path

# ==================== æ ¸å¿ƒå†…å­˜ä¼˜åŒ–è®¾ç½® ====================
# è¿™äº›è®¾ç½®å¯¹è§£å†³æ¥è¿‘å°¾å£°æ—¶å´©æºƒçš„é—®é¢˜è‡³å…³é‡è¦
os.environ["TOKENIZERS_PARALLELISM"] = "false"   # é¿å…tokenizerå¹¶è¡Œå¯¼è‡´çš„å†…å­˜å‹åŠ›
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"  # å¯ç”¨å›é€€é€‰é¡¹ï¼Œé¿å…æœªå®ç°çš„æ“ä½œæŠ¥é”™
os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"  # å®Œå…¨ç¦ç”¨å†…å­˜ä¸Šé™
os.environ["PYTORCH_MPS_LOW_WATERMARK_RATIO"] = "0.0"   # å®Œå…¨ç¦ç”¨å†…å­˜ä¸‹é™

# ç¡¬ç¼–ç å¼€å…³ï¼Œè®¾ç½®ä¸ºTrueå¯è·³è¿‡åˆæ¬¡æ¨ç†
SKIP_INITIAL_INFERENCE = True #False

# å®šä¹‰å†…å­˜æ¸…ç†å‡½æ•°
def clean_memory():
    """æ¸…ç†GPUå’ŒCPUå†…å­˜ç¼“å­˜"""
    gc.collect()  # å¼ºåˆ¶Pythonåƒåœ¾æ”¶é›†
    if torch.backends.mps.is_available():
        torch.mps.empty_cache()  # æ¸…ç†MPSç¼“å­˜

# å¯é€‰ï¼šç™»å½•wandbè¿›è¡Œå®éªŒè·Ÿè¸ª
# wandb.login(key="ä½ çš„wandb.aiç½‘ç«™ä¸Šçš„token")
# åˆå§‹åŒ–wandbé¡¹ç›®
run = wandb.init(
    project='Lora-DeepSeek-R1-Distill-Qwen-14B-Mac',
    job_type="training",
    anonymous="allow"
)

####################################################################################################
# 1.åŠ è½½æ¨¡å‹

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model

# å†…å­˜ä¼˜åŒ–ä½†ä¸è¿‡åº¦é™ä½æ€§èƒ½çš„å¹³è¡¡å‚æ•°
max_seq_length = 2048  # æ¯”åŸå§‹çš„3072å°ï¼Œä½†ä»èƒ½ç»´æŒè¾ƒå¥½çš„ä¸Šä¸‹æ–‡çª—å£
dtype = torch.float32  # æ”¹ä¸ºå§‹ç»ˆä½¿ç”¨float32ä»¥é¿å…æ•°æ®ç±»å‹ä¸åŒ¹é…é—®é¢˜

# é‡è¦: ä½¿ç”¨os.path.expanduser()å±•å¼€è·¯å¾„ä¸­çš„~ç¬¦å·
model_path = os.path.expanduser("~/Documents/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B")
# ç¡®ä¿ç›®å½•å­˜åœ¨
if not os.path.exists(model_path):
    raise ValueError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}ï¼Œè¯·æ£€æŸ¥ä¸‹è½½æ˜¯å¦å®Œæˆæˆ–è·¯å¾„æ˜¯å¦æ­£ç¡®")

print(f"æ­£åœ¨ä»æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹: {model_path}")

# åŠ è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ - æ·»åŠ ä¸€äº›å…³é”®çš„å†…å­˜ä¼˜åŒ–é€‰é¡¹
print("å¼€å§‹åŠ è½½æ¨¡å‹ï¼Œè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...")
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float32,  # æ”¹ä¸ºä½¿ç”¨float32ä»¥é¿å…æ•°æ®ç±»å‹ä¸åŒ¹é…é—®é¢˜
    device_map="mps",  # ä½¿ç”¨Apple Siliconçš„Metalæ€§èƒ½ç€è‰²å™¨
    trust_remote_code=True,
    # å†…å­˜ä¼˜åŒ–é€‰é¡¹
    low_cpu_mem_usage=True,     # é™ä½CPUå†…å­˜ä½¿ç”¨
)

# åŠ è½½åæ¸…ç†å†…å­˜
clean_memory()
print(f"Model loaded: {model.__class__.__name__}")

####################################################################################################
# 2. å®šä¹‰æç¤ºæ¨¡æ¿ï¼Œå¹¶åœ¨å¾®è°ƒå‰åšä¸€æ¬¡æ¨ç†

prompt_style = """ä»¥ä¸‹æ˜¯æè¿°ä»»åŠ¡çš„æŒ‡ä»¤ï¼Œä»¥åŠæä¾›æ›´å¤šä¸Šä¸‹æ–‡çš„è¾“å…¥ã€‚
  è¯·å†™å‡ºæ°å½“å®Œæˆè¯¥è¯·æ±‚çš„å›ç­”ã€‚
  åœ¨å›ç­”ä¹‹å‰ï¼Œè¯·ä»”ç»†æ€è€ƒé—®é¢˜ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªé€æ­¥çš„æ€ç»´é“¾ï¼Œä»¥ç¡®ä¿å›ç­”åˆä¹é€»è¾‘ä¸”å‡†ç¡®ã€‚
  ### Instruction:
  ä½ æ˜¯ä¸€ä½åœ¨ä¸´åºŠæ¨ç†ã€è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’æ–¹é¢å…·æœ‰ä¸“ä¸šçŸ¥è¯†çš„åŒ»å­¦ä¸“å®¶ã€‚
  è¯·å›ç­”ä»¥ä¸‹åŒ»å­¦é—®é¢˜ã€‚
  ### Question:
  {}
  ### Response:
  <think>{}"""
train_prompt_style = prompt_style + """
  </think>
  {}"""

# æµ‹è¯•ç”¨åŒ»å­¦é—®é¢˜
question = "ä¸€å70å²çš„ç”·æ€§æ‚£è€…å› èƒ¸ç—›ä¼´å‘•å16å°æ—¶å°±åŒ»ï¼Œå¿ƒç”µå›¾æ˜¾ç¤ºä¸‹å£å¯¼è”å’Œå³èƒ¸å¯¼è”STæ®µæŠ¬é«˜0.1~0.3mVï¼Œç»è¡¥æ¶²åè¡€å‹é™è‡³80/60mmHgï¼Œæ‚£è€…å‡ºç°å‘¼å¸å›°éš¾å’Œä¸èƒ½å¹³å§çš„ç—‡çŠ¶ï¼Œä½“æ£€å‘ç°åŒè‚ºæœ‰å¤§é‡æ°´æ³¡éŸ³ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæœ€æ°å½“çš„è¯ç‰©å¤„ç†æ˜¯ä»€ä¹ˆï¼Ÿ"

# æ™ºèƒ½æ¨ç†å‡½æ•° - åªåœ¨å¿…è¦æ—¶å›é€€åˆ°CPU
def smart_inference(model, inputs, max_new_tokens=1200, temperature=0.7):
    """æ™ºèƒ½æ¨ç†å‡½æ•° - åªæœ‰åœ¨ç‰¹å®šæ“ä½œå¤±è´¥æ—¶å›é€€åˆ°CPU"""
    try:
        # å°è¯•ç›´æ¥ä½¿ç”¨MPSè¿›è¡Œæ¨ç†
        print("ä½¿ç”¨MPSè¿›è¡Œæ¨ç†...")
        # ç¦ç”¨gradient_checkpointingä»¥ä½¿ç”¨KVç¼“å­˜
        was_gradient_checkpointing = model.is_gradient_checkpointing
        if was_gradient_checkpointing:
            model.gradient_checkpointing_disable()
            
        # ç¡®ä¿æ‰€æœ‰è¾“å…¥å¼ é‡çš„æ•°æ®ç±»å‹ä¸€è‡´
        for k, v in inputs.items():
            if isinstance(v, torch.Tensor) and v.dtype == torch.float16:
                inputs[k] = v.to(dtype=torch.float32)
                
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,  # å‡å°‘ç”Ÿæˆçš„tokenæ•°é‡
            temperature=temperature,
            do_sample=True,
            use_cache=True,  # å¯ç”¨KVç¼“å­˜
        )
        
        # æ¢å¤gradient_checkpointingçŠ¶æ€
        if was_gradient_checkpointing:
            model.gradient_checkpointing_enable()
            
        return outputs
    except RuntimeError as e:
        error_msg = str(e)
        if "MPS backend out of memory" in error_msg:
            print(f"é‡åˆ°MPSå†…å­˜é”™è¯¯: {error_msg}")
            print("æ¸…ç†å†…å­˜å¹¶å°è¯•æ™ºèƒ½å›é€€ç­–ç•¥...")
            
            # æ¸…ç†å†…å­˜
            clean_memory()
            
            try:
                # å°è¯•ä½¿ç”¨æ›´å°çš„max_new_tokenså’Œæ›´ä½çš„æ¸©åº¦
                print("å°è¯•ä½¿ç”¨æ›´ä¿å®ˆçš„ç”Ÿæˆå‚æ•°...")
                # ç¦ç”¨gradient_checkpointingä»¥ä½¿ç”¨KVç¼“å­˜
                was_gradient_checkpointing = model.is_gradient_checkpointing
                if was_gradient_checkpointing:
                    model.gradient_checkpointing_disable()
                    
                # ç¡®ä¿æ‰€æœ‰è¾“å…¥å¼ é‡çš„æ•°æ®ç±»å‹ä¸€è‡´
                for k, v in inputs.items():
                    if isinstance(v, torch.Tensor) and v.dtype == torch.float16:
                        inputs[k] = v.to(dtype=torch.float32)
                        
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens // 2,  # å‡åŠ
                    temperature=0.1,  # æ›´ç¡®å®šæ€§çš„ç”Ÿæˆ
                    do_sample=False,  # ç¦ç”¨é‡‡æ ·
                    use_cache=True,
                )
                
                # æ¢å¤gradient_checkpointingçŠ¶æ€
                if was_gradient_checkpointing:
                    model.gradient_checkpointing_enable()
                    
                return outputs
            except RuntimeError as e2:
                print(f"ç¬¬äºŒæ¬¡å°è¯•ä¹Ÿå¤±è´¥: {str(e2)}")
                # å¦‚æœä»ç„¶å¤±è´¥ï¼Œå°†æ¨ç†ç§»è‡³CPU
                print("MPSæ¨ç†ä»ç„¶å¤±è´¥ï¼Œå›é€€åˆ°CPU...")
                model.to("cpu")
                cpu_inputs = {k: v.to("cpu") for k, v in inputs.items() if hasattr(v, "to")}
                
                with torch.no_grad():
                    outputs = model.generate(
                        **cpu_inputs,
                        max_new_tokens=max_new_tokens,
                        temperature=temperature,
                        do_sample=True,
                        use_cache=True,  # CPUä¸Šå¯ä»¥å®‰å…¨ä½¿ç”¨
                    )
                
                # æ¨ç†åç§»å›MPS
                model.to("mps")
                return outputs
        elif "does not match" in error_msg or "requires the same element type" in error_msg:
            # å¤„ç†æ•°æ®ç±»å‹ä¸åŒ¹é…é”™è¯¯
            print(f"é‡åˆ°æ•°æ®ç±»å‹ä¸åŒ¹é…é”™è¯¯: {error_msg}")
            print("å°è¯•ç»Ÿä¸€æ•°æ®ç±»å‹ä¸ºfloat32...")
            
            # ç¡®ä¿æ‰€æœ‰è¾“å…¥å¼ é‡çš„æ•°æ®ç±»å‹ä¸€è‡´ä¸ºfloat32
            for k, v in inputs.items():
                if isinstance(v, torch.Tensor):
                    inputs[k] = v.to(dtype=torch.float32)
            
            # ç¦ç”¨gradient_checkpointingä»¥ä½¿ç”¨KVç¼“å­˜
            was_gradient_checkpointing = model.is_gradient_checkpointing
            if was_gradient_checkpointing:
                model.gradient_checkpointing_disable()
                
            try:
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    do_sample=True,
                    use_cache=True,
                )
                
                # æ¢å¤gradient_checkpointingçŠ¶æ€
                if was_gradient_checkpointing:
                    model.gradient_checkpointing_enable()
                    
                return outputs
            except RuntimeError:
                # å¦‚æœä»ç„¶å¤±è´¥ï¼Œå°†æ¨ç†ç§»è‡³CPU
                print("MPSæ¨ç†ä»ç„¶å¤±è´¥ï¼Œå›é€€åˆ°CPU...")
                model.to("cpu")
                cpu_inputs = {k: v.to("cpu") for k, v in inputs.items() if hasattr(v, "to")}
                
                with torch.no_grad():
                    outputs = model.generate(
                        **cpu_inputs,
                        max_new_tokens=max_new_tokens,
                        temperature=temperature,
                        do_sample=True,
                        use_cache=True,
                    )
                
                # æ¨ç†åç§»å›MPS
                model.to("mps")
                return outputs
        else:
            # å¦‚æœä¸æ˜¯å·²çŸ¥é”™è¯¯ï¼Œé‡æ–°æŠ›å‡º
            raise

# æ‰§è¡Œåˆæ¬¡æ¨ç†
model.eval()
if not SKIP_INITIAL_INFERENCE:
    # åªæœ‰å½“SKIP_INITIAL_INFERENCE=Falseæ—¶æ‰æ‰§è¡Œæ¨ç†
    with torch.no_grad():
        # å‡†å¤‡è¾“å…¥
        model_input = prompt_style.format(question, "")
        inputs = tokenizer(model_input, return_tensors="pt").to("mps")
        
        # ä½¿ç”¨æ™ºèƒ½æ¨ç†
        outputs = smart_inference(model, inputs, max_new_tokens=1200, temperature=0.7)
        
        # è§£ç è¾“å‡º
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print("### å¾®è°ƒå‰æ¨¡å‹æ¨ç†ç»“æœï¼š")
        print(response.split("### Response:")[1])
else:
    print("å·²è·³è¿‡åˆæ¬¡æ¨ç†ï¼Œç›´æ¥å¼€å§‹è®­ç»ƒæµç¨‹...")

# æ¸…ç†å†…å­˜
clean_memory()

####################################################################################################
# 3. åº”ç”¨LoRAè¿›è¡Œå¾®è°ƒå‡†å¤‡

# é…ç½®LoRA - ä¿æŒåŸå§‹çš„target_modulesä½†ç•¥å¾®å‡å°rå€¼
target_modules = [
    "q_proj", "k_proj", "v_proj", "o_proj", 
    "gate_proj", "up_proj", "down_proj",
]

# åˆ›å»ºLoRAé…ç½® - ä½¿ç”¨ç¨å°çš„rå€¼ä»¥èŠ‚çœå†…å­˜
lora_config = LoraConfig(
    r=16,                      # ä»24å‡å°åˆ°16ï¼Œæ›´å¥½åœ°å¹³è¡¡å†…å­˜ä½¿ç”¨å’Œæ€§èƒ½
    lora_alpha=16,             # åŸå§‹alphaå€¼
    target_modules=target_modules,
    lora_dropout=0,            # åŸå§‹dropout
    bias="none",               # åŸå§‹biasè®¾ç½®
    task_type="CAUSAL_LM",     # ä»»åŠ¡ç±»å‹ä¸å˜
    inference_mode=False,      # è®­ç»ƒæ¨¡å¼
)

# å®‰å…¨çš„å‚æ•°è½¬æ¢å‡½æ•° - ä½¿ç”¨ç»Ÿä¸€çš„æ•°æ®ç±»å‹
def safe_prepare_model_for_kbit_training(model):
    """å®‰å…¨åœ°å‡†å¤‡æ¨¡å‹è¿›è¡Œé‡åŒ–è®­ç»ƒï¼Œæ‰¹é‡å¤„ç†å‚æ•°ä»¥é¿å…å†…å­˜å³°å€¼"""
    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥å‡å°‘å†…å­˜ä½¿ç”¨
    model.gradient_checkpointing_enable()
    # ç¡®ä¿å‚æ•°ç¼“å­˜æ˜¯ç¦ç”¨çš„ï¼Œä»¥é¿å…ä¸æ¢¯åº¦æ£€æŸ¥ç‚¹å†²çª
    model.config.use_cache = False
    
    print("åˆ†æ‰¹å¤„ç†å‚æ•°è½¬æ¢ï¼Œé¿å…å†…å­˜å³°å€¼...")
    
    # é¦–å…ˆå¤„ç†æ³¨æ„åŠ›å±‚
    print("å¤„ç†æ³¨æ„åŠ›å±‚å‚æ•°...")
    attention_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]
    for name, module in model.named_modules():
        if any(target in name for target in attention_modules):
            for param in module.parameters():
                if param.requires_grad:
                    # ç¡®ä¿ç»Ÿä¸€ä½¿ç”¨float32ç±»å‹
                    param.data = param.data.to(torch.float32)
            # æ¯ä¸ªæ¨¡å—åæ¸…ç†å†…å­˜
            clean_memory()
    
    # å¤„ç†å‰é¦ˆå±‚
    print("å¤„ç†å‰é¦ˆå±‚å‚æ•°...")
    feedforward_modules = ["gate_proj", "up_proj", "down_proj"]
    for name, module in model.named_modules():
        if any(target in name for target in feedforward_modules):
            for param in module.parameters():
                if param.requires_grad:
                    # ç¡®ä¿ç»Ÿä¸€ä½¿ç”¨float32ç±»å‹
                    param.data = param.data.to(torch.float32)
            # æ¯ä¸ªæ¨¡å—åæ¸…ç†å†…å­˜
            clean_memory()
    
    print("å‚æ•°è½¬æ¢å®Œæˆï¼")
    return model

# ä½¿ç”¨å®‰å…¨çš„å‚æ•°è½¬æ¢å‡½æ•°
try:
    model = safe_prepare_model_for_kbit_training(model)
except Exception as e:
    print(f"å‚æ•°è½¬æ¢å‡ºé”™: {e}")
    print("å°è¯•ä½¿ç”¨æ ‡å‡†æ–¹æ³•...")
    # å†æ¬¡æ¸…ç†å†…å­˜
    clean_memory()
    # å°è¯•æ ‡å‡†æ–¹æ³•
    model = prepare_model_for_kbit_training(model)

# åº”ç”¨LoRAé…ç½®
model = get_peft_model(model, lora_config)
print(f"Trainable params: {model.print_trainable_parameters()}")

# æ¸…ç†å†…å­˜
clean_memory()

####################################################################################################
# 4. å¤„ç†æ•°æ®é›†

EOS_TOKEN = tokenizer.eos_token

# æ ¼å¼åŒ–æç¤ºå‡½æ•°ï¼Œç”¨äºå¤„ç†æ•°æ®é›†ä¸­çš„ç¤ºä¾‹
def formatting_prompts_func(examples):
    # ä»examplesä¸­æå–é—®é¢˜ã€æ€ç»´é“¾å’Œå›ç­”
    inputs = examples["Question"]      # åŒ»å­¦é—®é¢˜åˆ—è¡¨
    cots = examples["Complex_CoT"]     # æ€ç»´é“¾åˆ—è¡¨
    outputs = examples["Response"]     # å›ç­”åˆ—è¡¨
    
    # å­˜å‚¨æ ¼å¼åŒ–åçš„æ–‡æœ¬
    texts = []
    
    # éå†æ¯ä¸ªç¤ºä¾‹ï¼Œå°†é—®é¢˜ã€æ€ç»´é“¾å’Œå›ç­”ç»„åˆæˆæŒ‡å®šæ ¼å¼
    for input, cot, output in zip(inputs, cots, outputs):
        # ä½¿ç”¨train_prompt_styleæ¨¡æ¿æ ¼å¼åŒ–æ–‡æœ¬ï¼Œå¹¶æ·»åŠ ç»“æŸç¬¦
        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN
        texts.append(text)
        
    # è¿”å›æ ¼å¼åŒ–åçš„æ–‡æœ¬å­—å…¸
    return {
        "text": texts,
    }

# åŠ è½½æ•°æ®é›†å¹¶åº”ç”¨æ ¼å¼åŒ–
from datasets import load_dataset
# é‡è¦: ä½¿ç”¨os.path.expanduser()å±•å¼€è·¯å¾„ä¸­çš„~ç¬¦å·
data_path = os.path.expanduser("~/Documents/datasets/medical-o1-reasoning-SFT/medical_o1_sft_Chinese.json")
if not os.path.exists(data_path):
    raise ValueError(f"æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {data_path}ï¼Œè¯·æ£€æŸ¥ä¸‹è½½æ˜¯å¦å®Œæˆæˆ–è·¯å¾„æ˜¯å¦æ­£ç¡®")

# ä½¿ç”¨åˆç†æ•°é‡çš„æ ·æœ¬ - åˆå§‹éªŒè¯æ—¶ä½¿ç”¨è¾ƒå°‘æ ·æœ¬
dataset = load_dataset(
    "json",  # æŒ‡å®šæ•°æ®æ ¼å¼ä¸ºJSON
    data_files=data_path,
    split="train[0:200]",  # ä½¿ç”¨å‰200æ¡æ•°æ®ç”¨äºåˆå§‹éªŒè¯
    trust_remote_code=True  # å…¼å®¹remote codeçš„è¡Œä¸º
)

# å¦‚æœè¿”å›çš„æ˜¯DatasetDictï¼Œåˆ™å–å‡º"train"è¿™ä¸€éƒ¨åˆ†
if isinstance(dataset, dict):  
    dataset = dataset["train"]
    
# åˆ†æ‰¹å¤„ç†ä»¥é¿å…å†…å­˜å³°å€¼
dataset = dataset.map(
    formatting_prompts_func, 
    batched=True,
    batch_size=10,  # åˆç†çš„æ‰¹å¤„ç†å¤§å°
    num_proc=1,     # å•è¿›ç¨‹ä»¥é¿å…é¢å¤–å†…å­˜å¼€é”€
)
print(f"Dataset loaded: {len(dataset)} examples")

# æ¸…ç†å†…å­˜
clean_memory()

####################################################################################################
# 5. é…ç½®è®­ç»ƒå‚æ•°å¯åŠ¨è®­ç»ƒ

from transformers import TrainingArguments, Trainer
import transformers

# å®šä¹‰æ•°æ®æ•´ç†å‡½æ•° - é’ˆå¯¹MPSä¼˜åŒ–
def data_collator(features):
    """ç¡®ä¿è¾“å‡ºçš„å¼ é‡ç±»å‹ä¸€è‡´ä¸ºfloat32å¹¶æ·»åŠ å¿…è¦çš„labels"""
    texts = [f["text"] for f in features]
    batch = tokenizer(
        texts, 
        padding="longest",  # åªå¡«å……åˆ°æœ€é•¿åºåˆ—é•¿åº¦ï¼Œè€Œä¸æ˜¯max_length
        truncation=True, 
        max_length=max_seq_length, 
        return_tensors="pt"
    )
    
    # åˆ›å»ºæ ‡ç­¾å¼ é‡ï¼šå¯¹äºå› æœè¯­è¨€æ¨¡å‹ï¼Œæ ‡ç­¾é€šå¸¸ä¸è¾“å…¥IDç›¸åŒ
    # ä½†æ³¨æ„ï¼šæˆ‘ä»¬è¦å¤åˆ¶input_idsä½œä¸ºlabels
    batch["labels"] = batch["input_ids"].clone()
    
    # ç¡®ä¿æ‰€æœ‰å¼ é‡ä¸ºfloat32ç±»å‹
    for key, value in batch.items():
        if isinstance(value, torch.Tensor) and value.dtype == torch.float16:
            batch[key] = value.to(dtype=torch.float32)
    
    return batch

# è®­ç»ƒå‚æ•° - æ ¹æ®ä¸åŒå†…å­˜å¤§å°è°ƒæ•´
# 32GBå†…å­˜: batch_size=1, gradient_steps=8
# 64GBå†…å­˜: batch_size=1, gradient_steps=4
# 96GB+å†…å­˜: batch_size=2, gradient_steps=4
training_args = TrainingArguments(
    output_dir="outputs",
    per_device_train_batch_size=1,    # é™ä½æ‰¹æ¬¡å¤§å°ä»¥å‡å°‘å†…å­˜ä½¿ç”¨
    gradient_accumulation_steps=8,    # å¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ä»¥ä¿æŒæœ‰æ•ˆæ‰¹å¤§å°
    learning_rate=2e-4,               # å­¦ä¹ ç‡
    lr_scheduler_type="linear",       # çº¿æ€§å­¦ä¹ ç‡è°ƒåº¦å™¨
    warmup_steps=5,                   # é¢„çƒ­æ­¥æ•°
    max_steps=20,                     # åˆå§‹éªŒè¯åªéœ€å°‘é‡æ­¥éª¤
    logging_steps=1,                  # æ¯æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—ä»¥ä¾¿äºéªŒè¯
    save_steps=10,                    # æ¯10æ­¥ä¿å­˜ä¸€æ¬¡
    fp16=False,                       # ä¸ä½¿ç”¨åŠç²¾åº¦ï¼ŒMPSä¸æ”¯æŒ
    bf16=False,                       # åŒæ ·ä¸ä½¿ç”¨bf16
    remove_unused_columns=False,      # é¿å…æ•°æ®é›†åˆ—ä¸åŒ¹é…é”™è¯¯
    gradient_checkpointing=True,      # æ¢¯åº¦æ£€æŸ¥ç‚¹
    weight_decay=0.01,                # æƒé‡è¡°å‡
    seed=8137,                        # éšæœºæ•°ç§å­
    # å†…å­˜ä¼˜åŒ–å‚æ•°
    dataloader_num_workers=0,         # ä¸ä½¿ç”¨å¤šè¿›ç¨‹æ•°æ®åŠ è½½
    dataloader_pin_memory=False,      # ä¸ä½¿ç”¨å›ºå®šå†…å­˜
    report_to="none" if not wandb.run else "wandb",  # æ ¹æ®wandbæ˜¯å¦å¯ç”¨å†³å®šæŠ¥å‘Š
    run_name="medical-o1-sft-experiment-mac",  # wandbè¿è¡Œåç§°
)

# åˆ›å»ºè®­ç»ƒå™¨
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    data_collator=data_collator,
)

# åœ¨å¼€å§‹è®­ç»ƒå‰ç¡®ä¿æ¨¡å‹é…ç½®æ­£ç¡®
model.config.use_cache = False  # ç¡®ä¿ç¦ç”¨ç¼“å­˜ï¼Œä¸æ¢¯åº¦æ£€æŸ¥ç‚¹å…¼å®¹

# å¼€å§‹è®­ç»ƒ
print(f"å¼€å§‹è®­ç»ƒ: {training_args.max_steps} æ­¥ï¼Œæ‰¹æ¬¡å¤§å°: {training_args.per_device_train_batch_size}ï¼Œæ¢¯åº¦ç´¯ç§¯: {training_args.gradient_accumulation_steps}")

# æ·»åŠ é¢å¤–çš„è®­ç»ƒå‰å‡†å¤‡
def prepare_model_for_mps_training(model):
    """ç‰¹æ®Šå¤„ç†ä»¥è§£å†³MPSåç«¯æ•°æ®ç±»å‹ä¸åŒ¹é…é—®é¢˜"""
    print("æ­£åœ¨ä¸ºMPSè®­ç»ƒå‡†å¤‡æ¨¡å‹...")
    
    # ç¡®ä¿æ‰€æœ‰attentionå±‚å’Œfeedforwardå±‚çš„å‚æ•°ä½¿ç”¨ç›¸åŒçš„æ•°æ®ç±»å‹
    # è¿™æ˜¯ä¸ºäº†è§£å†³ç‰¹å®šçš„tensor<1x688x5120xf16>, tensor<5120xf32>æ•°æ®ç±»å‹ä¸åŒ¹é…é—®é¢˜
    for name, module in model.named_modules():
        if any(x in name for x in ["attention", "mlp", "down_proj", "up_proj", "gate_proj"]):
            # æ£€æŸ¥æ˜¯å¦æœ‰æƒé‡å’Œåç½®ä½¿ç”¨ä¸åŒçš„æ•°æ®ç±»å‹
            for child_name, child in module.named_children():
                if hasattr(child, "weight") and hasattr(child, "bias") and child.bias is not None:
                    # ç¡®ä¿æƒé‡å’Œåç½®ä½¿ç”¨ç›¸åŒçš„æ•°æ®ç±»å‹
                    if child.weight.dtype != child.bias.dtype:
                        print(f"ä¿®å¤æ•°æ®ç±»å‹ä¸åŒ¹é…: {name}.{child_name}, æƒé‡={child.weight.dtype}, åç½®={child.bias.dtype}")
                        child.bias.data = child.bias.data.to(dtype=child.weight.dtype)
    
    # ç¦ç”¨KVç¼“å­˜ï¼Œä»¥é˜²æ­¢ä¸æ¢¯åº¦æ£€æŸ¥ç‚¹å†²çª
    model.config.use_cache = False
    
    print("MPSè®­ç»ƒå‡†å¤‡å®Œæˆã€‚")
    return model

# åº”ç”¨é¢å¤–çš„MPSè®­ç»ƒå‡†å¤‡
model = prepare_model_for_mps_training(model)

try:
    trainer.train()
    print(f"è®­ç»ƒå®Œæˆã€‚æ­¥æ•°: {trainer.state.global_step}")
except RuntimeError as e:
    error_msg = str(e)
    if "MPS backend out of memory" in error_msg:
        print("\nè®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°å†…å­˜ä¸è¶³ã€‚è¯·å°è¯•ä»¥ä¸‹è§£å†³æ–¹æ¡ˆ:")
        print("1. å‡å°max_seq_lengthå€¼è‡³1536æˆ–1024")
        print("2. å‡å°LoRAé…ç½®ä¸­çš„rå€¼è‡³8")
        print("3. å‡å°‘target_modulesçš„æ•°é‡ï¼Œä»…ä¿ç•™[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]")
        print("4. å¢åŠ gradient_accumulation_stepsè‡³16")
    elif "requires the same element type" in error_msg or "does not match" in error_msg:
        print("\nè®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°æ•°æ®ç±»å‹ä¸åŒ¹é…é”™è¯¯:")
        print(error_msg)
        print("\nè¯·å°è¯•ä»¥ä¸‹è§£å†³æ–¹æ¡ˆ:")
        print("1. ç¡®ä¿æ‰€æœ‰æ¨¡å‹å‚æ•°ç»Ÿä¸€ä½¿ç”¨torch.float32ç±»å‹")
        print("2. å‡å°‘LoRAé…ç½®ä¸­çš„target_modulesï¼Œä»…ä¿ç•™attentionæ¨¡å—")
        print("3. å°è¯•è®¾ç½®ç¯å¢ƒå˜é‡ï¼šexport PYTORCH_ENABLE_MPS_FALLBACK=1")
    else:
        raise e

# æ¸…ç†å†…å­˜
clean_memory()

####################################################################################################
# 6. å¾®è°ƒåçš„æ¨¡å‹åšä¸€æ¬¡æ¨ç†

# è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
model.eval()

with torch.no_grad():
    # å‡†å¤‡è¾“å…¥
    model_input = prompt_style.format(question, "")
    inputs = tokenizer(model_input, return_tensors="pt").to("mps")
    
    # ä½¿ç”¨æ™ºèƒ½æ¨ç†
    outputs = smart_inference(model, inputs, max_new_tokens=1200, temperature=0.7)
    
    # è§£ç è¾“å‡º
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print("### å¾®è°ƒåæ¨¡å‹æ¨ç†ç»“æœï¼š")
    print(response.split("### Response:")[1])

####################################################################################################
# 7. ä¿å­˜æ¨¡å‹

# æ¸…ç†å†…å­˜
clean_memory()

# ä¿å­˜æ¨¡å‹
model_save_path = "DeepSeek-R1-Medical-COT-Qwen-14B-Mac"
model.save_pretrained(model_save_path)
tokenizer.save_pretrained(model_save_path)

print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {os.path.abspath(model_save_path)}")
print("éªŒè¯å®Œæˆåï¼Œå¯ä»¥ç§»é™¤max_stepsé™åˆ¶å¹¶ä½¿ç”¨å®Œæ•´æ•°æ®é›†è¿›è¡Œè®­ç»ƒ")

## äº”ã€æ‰§è¡Œå¾®è°ƒè¿‡ç¨‹

```bash
# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒï¼ˆå¦‚æœå°šæœªæ¿€æ´»ï¼‰
cd ~/Documents/llm-finetune
source peft-env/bin/activate

# å®‰è£…NumPy 1.xç‰ˆæœ¬è§£å†³å…¼å®¹æ€§é—®é¢˜
pip install numpy==1.26.4

# å®‰è£…å†…å­˜ç›‘æ§å·¥å…·(å¯é€‰)
pip install psutil

# è¿è¡Œå¾®è°ƒè„šæœ¬
python r1-lora-peft-mac.py
```

## å…­ã€æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. å†…å­˜é…ç½®æŒ‡å—

| Macå†…å­˜é…ç½® | max_seq_length | batch_size | gradient_steps | LoRA rå€¼ | target_modules |
|------------|----------------|------------|----------------|---------|----------------|
| 32GB | 1024-1536 | 1 | 8-16 | 8 | ä»…attention (q,k,v,o) |
| 64GB | 2048 | 1 | 4-8 | 16 | æ‰€æœ‰7ä¸ªæ¨¡å— |
| 96GB+ | 2048-3072 | 1-2 | 4 | 32 | æ‰€æœ‰7ä¸ªæ¨¡å— |

### 2. è§£å†³å†…å­˜æº¢å‡ºé—®é¢˜

å¦‚æœé‡åˆ°`MPS backend out of memory`é”™è¯¯ï¼Œè¯·å°è¯•ä»¥ä¸‹æ­¥éª¤ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰ï¼š

1. **å‡å°‘åºåˆ—é•¿åº¦**ï¼š
   - å°†`max_seq_length`é™è‡³1024ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
   - è¿™æ˜¯æœ€æœ‰æ•ˆçš„å†…å­˜èŠ‚çœæ–¹å¼

2. **é™ä½LoRAå‚æ•°**ï¼š
   - å°†LoRAçš„`r`å€¼ä»32é™è‡³16æˆ–8
   - å‡å°‘`target_modules`ï¼Œä»…ä¿ç•™`["q_proj", "k_proj", "v_proj", "o_proj"]`

3. **è°ƒæ•´æ‰¹å¤„ç†è®¾ç½®**ï¼š
   - ç¡®ä¿`per_device_train_batch_size=1`
   - å¢åŠ `gradient_accumulation_steps`å€¼ï¼ˆ8æˆ–æ›´é«˜ï¼‰

4. **æ·»åŠ å†…å­˜ä¼˜åŒ–ç¯å¢ƒå˜é‡**ï¼š
   ```bash
   export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0
   export PYTORCH_ENABLE_MPS_FALLBACK=1
   ```

5. **å‡å°‘è®­ç»ƒæ•°æ®**ï¼š
   - ä½¿ç”¨`split="train[0:100]"`ç­‰ç­–ç•¥é™åˆ¶æ ·æœ¬æ•°é‡
   - åœ¨ä»£ç ä¸­é¢‘ç¹è°ƒç”¨`gc.collect()`å’Œ`torch.mps.empty_cache()`

### 3. MPSå…¼å®¹æ€§é—®é¢˜è§£å†³æ–¹æ¡ˆ

Qwen2å’ŒDeepSeek-R1-Distill-Qwenç³»åˆ—æ¨¡å‹åœ¨MPSä¸Šå¯èƒ½é‡åˆ°ä»¥ä¸‹å…¼å®¹æ€§é—®é¢˜ï¼š

1. **å¸¸è§é”™è¯¯ç±»å‹**ï¼š
   - `validateComputeFunctionArguments` ç›¸å…³é”™è¯¯
   - `gather_kernel_1: missing buffer binding at index 2` é”™è¯¯
   - **æ•°æ®ç±»å‹ä¸åŒ¹é…é”™è¯¯**ï¼šä¾‹å¦‚ `'mps.add' op requires the same element type for all operands and results`
   - **use_cacheä¸gradient_checkpointingå†²çª**ï¼š`use_cache=True` ä¸ `gradient_checkpointing=True` ä¸å…¼å®¹
   - å…¶ä»–MPSæ“ä½œæœªå®ç°çš„é”™è¯¯
   - **fp16æ··åˆç²¾åº¦è®­ç»ƒä¸å…¼å®¹**ï¼š`ValueError: fp16 mixed precision requires a GPU (not 'mps')`

2. **è§£å†³ç­–ç•¥**ï¼š
   - **ç»Ÿä¸€æ•°æ®ç±»å‹**ï¼šç¡®ä¿æ‰€æœ‰å¼ é‡éƒ½ä½¿ç”¨ç›¸åŒçš„æ•°æ®ç±»å‹ï¼ˆä½¿ç”¨float32è€Œéfloat16ï¼‰
   - **æ£€æŸ¥æƒé‡å’Œåç½®ç±»å‹åŒ¹é…**ï¼šç¡®ä¿æ¯ä¸ªå±‚çš„æƒé‡å’Œåç½®ä½¿ç”¨ç›¸åŒçš„æ•°æ®ç±»å‹
   - **æ˜¾å¼ç¦ç”¨ç¼“å­˜**ï¼šè®¾ç½®`model.config.use_cache = False`ä»¥é¿å…ä¸æ¢¯åº¦æ£€æŸ¥ç‚¹å†²çª
   - **æ™ºèƒ½ç¦ç”¨gradient_checkpointing**ï¼šåœ¨æ¨ç†å‰ä¸´æ—¶ç¦ç”¨gradient_checkpointingä»¥ä½¿ç”¨KVç¼“å­˜
   - **æ¨ç†å›é€€åˆ°CPU**ï¼šå½“MPSå‡ºç°ä¸å¯æ¢å¤çš„é”™è¯¯æ—¶è‡ªåŠ¨å›é€€åˆ°CPUå¤„ç†
   - **ç¦ç”¨fp16è®­ç»ƒ**ï¼šç¡®ä¿å°†`fp16=False`è®¾ç½®åœ¨`TrainingArguments`ä¸­
   - **æ·»åŠ ç¯å¢ƒå˜é‡**ï¼šä½¿ç”¨`PYTORCH_ENABLE_MPS_FALLBACK=1`å…è®¸æœªå®ç°çš„æ“ä½œå›é€€åˆ°CPU

3. **æ€§èƒ½å½±å“**ï¼š
   - æ¨ç†åœ¨CPUä¸Šé€Ÿåº¦ä¼šæ…¢ä¸€äº›ï¼Œä½†ç»“æœå‡†ç¡®
   - è®­ç»ƒè¿‡ç¨‹ä»èƒ½å……åˆ†åˆ©ç”¨MPSåŠ é€Ÿ
   - åˆç†é…ç½®å‚æ•°å¯ä»¥å‡å°‘æ•°æ®ç±»å‹è½¬æ¢çš„æ€§èƒ½æŸå¤±

### 4. è®­ç»ƒè¿‡ç¨‹ä¸­çš„MPSæ•°æ®ç±»å‹ä¸åŒ¹é…é”™è¯¯ä¿®å¤

å¦‚æœæ‚¨é‡åˆ°ç±»ä¼¼ä»¥ä¸‹é”™è¯¯ï¼š
```
'mps.add' op requires the same element type for all operands and results
%7 = "mps.add"(%6, %arg2) : (tensor<1x688x5120xf16>, tensor<5120xf32>) -> tensor<*xf32>
```

è¿™æ˜¯ç”±äºMPSåç«¯è¦æ±‚æ‰€æœ‰æ“ä½œæ•°å…·æœ‰ç›¸åŒçš„æ•°æ®ç±»å‹ï¼Œä½†æ¨¡å‹ä¸­å­˜åœ¨ä¸åŒç±»å‹çš„å¼ é‡ï¼ˆå¦‚ç¤ºä¾‹ä¸­çš„f16å’Œf32ï¼‰ã€‚æœ¬æŒ‡å—ä¸­å·²ç»å®ç°äº†ä»¥ä¸‹ä¿®å¤ï¼š

1. **ç»Ÿä¸€æ¨¡å‹åŠ è½½æ—¶çš„æ•°æ®ç±»å‹**ï¼š
   ```python
   model = AutoModelForCausalLM.from_pretrained(
       model_path,
       torch_dtype=torch.float32,  # ä½¿ç”¨float32è€Œéfloat16
       device_map="mps",
       # ...
   )
   ```

2. **ç‰¹æ®Šçš„MPSè®­ç»ƒå‡†å¤‡å‡½æ•°**ï¼š
   ```python
   def prepare_model_for_mps_training(model):
       # ç¡®ä¿æ‰€æœ‰æ³¨æ„åŠ›å±‚å’Œå‰é¦ˆå±‚çš„å‚æ•°ä½¿ç”¨ç›¸åŒæ•°æ®ç±»å‹
       for name, module in model.named_modules():
           # æ£€æŸ¥æƒé‡å’Œåç½®æ˜¯å¦ä½¿ç”¨ä¸åŒçš„æ•°æ®ç±»å‹
           # ...
       return model
   ```

3. **ç¦ç”¨KVç¼“å­˜ä¸æ¢¯åº¦æ£€æŸ¥ç‚¹ä¸€èµ·ä½¿ç”¨**ï¼š
   ```python
   model.config.use_cache = False  # é¿å…ä¸æ¢¯åº¦æ£€æŸ¥ç‚¹å†²çª
   ```

4. **ç¡®ä¿æ•°æ®åŠ è½½å™¨è¿”å›ç»Ÿä¸€ç±»å‹çš„å¼ é‡å¹¶åŒ…å«æ ‡ç­¾**ï¼š
   ```python
   def data_collator(features):
       # ...
       # åˆ›å»ºæ ‡ç­¾å¼ é‡
       batch["labels"] = batch["input_ids"].clone()
       
       # ç¡®ä¿æ‰€æœ‰å¼ é‡ä¸ºfloat32ç±»å‹
       for key, value in batch.items():
           if isinstance(value, torch.Tensor) and value.dtype == torch.float16:
               batch[key] = value.to(dtype=torch.float32)
       return batch
   ```

### 5. å…¶ä»–å¸¸è§è®­ç»ƒé”™è¯¯

1. **ç¼ºå°‘æ ‡ç­¾é”™è¯¯**ï¼šå¦‚æœæ‚¨é‡åˆ°ä»¥ä¸‹é”™è¯¯ï¼š
   ```
   ValueError: The model did not return a loss from the inputs, only the following keys: logits.
   ```
   
   è¿™æ„å‘³ç€æ¨¡å‹åœ¨è®­ç»ƒæ—¶æ²¡æœ‰æ”¶åˆ°`labels`å­—æ®µï¼Œæ— æ³•è®¡ç®—æŸå¤±å‡½æ•°ã€‚è§£å†³æ–¹æ³•æ˜¯ä¿®æ”¹æ•°æ®æ•´ç†å‡½æ•°ï¼Œç¡®ä¿å®ƒä¸ºæ¨¡å‹æä¾›è¾“å…¥çš„å‰¯æœ¬ä½œä¸ºæ ‡ç­¾ï¼š
   ```python
   batch["labels"] = batch["input_ids"].clone()
   ```

2. **æ•°æ®é›†åˆ—ä¸åŒ¹é…é”™è¯¯**ï¼šè¿™æ˜¯PEFTæ¨¡å‹çš„ä¸€ä¸ªå¸¸è§é—®é¢˜ï¼Œè§£å†³æ–¹æ³•æ˜¯åœ¨`TrainingArguments`ä¸­è®¾ç½®ï¼š
   ```python
   remove_unused_columns=False
   ```

ä»¥ä¸Šä¿®å¤æªæ–½åº”è¯¥èƒ½å¤Ÿè§£å†³MPSåç«¯ä¸Šçš„å¸¸è§è®­ç»ƒé—®é¢˜ã€‚

## ä¸ƒã€è®­ç»ƒæ—¥å¿—è§£è¯»ä¸å¸¸è§é—®é¢˜

è®­ç»ƒæ—¥å¿—ç¤ºä¾‹ï¼š

> wandb: â­ï¸ View project at https://wandb.ai/anony-mouse-733470366999568055/Lora-DeepSeek-R1-Distill-Qwen-14B-Mac?apiKey=c16d51f0be89758603632573346321aab91cbb6f
> wandb: ğŸš€ View run at https://wandb.ai/anony-mouse-733470366999568055/Lora-DeepSeek-R1-Distill-Qwen-14B-Mac/runs/9yxzquav?apiKey=c16d51f0be89758603632573346321aab91cbb6f


### 1. ç†è§£è®­ç»ƒæŸå¤±å’ŒæŒ‡æ ‡

å¾®è°ƒç»“æŸåï¼Œå¯ä»¥çœ‹åˆ°ç±»ä¼¼ä¸‹é¢çš„è®­ç»ƒæ—¥å¿—ï¼š

```
21 {'loss': 2.4172, 'grad_norm': 0.2653520107269287, 'learning_rate': 4e-05, 'epoch': 0.04}
22 {'loss': 2.4697, 'grad_norm': 0.2983250916004181, 'learning_rate': 8e-05, 'epoch': 0.08}
23 {'loss': 2.5866, 'grad_norm': 0.30050748586654663, 'learning_rate': 0.00012, 'epoch': 0.12}
24 {'loss': 2.508, 'grad_norm': 0.3454345464706421, 'learning_rate': 0.00016, 'epoch': 0.16}
25 {'loss': 2.5712, 'grad_norm': 0.4334571063518524, 'learning_rate': 0.0002, 'epoch': 0.2}
26 {'loss': 2.328, 'grad_norm': 0.49836465716362, 'learning_rate': 0.0001866666666666667, 'epoch': 0.24}
27 {'loss': 2.2054, 'grad_norm': 0.4785458743572235, 'learning_rate': 0.00017333333333333334, 'epoch': 0.28}
28 {'loss': 2.2087, 'grad_norm': 0.40370047092437744, 'learning_rate': 0.00016, 'epoch': 0.32}
29 {'loss': 1.9788, 'grad_norm': 0.3458884060382843, 'learning_rate': 0.00014666666666666666, 'epoch': 0.36}
30 {'loss': 2.1806, 'grad_norm': 0.2959587872028351, 'learning_rate': 0.00013333333333333334, 'epoch': 0.4}
31 {'loss': 1.8023, 'grad_norm': 0.2639026343822479, 'learning_rate': 0.00012, 'epoch': 0.44}
32 {'loss': 1.9279, 'grad_norm': 0.26907879114151, 'learning_rate': 0.00010666666666666667, 'epoch': 0.48}
33 {'loss': 1.8455, 'grad_norm': 0.28270483016967773, 'learning_rate': 9.333333333333334e-05, 'epoch': 0.52}
34 {'loss': 1.8335, 'grad_norm': 0.24587252736091614, 'learning_rate': 8e-05, 'epoch': 0.56}
35 {'loss': 1.7411, 'grad_norm': 0.21421942114830017, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.6}
36 {'loss': 1.67, 'grad_norm': 0.21373698115348816, 'learning_rate': 5.333333333333333e-05, 'epoch': 0.64}
37 {'loss': 1.7019, 'grad_norm': 0.20923933386802673, 'learning_rate': 4e-05, 'epoch': 0.68}
38 {'loss': 1.6447, 'grad_norm': 0.2079884558916092, 'learning_rate': 2.6666666666666667e-05, 'epoch': 0.72}
39 {'loss': 1.6787, 'grad_norm': 0.19983869791030884, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.76}
40 {'loss': 1.6625, 'grad_norm': 0.21983186900615692, 'learning_rate': 0.0, 'epoch': 0.8}
41 {'train_runtime': 98940.9247, 'train_samples_per_second': 0.002, 'train_steps_per_second': 0.0, 'train_loss': 2.048112761974335, 'epoch': 0.8}
42 è®­ç»ƒå®Œæˆã€‚æ­¥æ•°: 20
```

#### æŸå¤±å€¼(Loss)å˜åŒ–è¶‹åŠ¿åˆ†æ

ä»æ—¥å¿—ä¸­å¯ä»¥å°†æŸå¤±å˜åŒ–åˆ’åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼š

```
åˆå§‹é˜¶æ®µï¼ˆæ­¥éª¤21-25ï¼‰: 2.4172 â†’ 2.4697 â†’ 2.5866 â†’ 2.508 â†’ 2.5712 (é¢„çƒ­é˜¶æ®µï¼ŒæŸå¤±æ³¢åŠ¨ä¸Šå‡)
ä¸­æœŸé˜¶æ®µï¼ˆæ­¥éª¤26-33ï¼‰: 2.328 â†’ 2.2054 â†’ 2.2087 â†’ 1.9788 â†’ 2.1806 â†’ 1.8023 â†’ 1.9279 â†’ 1.8455 (è½¬æŠ˜ï¼Œå¼€å§‹ä¸‹é™)
åæœŸé˜¶æ®µï¼ˆæ­¥éª¤34-40ï¼‰: 1.8335 â†’ 1.7411 â†’ 1.67 â†’ 1.7019 â†’ 1.6447 â†’ 1.6787 â†’ 1.6625 (ç¨³å®šä¸‹é™)
```

**æŸå¤±å˜åŒ–ç‰¹ç‚¹**ï¼š
- **åˆå§‹é˜¶æ®µ**ï¼šæŸå¤±å€¼åœ¨2.4-2.6ä¹‹é—´æ³¢åŠ¨ä¸Šå‡ï¼Œè¿™ä¸å­¦ä¹ ç‡é¢„çƒ­æœŸé—´çš„æ­£å¸¸ç°è±¡ä¸€è‡´
- **ä¸­æœŸé˜¶æ®µ**ï¼šæŸå¤±å¼€å§‹æ˜¾è‘—ä¸‹é™ï¼Œä½†ä»æœ‰æ³¢åŠ¨ï¼Œè¡¨æ˜æ¨¡å‹æ­£åœ¨é€‚åº”æ•°æ®
- **åæœŸé˜¶æ®µ**ï¼šæŸå¤±æ›´åŠ ç¨³å®šåœ°ä¸‹é™ï¼Œæœ€ç»ˆç¨³å®šåœ¨çº¦1.65å·¦å³
- **æ€»ä½“é™å¹…**ï¼šä»åˆå§‹çš„çº¦2.5é™è‡³1.6ï¼Œé™å¹…çº¦36%ï¼Œè¡¨æ˜å¾®è°ƒéå¸¸æœ‰æ•ˆ

#### æ¢¯åº¦èŒƒæ•°(grad_norm)æ¼”å˜

æ¢¯åº¦èŒƒæ•°æ˜¾ç¤ºäº†ä¸€ä¸ªæ¸…æ™°çš„"å±±å³°"æ¨¡å¼ï¼š

```
åˆå§‹é˜¶æ®µ: 0.265 â†’ 0.298 â†’ 0.300 â†’ 0.345 â†’ 0.433 (é€æ¸ä¸Šå‡)
ä¸­æœŸé˜¶æ®µ: 0.498 â†’ 0.478 â†’ 0.403 â†’ 0.345 â†’ 0.295 â†’ 0.263 â†’ 0.269 â†’ 0.282 (è¾¾åˆ°å³°å€¼åä¸‹é™)
åæœŸé˜¶æ®µ: 0.245 â†’ 0.214 â†’ 0.213 â†’ 0.209 â†’ 0.207 â†’ 0.199 â†’ 0.219 (ç¨³å®šåœ¨è¾ƒä½æ°´å¹³)
```

**æ¢¯åº¦å˜åŒ–ç‰¹ç‚¹**ï¼š
- æ¢¯åº¦èŒƒæ•°å…ˆä¸Šå‡åä¸‹é™ï¼Œå‘ˆ"å±±å³°"å½¢çŠ¶ï¼Œæœ€é«˜è¾¾åˆ°çº¦0.5ï¼Œæœ€ç»ˆç¨³å®šåœ¨0.2å·¦å³
- è¿™ç§æ¨¡å¼ä¸å­¦ä¹ ç‡å˜åŒ–ç›¸å‘¼åº”ï¼Œè¡¨æ˜å‚æ•°è°ƒæ•´å¹…åº¦å…ˆå¢å¼ºåå‡å¼±
- åæœŸæ¢¯åº¦èŒƒæ•°ä¿æŒåœ¨é€‚ä¸­æ°´å¹³ï¼ˆ0.2å·¦å³ï¼‰ï¼Œè¡¨æ˜æ¨¡å‹é€æ¸æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜è§£

#### å­¦ä¹ ç‡(learning_rate)ç­–ç•¥å‰–æ

å­¦ä¹ ç‡é‡‡ç”¨äº†å…¸å‹çš„"é¢„çƒ­-å¹³å°-è¡°å‡"ä¸‰é˜¶æ®µç­–ç•¥ï¼š

```
é¢„çƒ­é˜¶æ®µ: 4e-05 â†’ 8e-05 â†’ 0.00012 â†’ 0.00016 â†’ 0.0002 (çº¿æ€§å¢åŠ )
å¹³å°é˜¶æ®µ: 0.000187 â†’ 0.000173 â†’ 0.00016 (çŸ­æš‚å¹³å°åå¼€å§‹ä¸‹é™)
è¡°å‡é˜¶æ®µ: 0.000147 â†’ ... â†’ 1.33e-05 â†’ 0.0 (çº¿æ€§è¡°å‡)
```

**å­¦ä¹ ç‡ç­–ç•¥ç‰¹ç‚¹**ï¼š
- é¢„çƒ­é˜¶æ®µï¼ˆæ­¥éª¤21-25ï¼‰ï¼šå­¦ä¹ ç‡ä»4e-05çº¿æ€§å¢åŠ åˆ°0.0002
- å¹³å°è¿‡æ¸¡ï¼ˆæ­¥éª¤26-28ï¼‰ï¼šå­¦ä¹ ç‡çŸ­æš‚ç»´æŒåœ¨é«˜ä½
- è¡°å‡é˜¶æ®µï¼ˆæ­¥éª¤29-40ï¼‰ï¼šå­¦ä¹ ç‡çº¿æ€§é™ä½ç›´è‡³0
- è¿™ç§ç­–ç•¥æœ‰æ•ˆé¿å…äº†è®­ç»ƒåˆæœŸçš„ä¸ç¨³å®šæ€§ï¼ŒåŒæ—¶å…è®¸æ¨¡å‹åœ¨åæœŸç²¾ç»†è°ƒæ•´

#### è®­ç»ƒæ•ˆç‡æŒ‡æ ‡è§£è¯»

ä»è®­ç»ƒçš„æœ€ç»ˆç»Ÿè®¡å¯ä»¥å¾—åˆ°ä»¥ä¸‹æ•ˆç‡æŒ‡æ ‡ï¼š

```
{'train_runtime': 98940.9247, 'train_samples_per_second': 0.002, 'train_steps_per_second': 0.0, 'train_loss': 2.048112761974335, 'epoch': 0.8}
```

**æ•ˆç‡åˆ†æ**ï¼š
- æ€»è®­ç»ƒæ—¶é—´ï¼šçº¦27.5å°æ—¶ï¼ˆ98940ç§’ï¼‰
- å¤„ç†é€Ÿåº¦ï¼š0.002æ ·æœ¬/ç§’
- å®Œæˆäº†0.8ä¸ªè®­ç»ƒå‘¨æœŸ
- å¹³å‡è®­ç»ƒæŸå¤±ï¼š2.048
- æ¯æ­¥è€—æ—¶ï¼šçº¦4947ç§’ï¼ˆ82.5åˆ†é’Ÿï¼‰

#### ç»¼åˆè¯„ä¼°

1. **å¾®è°ƒæˆåŠŸåº¦**ï¼š
   - æŸå¤±æ˜¾è‘—ä¸‹é™ï¼ˆçº¦36%ï¼‰è¡¨æ˜å¾®è°ƒéå¸¸æœ‰æ•ˆ
   - æ¢¯åº¦èŒƒæ•°çš„å˜åŒ–åˆç†ï¼Œæ²¡æœ‰å‡ºç°æ¢¯åº¦çˆ†ç‚¸æˆ–æ¶ˆå¤±
   - å­¦ä¹ ç‡ç­–ç•¥æ‰§è¡Œè‰¯å¥½ï¼Œéµå¾ªäº†æœ€ä½³å®è·µ

2. **æ¨¡å‹æ”¶æ•›çŠ¶å†µ**ï¼š
   - æ¨¡å‹åœ¨è®­ç»ƒç»“æŸæ—¶ä»åœ¨ç¨³å®šæ”¹è¿›ï¼ˆæŸå¤±ä»æœ‰ä¸‹é™è¶‹åŠ¿ï¼‰
   - æœ€ç»ˆå‡ ä¸ªæ­¥éª¤çš„æŸå¤±æ³¢åŠ¨è¾ƒå°ï¼Œè¡¨æ˜æ¨¡å‹æ¥è¿‘å±€éƒ¨æœ€ä¼˜è§£
   - æŸå¤±å€¼é™è‡³1.6å·¦å³è¡¨æ˜æ¨¡å‹å¯¹è®­ç»ƒæ•°æ®æœ‰äº†å¾ˆå¥½çš„æ‹Ÿåˆ

3. **è®­ç»ƒæ—¶é—´ä¸èµ„æºåˆ©ç”¨**ï¼š
   - åœ¨Apple Siliconä¸Šå¾®è°ƒ14Bæ¨¡å‹è€—æ—¶çº¦27.5å°æ—¶ï¼Œç¬¦åˆé¢„æœŸ
   - æ¯æ­¥è®­ç»ƒæ—¶é—´ï¼ˆçº¦82.5åˆ†é’Ÿï¼‰åæ˜ äº†Macè®¾å¤‡ä¸Šå¤§æ¨¡å‹å¾®è°ƒçš„ç°å®æ€§èƒ½

è¿™æ¬¡å¾®è°ƒDeepSeek-R1-Distill-Qwen-14Bæ¨¡å‹åœ¨Apple Siliconè®¾å¤‡ä¸Šå–å¾—äº†è‰¯å¥½çš„æ•ˆæœï¼ŒéªŒè¯äº†æœ¬æ–‡æ¡£ä¸­çš„æ–¹æ³•å’Œå‚æ•°è®¾ç½®æ˜¯æœ‰æ•ˆçš„ã€‚ä»æŒ‡æ ‡å˜åŒ–æ¥çœ‹ï¼Œæ¨¡å‹å‚æ•°è°ƒæ•´æ˜¯ç¨³å¥çš„ï¼Œæ²¡æœ‰å‡ºç°è¿‡æ‹Ÿåˆæˆ–è®­ç»ƒä¸ç¨³å®šçš„è¿¹è±¡ã€‚

### 2. å¸¸è§è­¦å‘Šè§£æ

#### MallocStackLoggingè­¦å‘Š

å¦‚æœæ‚¨åœ¨ä½¿ç”¨wandbæ—¶çœ‹åˆ°ç±»ä¼¼ä»¥ä¸‹è­¦å‘Šï¼š

```
wandb-core(2754) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
```

**è§£é‡Š**ï¼šè¿™æ˜¯Weights & Biases (wandb)å·¥å…·åœ¨MacOSä¸Šçš„ä¸€ä¸ªå·²çŸ¥é—®é¢˜ï¼Œwandbå°è¯•å…³é—­ä¸€ä¸ªæœ¬æ¥å°±æ²¡æœ‰å¯ç”¨çš„å†…å­˜è·Ÿè¸ªåŠŸèƒ½ã€‚

**å½±å“**ï¼šå®Œå…¨æ²¡æœ‰å½±å“ï¼Œå¯ä»¥å®‰å…¨å¿½ç•¥ã€‚è¿™ä¸ä¼šå½±å“è®­ç»ƒè¿‡ç¨‹ã€æ€§èƒ½æˆ–ç»“æœè´¨é‡ã€‚

#### CUDAç›¸å…³è­¦å‘Š

å¦‚æœçœ‹åˆ°ç±»ä¼¼ä»¥ä¸‹è­¦å‘Šï¼š

```
UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment
```

**è§£é‡Š**ï¼šè¿™æ˜¯å› ä¸ºPyTorchæ£€æµ‹åˆ°æ— æ³•åˆå§‹åŒ–CUDAç¯å¢ƒï¼Œåœ¨Apple Siliconä¸Šå±äºæ­£å¸¸æƒ…å†µã€‚

**å½±å“**ï¼šæ— éœ€æ‹…å¿ƒï¼Œä»£ç ä¼šè‡ªåŠ¨ä½¿ç”¨MPSåç«¯è€ŒéCUDAã€‚

### 3. åˆç†çš„è®­ç»ƒæ—¶é—´é¢„æœŸ

åœ¨Apple Siliconè®¾å¤‡ä¸Šè¿›è¡Œå¤§è¯­è¨€æ¨¡å‹å¾®è°ƒæ—¶ï¼Œè®­ç»ƒæ—¶é—´ä¼šæ¯”ä¸“ä¸šGPUæ…¢å¾ˆå¤šï¼š

- **14Bæ¨¡å‹è®­ç»ƒä¸€ä¸ªstepçš„æ—¶é—´**ï¼š
  - 32GB Mac (M1/M2 Pro/Max): çº¦500-800ç§’/step
  - 64GB Mac (M1/M2 Max/Ultra): çº¦300-500ç§’/step
  - å¯¹æ¯”: RTX 4090: çº¦10-30ç§’/step

- **å®Œæˆ20æ­¥è®­ç»ƒé¢„è®¡æ€»æ—¶é—´**ï¼š
  - 32GB Mac: çº¦28-44å°æ—¶
  - 64GB Mac: çº¦17-28å°æ—¶
  - å¯¹æ¯”: RTX 4090: çº¦3-10å°æ—¶

**æç¤º**ï¼šé‰´äºé•¿æ—¶é—´è®­ç»ƒçš„ç‰¹æ€§ï¼Œè¯·ç¡®ä¿æ‚¨çš„Macæœ‰è‰¯å¥½çš„æ•£çƒ­æ¡ä»¶ï¼Œå¹¶è€ƒè™‘åˆ†é˜¶æ®µè®­ç»ƒï¼Œå®šæœŸä¿å­˜æ£€æŸ¥ç‚¹ã€‚

### 4. Macä¸NVIDIA GPUçš„è®­ç»ƒæ•ˆç‡å·®å¼‚åŸå› 

å³ä½¿Macå…·æœ‰è¾ƒå¤§çš„ç»Ÿä¸€å†…å­˜(ä¾‹å¦‚64GB)ï¼Œä½†è®­ç»ƒæ•ˆç‡ä»ä½äºåªæœ‰24GBæ˜¾å­˜çš„RTX 4090ï¼Œä¸»è¦åŸå› åŒ…æ‹¬ï¼š

1. **å†…å­˜æ¶æ„å·®å¼‚**ï¼š
   - RTX 4090: 24GB**ä¸“ç”¨**é«˜é€ŸGDDR6Xæ˜¾å­˜(~1TB/så¸¦å®½)
   - Mac: ç»Ÿä¸€å†…å­˜åŒæ—¶è¢«ç³»ç»Ÿå’Œå…¶ä»–åº”ç”¨ç¨‹åºå ç”¨

2. **å¤„ç†å™¨ä¼˜åŒ–ç¨‹åº¦**ï¼š
   - NVIDIA GPUä¸“ä¸ºæ·±åº¦å­¦ä¹ ä¼˜åŒ–ï¼Œå…·æœ‰å¤§é‡Tensoræ ¸å¿ƒ
   - Apple Siliconçš„MLåŠ é€Ÿå™¨é’ˆå¯¹æ¨ç†æ›´ä¼˜åŒ–ï¼Œå¯¹è®­ç»ƒçš„ä¼˜åŒ–è¾ƒå°‘

3. **è½¯ä»¶æ ˆæˆç†Ÿåº¦**ï¼š
   - CUDAç”Ÿæ€ç³»ç»Ÿéå¸¸æˆç†Ÿï¼Œé’ˆå¯¹è®­ç»ƒæœ‰æ·±åº¦ä¼˜åŒ–
   - MPSåç«¯ç›¸å¯¹è¾ƒæ–°ï¼Œä¼˜åŒ–ç¨‹åº¦ä¸å¦‚CUDA

4. **æ•°æ®ç±»å‹é™åˆ¶**ï¼š
   - RTX 4090æ”¯æŒé«˜æ•ˆçš„FP16/BF16è®­ç»ƒ
   - MPSè®­ç»ƒå¿…é¡»ä½¿ç”¨FP32ä»¥é¿å…å…¼å®¹æ€§é—®é¢˜ï¼Œè¿™ä½¿å†…å­˜éœ€æ±‚åŠ å€

å› æ­¤ï¼Œå³ä½¿Macæ‹¥æœ‰æ›´å¤§çš„æ€»å†…å­˜ï¼Œä½†ç”±äºä¸Šè¿°å› ç´ ï¼Œåœ¨LLMå¾®è°ƒä»»åŠ¡ä¸Šä»éœ€ä½¿ç”¨æ›´ä¿å®ˆçš„å‚æ•°è®¾ç½®ã€‚


### å¾®è°ƒå‰æ¨¡å‹æ¨ç†ç»“æœ

```
% python r1-lora-peft-mac.py
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: anony-mouse-733470366999568055 to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /Users/yangjiefeng/Documents/unsloth-finetune/wandb/run-20250309_114503-sjw44dc1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run curious-forest-13
wandb: â­ï¸ View project at https://wandb.ai/anony-mouse-733470366999568055/Lora-DeepSeek-R1-Distill-Qwen-14B-Mac?apiKey=c16d51f0be89758603632573346321aab91cbb6f
wandb: ğŸš€ View run at https://wandb.ai/anony-mouse-733470366999568055/Lora-DeepSeek-R1-Distill-Qwen-14B-Mac/runs/sjw44dc1?apiKey=c16d51f0be89758603632573346321aab91cbb6f
wandb: WARNING Do NOT share these links with anyone. They can be used to claim your runs.
/Users/yangjiefeng/Documents/unsloth-finetune/unsloth-env/lib/python3.11/site-packages/transformers/utils/hub.py:106: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
æ­£åœ¨ä»æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹: /Users/yangjiefeng/Documents/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B
å¼€å§‹åŠ è½½æ¨¡å‹ï¼Œè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:56<00:00, 14.22s/it]
Model loaded: Qwen2ForCausalLM
ä½¿ç”¨MPSè¿›è¡Œæ¨ç†...
Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.
/Users/yangjiefeng/Documents/unsloth-finetune/unsloth-env/lib/python3.11/site-packages/transformers/pytorch_utils.py:338: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  test_elements = torch.tensor(test_elements)
### å¾®è°ƒå‰æ¨¡å‹æ¨ç†ç»“æœï¼š

  <think>
å¥½ï¼Œæˆ‘ç°åœ¨è¦è§£å†³è¿™ä¸ªåŒ»å­¦é—®é¢˜ã€‚é¦–å…ˆï¼Œæ‚£è€…æ˜¯ä¸€ä¸ª70å²çš„ç”·æ€§ï¼Œä¸»è¯‰æ˜¯èƒ¸ç—›å’Œå‘•åï¼Œå·²ç»æŒç»­äº†16ä¸ªå°æ—¶ã€‚å¿ƒç”µå›¾æ˜¾ç¤ºä¸‹å£å¯¼è”å’Œå³èƒ¸å¯¼è”çš„STæ®µæŠ¬é«˜ï¼Œå¹…åº¦åœ¨0.1åˆ°0.3mVä¹‹é—´ã€‚è¿™å¯èƒ½æç¤ºå¿ƒè‚Œæ¢—æ­»ï¼Œå°¤å…¶æ˜¯ä¸‹å£æ¢—æ­»ï¼Œå¯èƒ½å½±å“åˆ°å³å¿ƒå®¤ã€‚

æ¥ä¸‹æ¥ï¼Œæ‚£è€…æ¥å—äº†è¡¥æ¶²æ²»ç–—ï¼Œä½†è¡€å‹é™åˆ°äº†80/60mmHgï¼Œè¿™æ˜¯ä¸€ä¸ªä½è¡€å‹çš„çŠ¶æ€ã€‚åŒæ—¶ï¼Œæ‚£è€…å‡ºç°äº†å‘¼å¸å›°éš¾ï¼Œä¸èƒ½å¹³å§ï¼Œä½“æ£€å‘ç°åŒè‚ºæœ‰å¤§é‡æ°´æ³¡éŸ³ï¼Œè¿™æç¤ºè‚ºæ°´è‚¿ã€‚è¿™äº›ç—‡çŠ¶ç»“åˆèµ·æ¥ï¼Œå¯èƒ½æ„å‘³ç€æ‚£è€…å‘ç”Ÿäº†å¿ƒæºæ€§ä¼‘å…‹æˆ–æ€¥æ€§å¿ƒåŠ›è¡°ç«­ã€‚

é¦–å…ˆï¼Œæˆ‘éœ€è¦è€ƒè™‘æ‚£è€…çš„å¿ƒè„çŠ¶å†µã€‚ä¸‹å£å¿ƒè‚Œæ¢—æ­»é€šå¸¸å½±å“å³å¿ƒå®¤ï¼Œå¯èƒ½å¯¼è‡´å¿ƒè¾“å‡ºé‡å‡å°‘ï¼Œè¿›è€Œå¼•èµ·ä¼‘å…‹å’Œè‚ºæ°´è‚¿ã€‚è¡¥æ¶²åè¡€å‹ä¸‹é™ï¼Œå¯èƒ½æ˜¯å› ä¸ºå¿ƒè„æ— æ³•æœ‰æ•ˆå¤„ç†æ¶²ä½“ï¼Œå¯¼è‡´æ¶²ä½“æ½´ç•™ï¼Œè¿›è€Œå¼•å‘è‚ºæ°´è‚¿ã€‚

æ¥ä¸‹æ¥ï¼Œè¯ç‰©å¤„ç†æ–¹é¢ï¼Œåº”è¯¥è€ƒè™‘ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š

1. **å¸æ°§**ï¼šæ‚£è€…æœ‰å‘¼å¸å›°éš¾ï¼Œä¸èƒ½å¹³å§ï¼ŒåŒè‚ºæœ‰å¤§é‡æ°´æ³¡éŸ³ï¼Œå¸æ°§å¯ä»¥ç¼“è§£ç¼ºæ°§ç—‡çŠ¶ã€‚

2. **åˆ©å°¿å‰‚**ï¼šå¦‚å‘‹å¡ç±³ï¼Œç”¨äºå‡å°‘è‚ºæ°´è‚¿ï¼Œå¸®åŠ©æ’å‡ºå¤šä½™çš„æ¶²ä½“ï¼Œæ”¹å–„å‘¼å¸å›°éš¾ã€‚

3. **æ­£æ€§è‚ŒåŠ›è¯ç‰©**ï¼šå¦‚å¤šå·´èƒºæˆ–å»ç”²è‚¾ä¸Šè…ºç´ ï¼Œç”¨äºæå‡è¡€å‹å’Œå¿ƒè¾“å‡ºé‡ï¼Œæ”¹å–„å¿ƒè„åŠŸèƒ½ã€‚

4. **ç¡é…¸ç”˜æ²¹**ï¼šè™½ç„¶æ‚£è€…è¡€å‹å·²ç»å¾ˆä½ï¼Œä½†ç¡é…¸ç”˜æ²¹å¯ä»¥æ‰©å¼ è‚ºåŠ¨è„‰ï¼Œå‡è½»è‚ºæ°´è‚¿ï¼ŒåŒæ—¶è°¨æ…ä½¿ç”¨ä»¥é¿å…è¿›ä¸€æ­¥é™ä½è¡€å‹ã€‚

5. **æŠ—ç”Ÿç´ å’ŒæŠ—å‘•åè¯ç‰©**ï¼šå¤„ç†ä¼´éšç—‡çŠ¶ï¼Œå¦‚å‘•åï¼Œå¯èƒ½ä½¿ç”¨ç”²æ°§æ°¯æ™®èƒºï¼ŒåŒæ—¶æ’é™¤æ„ŸæŸ“å¯èƒ½ã€‚

6. **ç´§æ€¥PCIæˆ–æº¶æ “æ²»ç–—**ï¼šå¦‚æœæ‚£è€…æœ‰ç¦å¿Œç—‡æˆ–æ— æ³•ç«‹å³è¿›è¡ŒPCIï¼Œå¯èƒ½è€ƒè™‘æº¶æ “æ²»ç–—ï¼Œä»¥æ¢å¤å¿ƒè‚Œè¡€æµã€‚

æ­¤å¤–ï¼Œéœ€è¦è€ƒè™‘æ‚£è€…çš„è‚¾åŠŸèƒ½ï¼Œå› ä¸ºå‘‹å¡ç±³å’Œåˆ©å°¿å‰‚çš„ä½¿ç”¨å¯èƒ½å½±å“è‚¾è„ï¼Œéœ€ç›‘æµ‹è‚¾åŠŸèƒ½ã€‚åŒæ—¶ï¼Œè¡¥æ¶²è¦è°¨æ…ï¼Œé¿å…åŠ é‡è‚ºæ°´è‚¿ã€‚

æœ€åï¼Œæ‚£è€…å¯èƒ½éœ€è¦ç´§æ€¥çš„å¿ƒè„ä»‹å…¥æ²»ç–—ï¼Œå¦‚PCIï¼Œä»¥å¼€é€šé—­å¡çš„å† çŠ¶åŠ¨è„‰ï¼Œæ”¹å–„å¿ƒè‚Œä¾›è¡€ï¼Œè¿›è€Œæ”¹å–„å¿ƒè„åŠŸèƒ½å’Œä¼‘å…‹çŠ¶æ€ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¯ç‰©å¤„ç†åº”åŒ…æ‹¬å¸æ°§ã€åˆ©å°¿ã€æ­£æ€§è‚ŒåŠ›è¯ç‰©ã€ç¡é…¸ç”˜æ²¹ï¼Œä»¥åŠå¤„ç†å‘•åç—‡çŠ¶ï¼ŒåŒæ—¶è€ƒè™‘ç´§æ€¥PCIæˆ–æº¶æ “æ²»ç–—ã€‚
</think>

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚£è€…çš„ç—‡çŠ¶å’Œæ£€æŸ¥ç»“æœæç¤ºå¿ƒæºæ€§ä¼‘å…‹å’Œæ€¥æ€§è‚ºæ°´è‚¿ï¼Œæœ€å¯èƒ½çš„è¯Šæ–­æ˜¯æ€¥æ€§å¿ƒè‚Œæ¢—æ­»å¯¼è‡´çš„å¿ƒæºæ€§ä¼‘å…‹ã€‚ä»¥ä¸‹æ˜¯æ°å½“çš„è¯ç‰©å¤„ç†æ­¥éª¤ï¼š

1. **å¸æ°§**ï¼šç«‹å³ç»™äºˆé«˜æµé‡æ°§ç–—ï¼Œç¼“è§£ä½æ°§è¡€ç—‡ã€‚

2. **åˆ©å°¿å‰‚**ï¼šä½¿ç”¨å‘‹å¡ç±³ï¼ˆ40-80 mgé™è„‰æ³¨å°„ï¼‰ä»¥å‡å°‘è‚ºæ°´è‚¿ï¼Œæ”¹å–„å‘¼å¸å›°éš¾ã€‚

3. **æ­£æ€§è‚ŒåŠ›è¯ç‰©**ï¼šå¼€å§‹å¤šå·´èƒºï¼ˆåˆå§‹å‰‚é‡ä¸ºæ¯åˆ†é’Ÿ2-5å¾®å…‹ï¼Œæ ¹æ®éœ€è¦è°ƒæ•´ï¼‰æˆ–å»ç”²è‚¾ä¸Šè…ºç´ ï¼ˆæ¯åˆ†é’Ÿ0.5-5å¾®å…‹ï¼‰ä»¥æå‡è¡€å‹å’Œå¿ƒè¾“å‡ºé‡ã€‚

4. **ç¡é…¸ç”˜æ²¹**ï¼šè°¨æ…ä½¿ç”¨ï¼Œä»¥æ‰©å¼ è‚ºåŠ¨è„‰ï¼Œå‡è½»è‚ºæ°´è‚¿ï¼ŒåŒæ—¶ç›‘æµ‹è¡€å‹ï¼Œé¿å…è¿‡ä½ã€‚

5. **æŠ—å‘•åæ²»ç–—**ï¼šä½¿ç”¨ç”²æ°§æ°¯æ™®èƒºï¼ˆ10 mgé™è„‰æ³¨å°„ï¼‰ä»¥æ§åˆ¶å‘•åã€‚

6. **æŠ—ç”Ÿç´ **ï¼šè‹¥è€ƒè™‘è‚ºéƒ¨æ„ŸæŸ“ï¼Œä½¿ç”¨æŠ—ç”Ÿç´ å¦‚å¤´å­¢æ›²æ¾ï¼ˆ1-2 gé™è„‰æ»´æ³¨ï¼‰ã€‚

7. **è¡¥æ¶²ç®¡ç†**ï¼šæ ¹æ®è¡€æµåŠ¨åŠ›å­¦çŠ¶æ€è°ƒæ•´ï¼Œé¿å…è¿‡åº¦è¡¥æ¶²åŠ é‡æ°´è‚¿ã€‚

8. **ç´§æ€¥PCIæˆ–æº¶æ “**ï¼šå°½å¿«è¿›è¡ŒPCIä»¥æ¢å¤å¿ƒè‚Œè¡€æµï¼Œæˆ–åœ¨æ— æ³•PCIæ—¶è€ƒè™‘æº¶æ “æ²»ç–—ï¼Œå¦‚ä½¿ç”¨æ›¿ç½—éç­ã€‚

åŒæ—¶ï¼Œç›‘æµ‹è‚¾åŠŸèƒ½å’Œç”µè§£è´¨ï¼Œç¡®ä¿è¯ç‰©å‰‚é‡é€‚å½“ã€‚ç«‹å³è½¬é€è‡³å¿ƒè„ä¸­å¿ƒè¿›è¡Œè¿›ä¸€æ­¥å¤„ç†ã€‚
åˆ†æ‰¹å¤„ç†å‚æ•°è½¬æ¢ï¼Œé¿å…å†…å­˜å³°å€¼...
å¤„ç†æ³¨æ„åŠ›å±‚å‚æ•°...
å¤„ç†å‰é¦ˆå±‚å‚æ•°...
å‚æ•°è½¬æ¢å®Œæˆï¼
trainable params: 103,219,200 || all params: 14,873,252,864 || trainable%: 0.6940
Trainable params: None
Dataset loaded: 200 examples
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
å¼€å§‹è®­ç»ƒ: 50 æ­¥ï¼Œæ‰¹æ¬¡å¤§å°: 1ï¼Œæ¢¯åº¦ç´¯ç§¯: 6
```



## å…«ã€å¾®è°ƒåæ¨¡å‹çš„æ­£ç¡®ä½¿ç”¨ä¸å¸¸è§é—®é¢˜

### 1. å®‰å…¨ç»ˆæ­¢å¡ä½çš„æ¨ç†è¿›ç¨‹

å¦‚æœæ‚¨åœ¨å¾®è°ƒåè¿›è¡Œæ¨ç†æµ‹è¯•æ—¶é‡åˆ°è¿›ç¨‹å¡ä½çš„æƒ…å†µï¼ˆä¾‹å¦‚è¶…è¿‡2å°æ—¶æ²¡æœ‰è¾“å‡ºï¼‰ï¼Œå¯ä»¥å®‰å…¨åœ°ä½¿ç”¨`Ctrl+C`ç»ˆæ­¢å½“å‰è¿›ç¨‹ã€‚è¿™ä¸ä¼šå½±å“å·²ç»å®Œæˆçš„è®­ç»ƒå’Œä¿å­˜çš„æ¨¡å‹æƒé‡ã€‚

å¾®è°ƒçš„æˆæœï¼ˆé€‚é…å™¨æƒé‡ï¼‰å·²ç»å®Œå…¨ä¿å­˜åœ¨`outputs/checkpoint-XX`ç›®å½•ä¸­ï¼ˆå…¶ä¸­XXæ˜¯æœ€åä¸€ä¸ªæ£€æŸ¥ç‚¹çš„æ­¥æ•°ï¼Œé€šå¸¸æ˜¯æ‚¨è®¾ç½®çš„`max_steps`å€¼ï¼‰ã€‚

### 2. ç†è§£LoRAå¾®è°ƒçš„è¾“å‡ºç»“æ„

ä½¿ç”¨LoRA/PEFTè¿›è¡Œå¾®è°ƒæ—¶ï¼Œè¾“å‡ºçš„ä¸æ˜¯å®Œæ•´æ¨¡å‹ï¼Œè€Œæ˜¯ä¸€ç»„é€‚é…å™¨æƒé‡ã€‚è¿™äº›æƒé‡éœ€è¦ä¸åŸå§‹åŸºç¡€æ¨¡å‹ç»“åˆä½¿ç”¨ã€‚æ£€æŸ¥ç‚¹ç›®å½•é€šå¸¸åŒ…å«ï¼š

- `adapter_config.json` - é€‚é…å™¨é…ç½®ä¿¡æ¯
- `adapter_model.safetensors` - é€‚é…å™¨æƒé‡æ–‡ä»¶
- `training_args.bin` - è®­ç»ƒå‚æ•°
- å…¶ä»–è®­ç»ƒçŠ¶æ€æ–‡ä»¶

### 3. æ­£ç¡®åŠ è½½å’Œä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹

ä»¥ä¸‹æ˜¯æ­£ç¡®åŠ è½½å’Œä½¿ç”¨å¾®è°ƒåæ¨¡å‹çš„è„šæœ¬ç¤ºä¾‹ï¼š

```python
import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel, PeftConfig

# æ¸…ç†å†…å­˜å‡½æ•°
def clean_memory():
    """æ¸…ç†GPUå’ŒCPUå†…å­˜"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    elif hasattr(torch.mps, 'empty_cache'):
        torch.mps.empty_cache()
    import gc
    gc.collect()

# æ™ºèƒ½æ¨ç†å‡½æ•° - ä¸è®­ç»ƒè„šæœ¬ä¸­ç›¸åŒ
def smart_inference(model, inputs, max_new_tokens=1200, temperature=0.7):
    """æ™ºèƒ½æ¨ç†å‡½æ•° - åªæœ‰åœ¨ç‰¹å®šæ“ä½œå¤±è´¥æ—¶å›é€€åˆ°CPU"""
    try:
        # å°è¯•ç›´æ¥ä½¿ç”¨MPSè¿›è¡Œæ¨ç†
        print("ä½¿ç”¨MPSè¿›è¡Œæ¨ç†...")
        # ç¦ç”¨gradient_checkpointingä»¥ä½¿ç”¨KVç¼“å­˜
        was_gradient_checkpointing = False
        if hasattr(model, "is_gradient_checkpointing"):
            was_gradient_checkpointing = model.is_gradient_checkpointing
            if was_gradient_checkpointing:
                model.gradient_checkpointing_disable()
            
        # ç¡®ä¿æ‰€æœ‰è¾“å…¥å¼ é‡çš„æ•°æ®ç±»å‹ä¸€è‡´
        for k, v in inputs.items():
            if isinstance(v, torch.Tensor) and v.dtype == torch.float16:
                inputs[k] = v.to(dtype=torch.float32)
                
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            do_sample=True,
            use_cache=True,  # å¯ç”¨KVç¼“å­˜
        )
        
        # æ¢å¤gradient_checkpointingçŠ¶æ€
        if was_gradient_checkpointing:
            model.gradient_checkpointing_enable()
            
        return outputs
    except RuntimeError as e:
        error_msg = str(e)
        if "MPS backend out of memory" in error_msg:
            print(f"é‡åˆ°MPSå†…å­˜é”™è¯¯: {error_msg}")
            print("æ¸…ç†å†…å­˜å¹¶å°è¯•æ™ºèƒ½å›é€€ç­–ç•¥...")
            
            # æ¸…ç†å†…å­˜
            clean_memory()
            
            try:
                # å°è¯•ä½¿ç”¨æ›´å°çš„max_new_tokenså’Œæ›´ä½çš„æ¸©åº¦
                print("å°è¯•ä½¿ç”¨æ›´ä¿å®ˆçš„ç”Ÿæˆå‚æ•°...")
                # ç¦ç”¨gradient_checkpointingä»¥ä½¿ç”¨KVç¼“å­˜
                if hasattr(model, "is_gradient_checkpointing"):
                    was_gradient_checkpointing = model.is_gradient_checkpointing
                    if was_gradient_checkpointing:
                        model.gradient_checkpointing_disable()
                    
                # ç¡®ä¿æ‰€æœ‰è¾“å…¥å¼ é‡çš„æ•°æ®ç±»å‹ä¸€è‡´
                for k, v in inputs.items():
                    if isinstance(v, torch.Tensor) and v.dtype == torch.float16:
                        inputs[k] = v.to(dtype=torch.float32)
                        
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens // 2,  # å‡åŠ
                    temperature=0.1,  # æ›´ç¡®å®šæ€§çš„ç”Ÿæˆ
                    do_sample=False,  # ç¦ç”¨é‡‡æ ·
                    use_cache=True,
                )
                
                # æ¢å¤gradient_checkpointingçŠ¶æ€
                if was_gradient_checkpointing:
                    model.gradient_checkpointing_enable()
                    
                return outputs
            except RuntimeError as e2:
                print(f"ç¬¬äºŒæ¬¡å°è¯•ä¹Ÿå¤±è´¥: {str(e2)}")
                # å¦‚æœä»ç„¶å¤±è´¥ï¼Œå°†æ¨ç†ç§»è‡³CPU
                print("MPSæ¨ç†ä»ç„¶å¤±è´¥ï¼Œå›é€€åˆ°CPU...")
                model.to("cpu")
                cpu_inputs = {k: v.to("cpu") for k, v in inputs.items() if hasattr(v, "to")}
                
                with torch.no_grad():
                    outputs = model.generate(
                        **cpu_inputs,
                        max_new_tokens=max_new_tokens,
                        temperature=temperature,
                        do_sample=True,
                        use_cache=True,  # CPUä¸Šå¯ä»¥å®‰å…¨ä½¿ç”¨
                    )
                
                # æ¨ç†åç§»å›MPS
                model.to("mps")
                return outputs
        elif "does not match" in error_msg or "requires the same element type" in error_msg:
            # å¤„ç†æ•°æ®ç±»å‹ä¸åŒ¹é…é”™è¯¯
            print(f"é‡åˆ°æ•°æ®ç±»å‹ä¸åŒ¹é…é”™è¯¯: {error_msg}")
            print("å°è¯•ç»Ÿä¸€æ•°æ®ç±»å‹ä¸ºfloat32...")
            
            # ç¡®ä¿æ‰€æœ‰è¾“å…¥å¼ é‡çš„æ•°æ®ç±»å‹ä¸€è‡´ä¸ºfloat32
            for k, v in inputs.items():
                if isinstance(v, torch.Tensor):
                    inputs[k] = v.to(dtype=torch.float32)
            
            # ç¦ç”¨gradient_checkpointingä»¥ä½¿ç”¨KVç¼“å­˜
            if hasattr(model, "is_gradient_checkpointing"):
                was_gradient_checkpointing = model.is_gradient_checkpointing
                if was_gradient_checkpointing:
                    model.gradient_checkpointing_disable()
                
            try:
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    do_sample=True,
                    use_cache=True,
                )
                
                # æ¢å¤gradient_checkpointingçŠ¶æ€
                if was_gradient_checkpointing:
                    model.gradient_checkpointing_enable()
                    
                return outputs
            except RuntimeError:
                # å¦‚æœä»ç„¶å¤±è´¥ï¼Œå°†æ¨ç†ç§»è‡³CPU
                print("MPSæ¨ç†ä»ç„¶å¤±è´¥ï¼Œå›é€€åˆ°CPU...")
                model.to("cpu")
                cpu_inputs = {k: v.to("cpu") for k, v in inputs.items() if hasattr(v, "to")}
                
                with torch.no_grad():
                    outputs = model.generate(
                        **cpu_inputs,
                        max_new_tokens=max_new_tokens,
                        temperature=temperature,
                        do_sample=True,
                        use_cache=True,
                    )
                
                # æ¨ç†åç§»å›MPS
                model.to("mps")
                return outputs
        else:
            # å¦‚æœä¸æ˜¯å·²çŸ¥é”™è¯¯ï¼Œé‡æ–°æŠ›å‡º
            raise

# ä¸»å‡½æ•°
def main():
    # 1. è®¾ç½®è·¯å¾„
    base_model_path = "~/Documents/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B"  # åŸå§‹æ¨¡å‹è·¯å¾„
    adapter_path = "outputs/checkpoint-20"  # é€‚é…å™¨è·¯å¾„ï¼Œä½¿ç”¨æœ€åä¸€ä¸ªæ£€æŸ¥ç‚¹
    
    # 2. åŠ è½½åˆ†è¯å™¨
    print("åŠ è½½åˆ†è¯å™¨...")
    tokenizer = AutoTokenizer.from_pretrained(base_model_path)
    
    # 3. åŠ è½½åŸºç¡€æ¨¡å‹
    print("åŠ è½½åŸºç¡€æ¨¡å‹...")
    model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        device_map="mps",  # ä½¿ç”¨MPSåç«¯
        torch_dtype=torch.float32,  # ä½¿ç”¨float32ä»¥é¿å…å…¼å®¹æ€§é—®é¢˜
    )
    
    # 4. åŠ è½½é€‚é…å™¨æƒé‡
    print(f"åŠ è½½é€‚é…å™¨æƒé‡: {adapter_path}")
    model = PeftModel.from_pretrained(model, adapter_path)
    
    # 5. è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()
    
    # 6. å‡†å¤‡è¾“å…¥æç¤º

    prompt_style = """ä»¥ä¸‹æ˜¯æè¿°ä»»åŠ¡çš„æŒ‡ä»¤ï¼Œä»¥åŠæä¾›æ›´å¤šä¸Šä¸‹æ–‡çš„è¾“å…¥ã€‚
    è¯·å†™å‡ºæ°å½“å®Œæˆè¯¥è¯·æ±‚çš„å›ç­”ã€‚
    åœ¨å›ç­”ä¹‹å‰ï¼Œè¯·ä»”ç»†æ€è€ƒé—®é¢˜ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªé€æ­¥çš„æ€ç»´é“¾ï¼Œä»¥ç¡®ä¿å›ç­”åˆä¹é€»è¾‘ä¸”å‡†ç¡®ã€‚
    ### Instruction:
    ä½ æ˜¯ä¸€ä½åœ¨ä¸´åºŠæ¨ç†ã€è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’æ–¹é¢å…·æœ‰ä¸“ä¸šçŸ¥è¯†çš„åŒ»å­¦ä¸“å®¶ã€‚
    è¯·å›ç­”ä»¥ä¸‹åŒ»å­¦é—®é¢˜ã€‚
    ### Question:
    {}
    ### Response:
    <think>{}"""
    # train_prompt_style = prompt_style + """
    #   </think>
    #   {}"""

    # æµ‹è¯•ç”¨åŒ»å­¦é—®é¢˜
    question = "ä¸€å70å²çš„ç”·æ€§æ‚£è€…å› èƒ¸ç—›ä¼´å‘•å16å°æ—¶å°±åŒ»ï¼Œå¿ƒç”µå›¾æ˜¾ç¤ºä¸‹å£å¯¼è”å’Œå³èƒ¸å¯¼è”STæ®µæŠ¬é«˜0.1~0.3mVï¼Œç»è¡¥æ¶²åè¡€å‹é™è‡³80/60mmHgï¼Œæ‚£è€…å‡ºç°å‘¼å¸å›°éš¾å’Œä¸èƒ½å¹³å§çš„ç—‡çŠ¶ï¼Œä½“æ£€å‘ç°åŒè‚ºæœ‰å¤§é‡æ°´æ³¡éŸ³ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæœ€æ°å½“çš„è¯ç‰©å¤„ç†æ˜¯ä»€ä¹ˆï¼Ÿ"
        
    # 7. å‡†å¤‡è¾“å…¥
    print("å‡†å¤‡è¾“å…¥...")
    model_input = prompt_style.format(question, "")
    inputs = tokenizer(model_input, return_tensors="pt").to("mps")
    
    # 8. æ‰§è¡Œæ¨ç†
    print("æ‰§è¡Œæ¨ç†...")
    with torch.no_grad():
        outputs = smart_inference(model, inputs, max_new_tokens=1200, temperature=0.7)
    
    # 9. è§£ç è¾“å‡º
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print("\n### å¾®è°ƒåæ¨¡å‹æ¨ç†ç»“æœï¼š")
    try:
        print(response.split("### Response:")[1])
    except IndexError:
        print("æ— æ³•åˆ†å‰²å“åº”ï¼Œæ˜¾ç¤ºå®Œæ•´è¾“å‡º:")
        print(response)

if __name__ == "__main__":
    main()
```

### 4. å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

#### æ¨ç†å¡ä½æˆ–æ— å“åº”

å¦‚æœæ¨ç†è¿‡ç¨‹å¡ä½æˆ–é•¿æ—¶é—´æ— å“åº”ï¼š

1. **æ£€æŸ¥é€‚é…å™¨è·¯å¾„**ï¼šç¡®ä¿é€‚é…å™¨è·¯å¾„æŒ‡å‘æ­£ç¡®çš„æ£€æŸ¥ç‚¹ç›®å½•
2. **è®¾ç½®ç”Ÿæˆé™åˆ¶**ï¼šæ·»åŠ ä¸¥æ ¼çš„`max_new_tokens`å’Œ`max_time`é™åˆ¶
3. **ä½¿ç”¨CPUå›é€€**ï¼šå¦‚æœMPSæŒç»­å¤±è´¥ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨CPUè¿›è¡Œæ¨ç†
4. **åˆ†æ‰¹å¤„ç†é•¿æ–‡æœ¬**ï¼šå¯¹äºé•¿è¾“å…¥ï¼Œè€ƒè™‘åˆ†æ‰¹å¤„ç†ä»¥å‡å°‘å†…å­˜å‹åŠ›

#### æ•°æ®ç±»å‹ä¸åŒ¹é…é”™è¯¯

å¦‚æœé‡åˆ°æ•°æ®ç±»å‹ä¸åŒ¹é…é”™è¯¯ï¼š

1. **ç»Ÿä¸€æ•°æ®ç±»å‹**ï¼šç¡®ä¿æ‰€æœ‰å¼ é‡ä½¿ç”¨ç›¸åŒçš„æ•°æ®ç±»å‹ï¼ˆæ¨èfloat32ï¼‰
2. **é¿å…æ··åˆç²¾åº¦**ï¼šåœ¨MPSä¸Šé¿å…ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒå’Œæ¨ç†
3. **æ£€æŸ¥æ¨¡å‹é…ç½®**ï¼šç¡®ä¿æ¨¡å‹é…ç½®ä¸é€‚é…å™¨é…ç½®å…¼å®¹

#### å†…å­˜ä¸è¶³é”™è¯¯

å¦‚æœé‡åˆ°å†…å­˜ä¸è¶³é”™è¯¯ï¼š

1. **å‡å°‘ç”Ÿæˆå‚æ•°**ï¼šé™ä½`max_new_tokens`å’Œ`temperature`
2. **ç¦ç”¨é‡‡æ ·**ï¼šè®¾ç½®`do_sample=False`ä»¥å‡å°‘å†…å­˜ä½¿ç”¨
3. **æ¸…ç†ç¼“å­˜**ï¼šå®šæœŸè°ƒç”¨`torch.mps.empty_cache()`
4. **å‡å°‘æ‰¹é‡å¤§å°**ï¼šä½¿ç”¨è¾ƒå°çš„æ‰¹é‡å¤§å°è¿›è¡Œæ¨ç†

### 5. ä¿å­˜å®Œæ•´æ¨¡å‹ï¼ˆå¯é€‰ï¼‰

å¦‚æœæ‚¨å¸Œæœ›å°†é€‚é…å™¨ä¸åŸºç¡€æ¨¡å‹åˆå¹¶å¹¶ä¿å­˜ä¸ºå®Œæ•´æ¨¡å‹ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç ï¼š

```python
# åˆå¹¶é€‚é…å™¨å’ŒåŸºç¡€æ¨¡å‹
merged_model = model.merge_and_unload()

# ä¿å­˜å®Œæ•´æ¨¡å‹
merged_model_path = "DeepSeek-R1-Medical-Merged"
merged_model.save_pretrained(merged_model_path)
tokenizer.save_pretrained(merged_model_path)
print(f"åˆå¹¶åçš„æ¨¡å‹å·²ä¿å­˜åˆ°: {os.path.abspath(merged_model_path)}")
```

è¯·æ³¨æ„ï¼Œåˆå¹¶åçš„æ¨¡å‹å°†å ç”¨æ›´å¤šç£ç›˜ç©ºé—´ï¼Œä½†åœ¨æ¨ç†æ—¶ä¸å†éœ€è¦åŸå§‹åŸºç¡€æ¨¡å‹ã€‚




---

## é™„ï¼šåŸç‰ˆæ–‡ç« å†…å®¹


å‚è€ƒæ–‡ç« ï¼š[ã€Šå•å¡ RTX 4090 ç”¨ unsloth å’ŒåŒ»å­¦æ•°æ®å¾®è°ƒ DeepSeek-R1-Distill-Qwen-14Bã€‹](https://mp.weixin.qq.com/s?__biz=MzUxMTczMTY1Ng==&mid=2247483742&idx=1&sn=b90af9f3a12f182f5e48859b3b72a40a&scene=21#wechat_redirect)

åœ¨æ–‡ç« [ã€Šå•å¡4090å¾®è°ƒDeepSeek-R1-32Bã€‹](https://mp.weixin.qq.com/s?__biz=MzUxMTczMTY1Ng==&mid=2247483746&idx=1&sn=b7e16a70ffb903b64f28424677fb22c0&chksm=f96e73c5ce19fad3b1b26f56826a3e2470bcfbc8675995c6a47b2b9e466d19ba6ec2c6a32277&cur_album_id=3854852837976768515&scene=189#wechat_redirect)ä¸­ï¼Œæä¾›çš„è®­ç»ƒä»£ç å¦‚ä¸‹ï¼š


```python
import wandb
# ç™»å½• wandb.ai ç”¨äºå®éªŒè·Ÿè¸ª
wandb.login(key="æ”¾ç½®ä½ çš„wandb.aiç½‘ç«™ä¸Šçš„token")
# åˆå§‹åŒ–wandbé¡¹ç›®
run = wandb.init(
    project='Lora-R1-Distill-Qwen on Medical COT Dataset',
    job_type="training",
    anonymous="allow"
)

####################################################################################################
# 1.åŠ è½½æ¨¡å‹

# ä½¿ç”¨ unsloth ä¼˜åŒ–çš„ FastLanguageModel åŠ è½½æ¨¡å‹
from unsloth import FastLanguageModel
max_seq_length = 4096 # æœ€å¤§åºåˆ—é•¿åº¦
dtype = None          # æ•°æ®ç±»å‹ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©
load_in_4bit = True   # ä½¿ç”¨4bité‡åŒ–åŠ è½½æ¨¡å‹ä»¥èŠ‚çœæ˜¾å­˜

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨
model, tokenizer = FastLanguageModel.from_pretrained(
    #model_name = "unsloth/DeepSeek-R1-Distill-Qwen-7B",
    model_name = "/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
    local_files_only=True,  # é¿å…è”ç½‘
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    #token = hf_token, 
)
print(model)

####################################################################################################
# 2. å®šä¹‰æç¤ºæ¨¡æ¿ï¼Œå¹¶åœ¨å¾®è°ƒå‰åšä¸€æ¬¡æ¨ç†

prompt_style = """ä»¥ä¸‹æ˜¯æè¿°ä»»åŠ¡çš„æŒ‡ä»¤ï¼Œä»¥åŠæä¾›æ›´å¤šä¸Šä¸‹æ–‡çš„è¾“å…¥ã€‚
  è¯·å†™å‡ºæ°å½“å®Œæˆè¯¥è¯·æ±‚çš„å›ç­”ã€‚
  åœ¨å›ç­”ä¹‹å‰ï¼Œè¯·ä»”ç»†æ€è€ƒé—®é¢˜ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªé€æ­¥çš„æ€ç»´é“¾ï¼Œä»¥ç¡®ä¿å›ç­”åˆä¹é€»è¾‘ä¸”å‡†ç¡®ã€‚
  ### Instruction:
  ä½ æ˜¯ä¸€ä½åœ¨ä¸´åºŠæ¨ç†ã€è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’æ–¹é¢å…·æœ‰ä¸“ä¸šçŸ¥è¯†çš„åŒ»å­¦ä¸“å®¶ã€‚
  è¯·å›ç­”ä»¥ä¸‹åŒ»å­¦é—®é¢˜ã€‚
  ### Question:
  {}
  ### Response:
  <think>{}"""
train_prompt_style = prompt_style + """
  </think>
  {}"""

# æµ‹è¯•ç”¨åŒ»å­¦é—®é¢˜
question = "ä¸€å70å²çš„ç”·æ€§æ‚£è€…å› èƒ¸ç—›ä¼´å‘•å16å°æ—¶å°±åŒ»ï¼Œå¿ƒç”µå›¾æ˜¾ç¤ºä¸‹å£å¯¼è”å’Œå³èƒ¸å¯¼è”STæ®µæŠ¬é«˜0.1~0.3mVï¼Œç»è¡¥æ¶²åè¡€å‹é™è‡³80/60mmHgï¼Œæ‚£è€…å‡ºç°å‘¼å¸å›°éš¾å’Œä¸èƒ½å¹³å§çš„ç—‡çŠ¶ï¼Œä½“æ£€å‘ç°åŒè‚ºæœ‰å¤§é‡æ°´æ³¡éŸ³ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæœ€æ°å½“çš„è¯ç‰©å¤„ç†æ˜¯ä»€ä¹ˆï¼Ÿ"

# è®¾ç½®æ¨¡å‹ä¸ºæ¨ç†æ¨¡å¼
FastLanguageModel.for_inference(model) 
inputs = tokenizer([prompt_style.format(question, "")], return_tensors="pt").to("cuda")

# ç”Ÿæˆå›ç­”
outputs = model.generate(
    input_ids=inputs.input_ids,
    attention_mask=inputs.attention_mask,
    max_new_tokens=1200,
    use_cache=True,
)
response = tokenizer.batch_decode(outputs)
print("### å¾®è°ƒå‰æ¨¡å‹æ¨ç†ç»“æœï¼š")
print(response[0].split("### Response:")[1])

####################################################################################################
# 3. å¤„ç†æ•°æ®é›†

EOS_TOKEN = tokenizer.eos_token  # æ·»åŠ ç»“æŸç¬¦æ ‡è®°
#æ ¼å¼åŒ–æç¤ºå‡½æ•°,ç”¨äºå¤„ç†æ•°æ®é›†ä¸­çš„ç¤ºä¾‹
def formatting_prompts_func(examples):
    # ä»examplesä¸­æå–é—®é¢˜ã€æ€ç»´é“¾å’Œå›ç­”
    inputs = examples["Question"]      # åŒ»å­¦é—®é¢˜åˆ—è¡¨
    cots = examples["Complex_CoT"]     # æ€ç»´é“¾åˆ—è¡¨ 
    outputs = examples["Response"]     # å›ç­”åˆ—è¡¨
    
    # å­˜å‚¨æ ¼å¼åŒ–åçš„æ–‡æœ¬
    texts = []
    
    # éå†æ¯ä¸ªç¤ºä¾‹,å°†é—®é¢˜ã€æ€ç»´é“¾å’Œå›ç­”ç»„åˆæˆæŒ‡å®šæ ¼å¼
    for input, cot, output in zip(inputs, cots, outputs):
        # ä½¿ç”¨train_prompt_styleæ¨¡æ¿æ ¼å¼åŒ–æ–‡æœ¬,å¹¶æ·»åŠ ç»“æŸç¬¦
        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN
        texts.append(text)
        
    # è¿”å›æ ¼å¼åŒ–åçš„æ–‡æœ¬å­—å…¸
    return {
        "text": texts,
    }

# åŠ è½½æ•°æ®é›†å¹¶åº”ç”¨æ ¼å¼åŒ–
from datasets import load_dataset,load_from_disk
dataset = load_dataset(
    "json",  # æŒ‡å®šæ•°æ®æ ¼å¼ä¸º JSON
    data_files="/datasets/FreedomIntelligence/medical-o1-reasoning-SFT/medical_o1_sft_Chinese.json",
    #split="train[0:500]",  # åªå–å‰ 500 æ¡æ•°æ®
    trust_remote_code=True  # å…¼å®¹ remote code çš„è¡Œä¸º
)

# å¦‚æœè¿”å›çš„æ˜¯ DatasetDictï¼Œåˆ™å–å‡º "train" è¿™ä¸€éƒ¨åˆ†
if isinstance(dataset, dict):  
    dataset = dataset["train"]
    
dataset = dataset.map(formatting_prompts_func, batched = True,)
print(dataset)  # æŸ¥çœ‹æ•°æ®é›†ç»“æ„

####################################################################################################
# 4. é…ç½®è®­ç»ƒå‚æ•°å¯åŠ¨è®­ç»ƒ

model = FastLanguageModel.get_peft_model(
    model, 
    r=32,
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj", 
        "gate_proj", "up_proj", "down_proj",    
    ],
    lora_alpha=16,
    lora_dropout=0,  
    bias="none",  
    use_gradient_checkpointing="unsloth", 
    random_state=8137,
    use_rslora=False,  
    loftq_config=None,
)
print(model)

# é…ç½®è®­ç»ƒå‚æ•°å’Œåˆå§‹åŒ–è®­ç»ƒå™¨
from trl import SFTTrainer  
from transformers import TrainingArguments  
from unsloth import is_bfloat16_supported  

# åˆå§‹åŒ– SFT è®­ç»ƒå™¨
trainer = SFTTrainer(
    model=model,  
    tokenizer=tokenizer,  
    train_dataset=dataset,  
    dataset_text_field="text",  # æ•°æ®é›†å­—æ®µçš„åç§°
    max_seq_length=max_seq_length,  
    dataset_num_proc=2,  # æ•°æ®é›†å¤„ç†çš„å¹¶è¡Œè¿›ç¨‹æ•°ï¼Œæé«˜CPUåˆ©ç”¨ç‡
    args=TrainingArguments(
        per_device_train_batch_size=2, 
        gradient_accumulation_steps=4,   
        warmup_steps=5,  # é¢„çƒ­æ­¥æ•°,é€æ­¥å¢åŠ å­¦ä¹ ç‡
        learning_rate=2e-4,  # å­¦ä¹ ç‡
        lr_scheduler_type="linear",  # çº¿æ€§å­¦ä¹ ç‡è°ƒåº¦å™¨
        # max_steps=200,    # æœ€å¤§è®­ç»ƒæ­¥æ•°ï¼ˆä¸€æ­¥ = å¤„ç†ä¸€ä¸ªbatchçš„æ•°æ®ï¼‰
        fp16=not is_bfloat16_supported(),  # å¦‚æœä¸æ”¯æŒbf16åˆ™ä½¿ç”¨fp16
        bf16=is_bfloat16_supported(),      # å¦‚æœæ”¯æŒåˆ™ä½¿ç”¨bf16
        logging_steps=10,  # æ¯10æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—
        optim="adamw_8bit",  # ä½¿ç”¨8ä½AdamWä¼˜åŒ–å™¨èŠ‚çœæ˜¾å­˜ï¼Œå‡ ä¹ä¸å½±å“è®­ç»ƒæ•ˆæœ
        weight_decay=0.01,   # æƒé‡è¡°å‡ç³»æ•°,ç”¨äºæ­£åˆ™åŒ–ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
        seed=8137,  # éšæœºæ•°ç§å­
        output_dir="outputs",  # ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹å’Œè®­ç»ƒæ—¥å¿—
        run_name="medical-o1-sft-experiment",  # æ˜¾å¼è®¾ç½® wandb è¿è¡Œåç§°ï¼Œé¿å…è­¦å‘Š
    ),
)

# å¼€å§‹è®­ç»ƒ
print(f"trainer.args.max_steps: {trainer.args.max_steps}")
print(f"trainer.args.num_train_epochs: {trainer.args.num_train_epochs}")
trainer.train()
print(f"Total training steps: {trainer.state.max_steps}")
print(f"Total epochs: {trainer.state.epoch}")

####################################################################################################
# 5. å¾®è°ƒåçš„æ¨¡å‹åšä¸€æ¬¡æ¨ç†

FastLanguageModel.for_inference(model)  
inputs = tokenizer([prompt_style.format(question, "")], return_tensors="pt").to("cuda")

# ç”Ÿæˆå›ç­”
outputs = model.generate(
    input_ids=inputs.input_ids, # è¾“å…¥tokençš„idåºåˆ—
    attention_mask=inputs.attention_mask,  # æ³¨æ„åŠ›æ©ç ,ç”¨äºæ ‡è®°æœ‰æ•ˆè¾“å…¥ä½ç½®
    max_new_tokens=1200, # ç”Ÿæˆçš„æœ€å¤§æ–°tokenæ•°é‡
    use_cache=True, # æ˜¯å¦ä½¿ç”¨KVç¼“å­˜åŠ é€Ÿç”Ÿæˆ
)

response = tokenizer.batch_decode(outputs)
print("### å¾®è°ƒåæ¨¡å‹æ¨ç†ç»“æœï¼š")
print(response[0].split("### Response:")[1])

####################################################################################################
# 6. ä¿å­˜æ¨¡å‹

new_model_local = "DeepSeek-R1-Medical-COT-Qwen-32B"
model.save_pretrained(new_model_local) 
tokenizer.save_pretrained(new_model_local)

# ä¿å­˜åˆå¹¶åçš„16bitæ¨¡å‹
model.save_pretrained_merged(new_model_local, tokenizer, save_method = "merged_16bit",)

# ä¿å­˜ä¸º GGUF æ¨¡å‹
# model.save_pretrained_gguf("DeepSeek-R1-Qwen-32B-Medical-COT-GGUF", tokenizer,)
```

## æ€»ç»“ï¼šmacOSç‰ˆæœ¬ä¸åŸç‰ˆä»£ç çš„ä¸»è¦è°ƒæ•´

å¯¹æ¯”åŸå§‹RTX 4090ä¸Šä½¿ç”¨Unslothæ¡†æ¶çš„ä»£ç ä¸é€‚é…macOSçš„ç‰ˆæœ¬ï¼Œä¸»è¦è¿›è¡Œäº†ä»¥ä¸‹è°ƒæ•´ï¼š

### 1. æ¡†æ¶ä¸æ¶æ„è°ƒæ•´
- **æ¡†æ¶æ›¿æ¢**ï¼šå®Œå…¨æ”¾å¼ƒäº†Unslothæ¡†æ¶ï¼Œæ”¹ç”¨æ ‡å‡†çš„Hugging Face PEFTåº“å®ç°LoRAå¾®è°ƒ
- **åç«¯é€‚é…**ï¼šä»CUDAåç«¯è½¬æ¢åˆ°Apple Siliconçš„MPSåç«¯ï¼Œå¹¶é’ˆå¯¹MPSçš„ç‰¹æ€§è¿›è¡Œäº†å¤§é‡å…¼å®¹æ€§ä¿®å¤
- **è®­ç»ƒå™¨å˜æ›´**ï¼šä»Unslothçš„`SFTTrainer`æ›¿æ¢ä¸ºæ ‡å‡†çš„Hugging Face `Trainer`

### 2. å†…å­˜ä¼˜åŒ–ç­–ç•¥
- **æ·»åŠ ä¸»åŠ¨å†…å­˜ç®¡ç†**ï¼šå®šä¹‰äº†`clean_memory()`å‡½æ•°ï¼Œåœ¨å…³é”®èŠ‚ç‚¹ä¸»åŠ¨æ¸…ç†GPUå’ŒCPUå†…å­˜
- **ç¯å¢ƒå˜é‡ä¼˜åŒ–**ï¼šæ·»åŠ äº†å¤šä¸ªMPSç›¸å…³ç¯å¢ƒå˜é‡æ§åˆ¶å†…å­˜ä½¿ç”¨å’Œé”™è¯¯å¤„ç†
- **æ‰¹é‡å¤„ç†å‚æ•°è½¬æ¢**ï¼šå®ç°äº†`safe_prepare_model_for_kbit_training`å‡½æ•°ï¼Œåˆ†æ‰¹å¤„ç†å‚æ•°è½¬æ¢ä»¥é¿å…å†…å­˜å³°å€¼

### 3. è®­ç»ƒå‚æ•°è°ƒæ•´
- **æ‰¹æ¬¡å¤§å°é™ä½**ï¼šä»`per_device_train_batch_size=2`é™ä½åˆ°`1`
- **æ¢¯åº¦ç´¯ç§¯å¢åŠ **ï¼šä»`gradient_accumulation_steps=4`å¢åŠ åˆ°`8`
- **åºåˆ—é•¿åº¦ç¼©çŸ­**ï¼šä»`max_seq_length=4096`é™ä½ä¸º`2048`
- **LoRAå‚æ•°å‡å°**ï¼šä»`r=32`å‡å°åˆ°`r=16`ä»¥èŠ‚çœå†…å­˜
- **æ•°æ®ç²¾åº¦è°ƒæ•´**ï¼šç¦ç”¨åŠç²¾åº¦(`fp16=False`ã€`bf16=False`)ï¼Œç»Ÿä¸€ä½¿ç”¨`float32`é¿å…MPSæ•°æ®ç±»å‹ä¸åŒ¹é…é”™è¯¯
- **ç¦ç”¨å¤šè¿›ç¨‹**ï¼šè®¾ç½®`dataloader_num_workers=0`é¿å…å¤šè¿›ç¨‹å¸¦æ¥çš„é¢å¤–å†…å­˜å¼€é”€

### 4. å…¼å®¹æ€§ä¿®å¤ä¸æ™ºèƒ½å›é€€
- **æ•°æ®ç±»å‹ç»Ÿä¸€**ï¼šå®ç°äº†ç¡®ä¿æ‰€æœ‰å¼ é‡ä½¿ç”¨ç›¸åŒæ•°æ®ç±»å‹(`float32`)çš„ç‰¹æ®Šå¤„ç†
- **æ™ºèƒ½æ¨ç†å›é€€**ï¼šæ·»åŠ äº†`smart_inference`å‡½æ•°ï¼Œåœ¨MPSåç«¯å¤±è´¥æ—¶è‡ªåŠ¨å›é€€åˆ°CPUæ¨ç†
- **æ¢¯åº¦æ£€æŸ¥ç‚¹ä¸KVç¼“å­˜å†²çªè§£å†³**ï¼šåœ¨ä¸åŒé˜¶æ®µé€‚å½“å¯ç”¨/ç¦ç”¨`gradient_checkpointing`å’Œ`use_cache`

### 5. é”™è¯¯å¤„ç†ä¸ç”¨æˆ·ä½“éªŒ
- **è¯¦ç»†é”™è¯¯åˆ†æ**ï¼šå¯¹å¸¸è§é”™è¯¯ç±»å‹æä¾›å…·ä½“çš„ä¿®å¤å»ºè®®
- **åˆ†çº§é…ç½®æŒ‡å—**ï¼šæ ¹æ®ä¸åŒMacå†…å­˜é…ç½®æä¾›ç›¸åº”çš„å‚æ•°è°ƒæ•´å»ºè®®
- **è·¯å¾„å¤„ç†æ”¹è¿›**ï¼šå¼ºè°ƒå¹¶è§£å†³äº†Pythonè„šæœ¬ä¸­`~/`è·¯å¾„ä¸ä¼šè‡ªåŠ¨å±•å¼€çš„é—®é¢˜

è¿™äº›è°ƒæ•´ä¸ä»…è§£å†³äº†Apple Siliconæ¶æ„ä¸MPSåç«¯çš„å…¼å®¹æ€§é—®é¢˜ï¼Œè¿˜é€šè¿‡ç²¾å¿ƒä¼˜åŒ–çš„å†…å­˜ç®¡ç†å’Œå‚æ•°è°ƒæ•´ï¼Œä½¿å¾—åœ¨å†…å­˜æœ‰é™çš„Macè®¾å¤‡ä¸Šä¹Ÿèƒ½ç¨³å®šåœ°å¾®è°ƒ14Bå‚æ•°çº§åˆ«çš„å¤§è¯­è¨€æ¨¡å‹ã€‚



## Macè®¾å¤‡å¾®è°ƒå¤§è¯­è¨€æ¨¡å‹çš„å…¶ä»–å‚è€ƒä¿¡æ¯

### éCUDAç¯å¢ƒä¸‹çš„å¤§æ¨¡å‹å¾®è°ƒå®è·µ

[æœ‰äº‘æŠ€æœ¯å›¢é˜Ÿçš„å®è·µ](https://mp.weixin.qq.com/s?__biz=MjM5NjUxNDIwNw==&mid=2654069282&idx=1&sn=ac26cc4dc52b0d63c11cfee95edd2187&chksm=bc35d7c842283b695ab733fa22829780c7e39ca62ae460fa4fd2cae2594bf175a0b4291eeb4e#rd)å±•ç¤ºäº†å¦‚ä½•åœ¨Mac Mç³»åˆ—èŠ¯ç‰‡ä¸Šä½¿ç”¨LoRAæŠ€æœ¯å¯¹DeepSeek Coderæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä¸ºä½¿ç”¨ Mac çš„å¼€å‘è€…æä¾›å‚è€ƒã€‚

#### 1. é¡¹ç›®æ¦‚è¿°

**ç›®æ ‡**ï¼š
- ä½¿ç”¨LoRAæŠ€æœ¯å¾®è°ƒDeepSeek Coder 7Bæ¨¡å‹
- ä¼˜åŒ–æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šçš„è¡¨ç°
- åœ¨Mac Mç³»åˆ—èŠ¯ç‰‡ä¸Šå®ç°å¾®è°ƒè®­ç»ƒå’Œæ¨ç†

**æŠ€æœ¯æ ˆ**ï¼š
- åŸºç¡€æ¨¡å‹ï¼šdeepseek-ai/deepseek-coder-7b-base
- è®­ç»ƒæ¡†æ¶ï¼šPyTorch + Transformers + PEFT
- ç¡¬ä»¶å¹³å°ï¼šMac Mç³»åˆ—èŠ¯ç‰‡(MPSåç«¯)

#### 2. å®ç°æµç¨‹

**æ•°æ®å‡†å¤‡**ï¼š
```python
training_data = [
    {
        "instruction": "å®ç°ä¸€ä¸ªäºŒåˆ†æŸ¥æ‰¾ç®—æ³•",
        "input": "",
        "output": """def binary_search(arr, target):
    left, right = 0, len(arr) - 1
    while left <= right:
        mid = (left + right) // 2
        if arr[mid] == target:
            return mid
        elif arr[mid] < target:
            left = mid + 1
        else:
            right = mid - 1
    return -1"""
    },
    {
        "instruction": "ç¼–å†™ä¸€ä¸ªå¿«é€Ÿæ’åºå‡½æ•°",
        "input": "",
        "output": """def quicksort(arr):
    if len(arr) <= 1:
        return arr
    pivot = arr[len(arr) // 2]
    left = [x for x in arr if x < pivot]
    middle = [x for x in arr if x == pivot]
    right = [x for x in arr if x > pivot]
    return quicksort(left) + middle + quicksort(right)"""
    }
]
```

**LoRAé…ç½®**ï¼š
```python
LORA_CONFIG = LoraConfig(
    task_type=TaskType.CAUSAL_LM,      # ä»»åŠ¡ç±»å‹:å› æœè¯­è¨€æ¨¡å‹
    r=8,                               # LoRAçŸ©é˜µçš„ç§©,æ§åˆ¶å¯è®­ç»ƒå‚æ•°é‡
    lora_alpha=32,                     # ç¼©æ”¾å› å­,æ§åˆ¶LoRAæ›´æ–°çš„é‡è¦æ€§
    lora_dropout=0.1,                  # Dropoutæ¯”ä¾‹,é˜²æ­¢è¿‡æ‹Ÿåˆ
    target_modules=[                   # éœ€è¦è®­ç»ƒçš„æ¨¡å—
        "q_proj",                      # æŸ¥è¯¢çŸ©é˜µæŠ•å½±å±‚
        "v_proj",                      # å€¼çŸ©é˜µæŠ•å½±å±‚  
        "k_proj",                      # é”®çŸ©é˜µæŠ•å½±å±‚
        "o_proj",                      # è¾“å‡ºæŠ•å½±å±‚
    ],
    bias="none",                       # æ˜¯å¦è®­ç»ƒåç½®é¡¹
    modules_to_save=None               # éœ€è¦å®Œæ•´ä¿å­˜çš„æ¨¡å—
)
```

**è®­ç»ƒå‚æ•°ä¼˜åŒ–**ï¼š
```python
training_args = TrainingArguments(
    output_dir="./deepseek-finetuned",
    per_device_train_batch_size=1,     # å—é™äºMç³»åˆ—èŠ¯ç‰‡å†…å­˜
    gradient_accumulation_steps=64,    # ç´¯ç§¯æ¢¯åº¦ä»¥æ¨¡æ‹Ÿå¤§æ‰¹é‡
    learning_rate=5e-5,
    num_train_epochs=3,
    gradient_checkpointing=True,       # å‡å°‘å†…å­˜ä½¿ç”¨
    max_grad_norm=0.3,                 # æ¢¯åº¦è£å‰ª,é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
    warmup_steps=50                    # é¢„çƒ­æ­¥æ•°
)
```

#### 3. Mac Mç³»åˆ—é€‚é…æŠ€å·§

**å†…å­˜ç®¡ç†**ï¼š
```python
# è®¾ç½®MPSå†…å­˜æ°´ä½
os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
os.environ['PYTORCH_MPS_LOW_WATERMARK_RATIO'] = '0.0'

# å®šæœŸæ¸…ç†å†…å­˜
if torch.backends.mps.is_available():
    torch.mps.empty_cache()
```

**æ€§èƒ½ä¼˜åŒ–ç­–ç•¥**ï¼š

1. **æ¨¡å‹ä¼˜åŒ–**ï¼š
   - KV Cacheä¼˜åŒ–ï¼šå¯ç”¨æ³¨æ„åŠ›ç¼“å­˜æœºåˆ¶,ä¼˜åŒ–ç¼“å­˜æ›´æ–°ç­–ç•¥
   - è®¡ç®—å›¾ä¼˜åŒ–ï¼šä½¿ç”¨torch.inferencemode(),å¯ç”¨é™æ€å›¾ä¼˜åŒ–
   - ç”Ÿæˆå‚æ•°è°ƒä¼˜ï¼šè‡ªé€‚åº”batch size,ä¼˜åŒ–é‡‡æ ·ç­–ç•¥

2. **å†…å­˜ä¼˜åŒ–**ï¼š
   - æ‰¹å¤„ç†ç­–ç•¥ï¼šåŠ¨æ€batch sizeè°ƒæ•´,æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ä¼˜åŒ–
   - æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼šé€‰æ‹©æ€§æ¿€æ´»æ£€æŸ¥ç‚¹,å¹³è¡¡å†…å­˜ä¸é€Ÿåº¦
   - æ˜¾å­˜ç®¡ç†ï¼šåŠæ—¶é‡Šæ”¾æ— ç”¨å¼ é‡,ä½¿ç”¨CPU offload,å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ

3. **æ•°æ®åŠ è½½ä¼˜åŒ–**ï¼š
   - é¢„å¤„ç†åŠ é€Ÿï¼šå¤šè¿›ç¨‹æ•°æ®åŠ è½½,æ•°æ®é¢„å–ä¸ç¼“å­˜
   - å†…å­˜ç®¡ç†ï¼šä½¿ç”¨å†…å­˜æ˜ å°„,æ¸è¿›å¼æ•°æ®åŠ è½½
   - IOä¼˜åŒ–ï¼šä½¿ç”¨å¼‚æ­¥åŠ è½½,ä¼˜åŒ–æ•°æ®æ ¼å¼

4. **è®­ç»ƒæµç¨‹ä¼˜åŒ–**ï¼š
   - ç›‘æ§ä¸è°ƒæ•´ï¼šå®æ—¶æ€§èƒ½åˆ†æ,è‡ªåŠ¨åŒ–å‚æ•°è°ƒä¼˜
   - å®¹é”™æœºåˆ¶ï¼šæ£€æŸ¥ç‚¹ä¿å­˜,è®­ç»ƒçŠ¶æ€æ¢å¤

#### 4. æ¨ç†ä¼˜åŒ–é…ç½®

```python
generation_config = {
    "max_length": 2048,               # ç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦
    "temperature": 0.7,               # é‡‡æ ·æ¸©åº¦,æ§åˆ¶ç”Ÿæˆçš„éšæœºæ€§
    "top_p": 0.95,                    # æ ¸é‡‡æ ·é˜ˆå€¼,æ§åˆ¶è¯è¡¨é‡‡æ ·èŒƒå›´
    "top_k": 50,                      # ä¿ç•™æ¦‚ç‡æœ€é«˜çš„kä¸ªtoken
    "do_sample": True,                # æ˜¯å¦ä½¿ç”¨é‡‡æ ·ç­–ç•¥
    "num_beams": 1,                   # beam searchçš„æŸå®½
    "repetition_penalty": 1.1,        # é‡å¤æƒ©ç½šå› å­,é¿å…é‡å¤ç”Ÿæˆ
    "early_stopping": True,           # æ˜¯å¦æå‰åœæ­¢ç”Ÿæˆ
    "pad_token_id": tokenizer.pad_token_id,  # padding tokençš„ID
    "eos_token_id": tokenizer.eos_token_id,  # ç»“æŸç¬¦tokençš„ID
}
```

#### 5. ä¸»è¦æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆ

**MPSåç«¯å†…å­˜ç®¡ç†**ï¼š
- å†…å­˜æ³„æ¼é—®é¢˜ï¼šMPSåç«¯ç¼“å­˜ç§¯ç´¯ã€å¼ é‡ç¢ç‰‡åŒ–ã€è‡ªåŠ¨å›æ”¶ä¸åŠæ—¶
- è§£å†³æ–¹æ¡ˆï¼šå®šæœŸæ‰‹åŠ¨æ¸…ç†ç¼“å­˜ã€ç›‘æ§å†…å­˜ä½¿ç”¨ã€ä¼˜åŒ–å¼ é‡ç”Ÿå‘½å‘¨æœŸ

**æ¶ˆè´¹çº§ç¡¬ä»¶ä¼˜åŒ–**ï¼š
- è®¡ç®—èƒ½åŠ›é™åˆ¶ï¼šå•GPUæ˜¾å­˜ä¸è¶³ã€è®¡ç®—é€Ÿåº¦ç“¶é¢ˆ
- è§£å†³æ–¹æ¡ˆï¼šæ¨¡å‹é‡åŒ–ã€æ¢¯åº¦ç´¯ç§¯ã€ä¼˜åŒ–å™¨å†…å­˜ä¼˜åŒ–

**è®­ç»ƒå’Œæ¨ç†æ€§èƒ½å¹³è¡¡**ï¼š
- è®­ç»ƒæ•ˆç‡ï¼šæ‰¹å¤„ç†å¤§å°å—é™ã€æ¢¯åº¦ç´¯ç§¯å¼€é”€å¤§
- æ¨ç†å»¶è¿Ÿï¼šé¦–æ¬¡æ¨ç†å»¶è¿Ÿé«˜ã€å“åº”æ—¶é—´ä¸ç¨³å®š
- è§£å†³æ–¹æ¡ˆï¼šåŠ¨æ€æ‰¹å¤„ç†ã€KVç¼“å­˜ä¼˜åŒ–ã€é¢„çƒ­æ¨¡å‹

#### 6. å‚è€ƒèµ„æº

- DeepSeek Coderå®˜æ–¹æ–‡æ¡£
- PEFTåº“æ–‡æ¡£
- PyTorch MPSæ–‡æ¡£
- LoRAè®ºæ–‡


### åŸºäºApple MLXæ¡†æ¶çš„å¤§æ¨¡å‹å¾®è°ƒ

[Apple MLXæ¡†æ¶](https://github.com/ml-explore/mlx)æ˜¯è‹¹æœå®˜æ–¹å¼€æºçš„é’ˆå¯¹Apple SiliconèŠ¯ç‰‡ä¼˜åŒ–çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œä¸ºå¼€å‘è€…æä¾›äº†åœ¨Macç­‰è®¾å¤‡ä¸Šç›´æ¥è¿›è¡Œæ¨¡å‹è®­ç»ƒå’Œéƒ¨ç½²çš„èƒ½åŠ›ã€‚[æ–‡ç« ](https://mp.weixin.qq.com/s?__biz=MzA5NDc3ODQxNQ==&mid=2247484394&idx=1&sn=aeeb89f13a5269cbda8ca6f31d198b89&chksm=915cb4f81cab6148fda2f458f969edec253f789c4279ba67cd946f6d0fdbc3eb2a93f32c389a#rd)ä»‹ç»åŸºäºMLXæ¡†æ¶åœ¨M1è®¾å¤‡ä¸Šå¾®è°ƒMistral-7Bæ¨¡å‹çš„å®è·µè¿‡ç¨‹ã€‚

#### 1. MLXæ¡†æ¶ç‰¹æ€§

- **ç†Ÿæ‚‰çš„API**ï¼šæä¾›ç±»ä¼¼NumPyçš„Python APIå’Œç±»ä¼¼PyTorchçš„é«˜çº§API
- **å¯ç»„åˆå‡½æ•°è½¬æ¢**ï¼šæ”¯æŒè‡ªåŠ¨å¾®åˆ†ã€è‡ªåŠ¨çŸ¢é‡åŒ–å’Œè®¡ç®—å›¾ä¼˜åŒ–
- **æƒ°æ€§è®¡ç®—**ï¼šåªåœ¨éœ€è¦æ—¶æ‰å…·ä½“åŒ–æ•°ç»„ï¼Œæé«˜è®¡ç®—æ•ˆç‡
- **åŠ¨æ€å›¾æ„å»º**ï¼šæ— éœ€é‡ç¼–è¯‘å³å¯æ”¯æŒå½¢çŠ¶å˜åŒ–ï¼Œç®€åŒ–è°ƒè¯•
- **å¤šè®¾å¤‡æ”¯æŒ**ï¼šå¯åœ¨CPUå’ŒGPUä¸Šè¿è¡Œï¼Œå……åˆ†åˆ©ç”¨ç¡¬ä»¶
- **ç»Ÿä¸€å†…å­˜æ¨¡å‹**ï¼šæ•°ç»„ä½äºå…±äº«å†…å­˜ï¼Œæ— éœ€è®¾å¤‡é—´æ•°æ®ç§»åŠ¨

#### 2. å¾®è°ƒç¯å¢ƒå‡†å¤‡

```bash
# è·å–MLXç¤ºä¾‹ä»£ç 
git clone https://github.com/ml-explore/mlx-examples
cd mlx-examples/lora

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# ä¸‹è½½Mistral-7Bæ¨¡å‹
curl -O https://files.mistral-7b-v0-1.mistral.ai/mistral-7B-v0.1.tar
tar -xf mistral-7B-v0.1.tar

# è½¬æ¢æ¨¡å‹æ ¼å¼ä¸ºMLXæ ¼å¼
python convert.py \
  --torch-model mistral-7B-v0.1 \
  --mlx-model mistral-7b-v0.1-mlx
```

#### 3. è®­ç»ƒæ•°æ®æ ¼å¼

MLXç¤ºä¾‹ä¸­ä½¿ç”¨WikiSQLæ•°æ®é›†è¿›è¡Œæ–‡æœ¬åˆ°SQLçš„è½¬æ¢è®­ç»ƒï¼Œæ•°æ®æ ¼å¼å¦‚ä¸‹ï¼š

```json
{
    "text": "table: 1-1000181-1\ncolumns: State/territory, Text/background colour, Format, Current slogan, Current series, Notes\nQ: Tell me what the notes are for South Australia \nA: SELECT Notes FROM 1-1000181-1 WHERE Current slogan = 'SOUTH AUSTRALIA'"
}
```

æ•°æ®é›†ç»“æ„ï¼š
- `table`: è¡¨åç§°
- `columns`: åˆ—åç§°åˆ—è¡¨
- `Q`: ç”¨æˆ·è‡ªç„¶è¯­è¨€é—®é¢˜
- `A`: å¯¹åº”çš„SQLæŸ¥è¯¢è¯­å¥

#### 4. å†…å­˜ä¼˜åŒ–ä¸å‚æ•°è°ƒæ•´

åœ¨M1è®¾å¤‡ä¸Šè¿›è¡Œå¾®è°ƒæ—¶ï¼Œå†…å­˜ç®¡ç†æ˜¯å…³é”®æŒ‘æˆ˜ã€‚ä»¥ä¸‹æ˜¯ä¼˜åŒ–å‚æ•°ï¼š

```bash
# ä¼˜åŒ–å†…å­˜ä½¿ç”¨çš„è®­ç»ƒå‘½ä»¤
python lora.py \
   --model mistral-7b-v0.1-mlx \
   --train \
   --batch-size 1 \          # é™ä½æ‰¹æ¬¡å¤§å°å‡å°‘å†…å­˜å ç”¨
   --lora-layers 4           # å‡å°‘å¾®è°ƒå±‚æ•°
```

**å…³é”®å‚æ•°è§£æ**ï¼š
- `--batch-size`ï¼šé»˜è®¤ä¸º4ï¼Œé™ä½è‡³1æˆ–2å¯æ˜¾è‘—å‡å°‘å†…å­˜æ¶ˆè€—
- `--lora-layers`ï¼šé»˜è®¤ä¸º16ï¼Œå¯é™è‡³8æˆ–4å‡å°‘åå‘ä¼ æ’­å†…å­˜ï¼Œä½†å¯èƒ½å½±å“å¾®è°ƒè´¨é‡
- æ•°æ®é›†é•¿åº¦ï¼šè¾ƒé•¿æ ·æœ¬éœ€è¦æ›´å¤šå†…å­˜ï¼Œå¯è€ƒè™‘åˆ†è§£ä¸ºæ›´å°åºåˆ—

#### 5. æ€§èƒ½ä¸è®­ç»ƒè¿‡ç¨‹

åœ¨32GBçš„M1 Macä¸Šï¼Œä½¿ç”¨ä¼˜åŒ–å‚æ•°åçš„è®­ç»ƒæ€§èƒ½ï¼š
- å¤„ç†é€Ÿåº¦ï¼šçº¦110 tokens/ç§’
- æŸå¤±å˜åŒ–ï¼šä»åˆå§‹2.265é™è‡³1.325ï¼ˆ800æ¬¡è¿­ä»£åï¼‰

**ä¸åŒå‚æ•°é…ç½®çš„æŸå¤±å˜åŒ–å¯¹æ¯”**ï¼š

| é…ç½® | è¿­ä»£æ¬¡æ•° | Loss |
|------|---------|------|
| batch=1, lora=4 | 800 | 1.325 |
| batch=2, lora=8 | 800 | 1.213 |
| batch=2, lora=8, æ›´å¤§æ•°æ®é›† | 800 | 1.360 |


#### 6. æ¨¡å‹è¯„ä¼°ä¸è¾“å‡ºç¤ºä¾‹

å¾®è°ƒåçš„æ¨¡å‹ç”¨äºè‡ªç„¶è¯­è¨€åˆ°SQLçš„è½¬æ¢ä»»åŠ¡ï¼š

```bash
python lora.py --model mistral-7b-v0.1-mlx \
               --adapter-file adapters_2_8.npz \
               --num-tokens 50 \
               --prompt "table: 1-10015132-16
columns: Player, No., Nationality, Position, Years in Toronto, School/Club Team
Q: What is terrence ross' nationality
A: "
```

**æ¨¡å‹è¾“å‡º**ï¼š
```
A: SELECT Nationality FROM 1-10015132-16 WHERE Player = 'Terrence Ross'
```

#### 7. ç»éªŒæ€»ç»“ä¸å»ºè®®

1. **å†…å­˜ç®¡ç†ç­–ç•¥**ï¼š
   - é™ä½æ‰¹æ¬¡å¤§å°æ˜¯æœ€ç›´æ¥æœ‰æ•ˆçš„å†…å­˜å‡å°‘æ–¹æ³•
   - é™ä½LoRAå±‚æ•°å¯å‡å°‘å†…å­˜ï¼Œä½†ä¼šå½±å“æ¨¡å‹è´¨é‡
   - 32GBå†…å­˜è®¾å¤‡å»ºè®®æœ€å¤§ä½¿ç”¨8ä¸ªLoRAå±‚

2. **æ•°æ®é›†è€ƒé‡**ï¼š
   - è®­ç»ƒé›†å¤§å°å¯¹ç»“æœå½±å“æ˜¾è‘—
   - å¯¹äºå¤æ‚ä»»åŠ¡ï¼Œ1000æ¡æ ·æœ¬é€šå¸¸ä¸è¶³ï¼Œå»ºè®®ä½¿ç”¨5000-10000æ¡

3. **é€‚ç”¨åœºæ™¯**ï¼š
   - RAGç³»ç»Ÿè‡ªå®šä¹‰ï¼šé’ˆå¯¹ç‰¹å®šé¢†åŸŸçŸ¥è¯†å¾®è°ƒ
   - å°å‹ä¸“ä¸šæ¨¡å‹ï¼šå¦‚æ–‡æœ¬åˆ°SQLè½¬æ¢å™¨
   - å¼€å‘å’Œæµ‹è¯•ï¼šæ— éœ€é«˜ç«¯GPUå³å¯è¿›è¡ŒåŸå‹å¼€å‘

#### 8. æŠ€æœ¯ä»·å€¼ä¸æ„ä¹‰

- **ç”Ÿæ€å¤šæ ·åŒ–**ï¼šä¸ºéNVIDIAè®¾å¤‡æä¾›å¤§æ¨¡å‹å¾®è°ƒèƒ½åŠ›
- **å¼€å‘é—¨æ§›é™ä½**ï¼šé™ä½ç¡¬ä»¶è¦æ±‚ï¼Œä½¿æ›´å¤šå¼€å‘è€…èƒ½å‚ä¸å¤§æ¨¡å‹å¼€å‘
- **è‹¹æœè®¾å¤‡ä¼˜åŠ¿**ï¼šå……åˆ†åˆ©ç”¨Apple Siliconç»Ÿä¸€å†…å­˜æ¶æ„
- **åº”ç”¨åœºæ™¯æ‰©å±•**ï¼šæ”¯æŒåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šè¿›è¡Œä¸ªæ€§åŒ–æ¨¡å‹é€‚é…

MLXæ¡†æ¶ä¸ºAppleè®¾å¤‡æä¾›äº†ä¸€æ¡ä¸ä¾èµ–NVIDIA CUDAçš„å¤§æ¨¡å‹å¾®è°ƒè·¯å¾„ï¼Œè™½ç„¶åœ¨å¤„ç†å¤§è§„æ¨¡è®­ç»ƒæ—¶ä»æœ‰å±€é™ï¼Œä½†å¯¹äºç‰¹å®šä»»åŠ¡çš„é€‚åº”æ€§å¾®è°ƒå·²ç»å±•ç°å‡ºå®ç”¨ä»·å€¼ï¼Œæœªæ¥éšç€æ¡†æ¶ä¼˜åŒ–å’ŒAppleç¡¬ä»¶å‡çº§ï¼Œè¿™ä¸€æ–¹æ¡ˆçš„å¯è¡Œæ€§å°†è¿›ä¸€æ­¥å¢å¼ºã€‚

#### 9. å‚è€ƒèµ„æº

- [MLX GitHubä»“åº“](https://github.com/ml-explore/mlx)
- [MLX LoRAå¾®è°ƒç¤ºä¾‹](https://github.com/ml-explore/mlx-examples/tree/main/lora)
- [å…«ä¸€èœåˆ€ï¼šåŸºäºApple MLXæ¡†æ¶çš„M1è®¾å¤‡ä¸Šå¤§æ¨¡å‹å¾®è°ƒå®è·µ](https://mp.weixin.qq.com/s?__biz=MzA5NDc3ODQxNQ==&mid=2247484394&idx=1&sn=aeeb89f13a5269cbda8ca6f31d198b89&chksm=915cb4f81cab6148fda2f458f969edec253f789c4279ba67cd946f6d0fdbc3eb2a93f32c389a#rd)


### Appleè®¾å¤‡æ·±åº¦å­¦ä¹ æ¡†æ¶

#### MPSä¸MLXæ¡†æ¶å¯¹æ¯”

MPSåç«¯å’ŒMLXæ¡†æ¶è™½ç„¶éƒ½åˆ©ç”¨Appleç¡¬ä»¶èƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨æ¶æ„å’Œè®¾è®¡ç†å¿µä¸Šæœ‰æ˜æ˜¾åŒºåˆ«ï¼š

##### åº•å±‚ç¡¬ä»¶è°ƒç”¨
- **å…±åŒç‚¹**ï¼šäºŒè€…éƒ½ä½¿ç”¨è‹¹æœçš„Metalå›¾å½¢APIæ¥è®¿é—®è®¡ç®—èƒ½åŠ›
- **å…±åŒç‚¹**ï¼šéƒ½å……åˆ†åˆ©ç”¨äº†Apple SiliconèŠ¯ç‰‡çš„ç»Ÿä¸€å†…å­˜æ¶æ„

##### æ¶æ„å±‚æ¬¡
- **MPS (Metal Performance Shaders)**ï¼š
  - æ˜¯ä¸€ä¸ªé€‚é…å±‚ï¼Œè®©PyTorchç­‰ç°æœ‰æ¡†æ¶èƒ½åœ¨è‹¹æœç¡¬ä»¶ä¸Šè¿è¡Œ
  - æœ¬è´¨ä¸Šæ˜¯å°†ç°æœ‰æ¡†æ¶çš„è¿ç®—ç¿»è¯‘æˆMetalæŒ‡ä»¤
  - ç±»ä¼¼"ç¿»è¯‘å™¨"è§’è‰²ï¼Œå­˜åœ¨ä¸€å®šçš„è½¬æ¢å¼€é”€

- **MLXæ¡†æ¶**ï¼š
  - è‹¹æœä»å¤´å¼€å‘çš„æ¡†æ¶ï¼Œç›´æ¥ä¸ºApple Siliconæ¶æ„ä¼˜åŒ–
  - æ— éœ€"ç¿»è¯‘"å±‚ï¼Œç›´æ¥ç”Ÿæˆé’ˆå¯¹è‹¹æœç¡¬ä»¶çš„æœ€ä¼˜æŒ‡ä»¤
  - æ›´æ·±åº¦æ•´åˆäº†è‹¹æœç¡¬ä»¶çš„ç‰¹æ€§ï¼ˆç‰¹åˆ«æ˜¯ç»Ÿä¸€å†…å­˜ï¼‰

##### æŠ€æœ¯æ€§èƒ½å¯¹æ¯”

```
æ€§èƒ½ä¼˜åŒ–å±‚æ¬¡ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     åº”ç”¨ä»£ç  (Python)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  PyTorch  â”‚    MLX      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤             â”‚
â”‚ MPSåç«¯   â”‚             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚        Metal API        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    Apple Siliconç¡¬ä»¶    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- **å†…å­˜æ•ˆç‡**ï¼šMLXé€šå¸¸æ¯”PyTorch+MPSæœ‰æ›´é«˜çš„å†…å­˜åˆ©ç”¨ç‡
- **è®¡ç®—æ€§èƒ½**ï¼šMLXåœ¨ç›¸åŒç¡¬ä»¶ä¸Šé€šå¸¸èƒ½æä¾›æ›´å¥½çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨çŸ©é˜µè¿ç®—ä¸Š
- **å¯åŠ¨æ—¶é—´**ï¼šMLXé€šå¸¸æœ‰æ›´å¿«çš„å¯åŠ¨å’Œç¼–è¯‘æ—¶é—´

#### Apple Metalè®¡ç®—èµ„æºç®¡ç†

Metalä¸ä»…é™äºGPUï¼Œå®ƒæ˜¯Appleè®¾è®¡çš„åº•å±‚å›¾å½¢å’Œè®¡ç®—APIï¼Œèƒ½å¤Ÿè®¿é—®Apple SiliconèŠ¯ç‰‡ä¸Šçš„å¤šç§è®¡ç®—èµ„æºï¼š

##### Metalçš„å…¨é¢è®¡ç®—èƒ½åŠ›
- **CPU**ï¼šåŒ…æ‹¬æ€§èƒ½æ ¸å¿ƒå’Œèƒ½æ•ˆæ ¸å¿ƒ
- **GPU**ï¼šé›†æˆçš„å›¾å½¢å¤„ç†å•å…ƒ
- **ç¥ç»å¼•æ“(Neural Engine)**ï¼šä¸“ç”¨äºæœºå™¨å­¦ä¹ åŠ é€Ÿçš„ç¡¬ä»¶
- **AMX(Apple Matrix accelerators)**ï¼šçŸ©é˜µè®¡ç®—åŠ é€Ÿå™¨
- **åª’ä½“å¼•æ“**ï¼šè§†é¢‘ç¼–è§£ç ä¸“ç”¨å¤„ç†å•å…ƒ

##### è‡ªåŠ¨èµ„æºåˆ†é…èƒ½åŠ›

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           åº”ç”¨ç¨‹åº (MLX/PyTorchç­‰)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Metalæ¡†æ¶                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“           â†“            â†“         â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   CPU   â”‚ â”‚   GPU   â”‚ â”‚ ç¥ç»å¼•æ“  â”‚ â”‚  AMX    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Metalçš„ä»»åŠ¡åˆ†é…ç­–ç•¥ï¼š

1. **æ™ºèƒ½è°ƒåº¦**ï¼šæ ¹æ®å·¥ä½œè´Ÿè½½ç‰¹æ€§è‡ªåŠ¨é€‰æ‹©é€‚å½“çš„è®¡ç®—å•å…ƒ
2. **å¼‚æ„è®¡ç®—**ï¼šå¯ä»¥åŒæ—¶åˆ©ç”¨å¤šç§è®¡ç®—èµ„æºååŒå·¥ä½œ
3. **ä¸“ç”¨åŠ é€Ÿ**ï¼šç‰¹å®šè¿ç®—ä¼šè‡ªåŠ¨è·¯ç”±åˆ°æœ€é€‚åˆçš„ç¡¬ä»¶å•å…ƒ

##### æ¡†æ¶å·®å¼‚åœ¨èµ„æºåˆ©ç”¨ä¸Š

- **MPS(Metal Performance Shaders)**ï¼š
  - ä¸»è¦ä¼˜åŒ–GPUè®¡ç®—ï¼Œå¯¹å…¶ä»–è®¡ç®—å•å…ƒæ”¯æŒæœ‰é™
  - é€šè¿‡Metalè®¿é—®ç¡¬ä»¶ï¼Œä½†ä¼˜åŒ–å±‚æ¬¡è¾ƒæµ…

- **MLX**ï¼š
  - æ·±åº¦æ•´åˆæ‰€æœ‰è®¡ç®—èµ„æºï¼ŒåŒ…æ‹¬CPUã€GPUã€ç¥ç»å¼•æ“
  - æ›´æ™ºèƒ½åœ°åˆ†é…ä¸åŒç±»å‹çš„è®¡ç®—ä»»åŠ¡åˆ°æœ€é€‚åˆçš„ç¡¬ä»¶
  - ä¸“ä¸ºæœºå™¨å­¦ä¹ ä¼˜åŒ–ï¼Œäº†è§£ä¸åŒè®¡ç®—æ¨¡å¼çš„ç‰¹ç‚¹

åœ¨è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹æ—¶ï¼ŒMetalèƒ½å¤Ÿè‡ªåŠ¨å°†ä¸åŒç±»å‹çš„è®¡ç®—ä»»åŠ¡åˆ†é…ç»™æœ€é€‚åˆçš„ç¡¬ä»¶å•å…ƒï¼š
- çŸ©é˜µä¹˜æ³•å¯èƒ½è·¯ç”±åˆ°GPUæˆ–AMX
- æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥åˆ©ç”¨GPUçš„å¹¶è¡Œèƒ½åŠ›
- æ¿€æ´»å‡½æ•°å¯èƒ½åœ¨CPUä¸Šæ‰§è¡Œ
- ç‰¹å®šMLç®—å­å¯ä»¥åˆ©ç”¨ç¥ç»å¼•æ“åŠ é€Ÿ

è¿™ç§å…¨é¢çš„è®¡ç®—èµ„æºè°ƒåº¦æ˜¯Appleè®¾å¤‡ä¸Šèƒ½å¤Ÿè¿›è¡Œå¤§æ¨¡å‹å¾®è°ƒçš„å…³é”®æŠ€æœ¯åŸºç¡€ï¼Œä¹Ÿæ˜¯ä¸ºä»€ä¹ˆå³ä½¿æ²¡æœ‰é«˜ç«¯NVIDIA GPUï¼ŒAppleè®¾å¤‡ä»èƒ½æœ‰æ•ˆæ”¯æŒæœºå™¨å­¦ä¹ å·¥ä½œè´Ÿè½½çš„æ ¹æœ¬åŸå› ã€‚

### Macè®¾å¤‡ä¸Šå¾®è°ƒå¤§è¯­è¨€æ¨¡å‹çš„é€šç”¨æ³¨æ„äº‹é¡¹

é€šè¿‡åˆ†æä¸Šè¿°ä¸¤ç¯‡å…³äºMacè®¾å¤‡ä¸Šå¾®è°ƒå¤§æ¨¡å‹çš„å®è·µæ–‡ç« (DeepSeek+MPSä¸MLXæ¡†æ¶)ï¼Œå¯ä»¥æ€»ç»“å‡ºä»¥ä¸‹é€šç”¨çš„å…³é”®æ³¨æ„äº‹é¡¹ï¼š

#### 1. å†…å­˜ç®¡ç†æ˜¯æ ¸å¿ƒæŒ‘æˆ˜

- **å†…å­˜æ°´ä½æ§åˆ¶**ï¼šè®¾ç½®`PYTORCH_MPS_HIGH_WATERMARK_RATIO`å’Œ`PYTORCH_MPS_LOW_WATERMARK_RATIO`å‚æ•°æ§åˆ¶å†…å­˜ä½¿ç”¨ä¸Šé™
- **æ‰‹åŠ¨æ¸…ç†ç¼“å­˜**ï¼šå®šæœŸä½¿ç”¨`torch.mps.empty_cache()`æˆ–MLXç­‰æ•ˆå‘½ä»¤æ¸…ç†GPUå†…å­˜
- **å†…å­˜æ³„æ¼ç›‘æ§**ï¼šç‰¹åˆ«å…³æ³¨MPSåç«¯ç¼“å­˜ç§¯ç´¯ã€å¼ é‡ç¢ç‰‡åŒ–é—®é¢˜ï¼Œé•¿æ—¶é—´è®­ç»ƒæ—¶å°¤ä¸ºé‡è¦

#### 2. è®­ç»ƒå‚æ•°ä¼˜åŒ–

- **æ‰¹æ¬¡å¤§å°é™ä½**ï¼šå°†batch sizeé™è‡³1ï¼Œè¿™æ˜¯æœ€ç›´æ¥æœ‰æ•ˆçš„å†…å­˜èŠ‚çœæ–¹æ³•
- **æ¢¯åº¦ç´¯ç§¯**ï¼šä½¿ç”¨åˆç†çš„`gradient_accumulation_steps`(å¦‚8-16)æ¥æ¨¡æ‹Ÿå¤§æ‰¹é‡è®­ç»ƒæ•ˆæœï¼Œä½†ä¸å¿…è¿‡é«˜(å¦‚64)ï¼Œè¿‡é«˜ä¼šå½±å“æ”¶æ•›é€Ÿåº¦å’Œè®­ç»ƒæ•ˆç‡
- **å¾®è°ƒå±‚æ•°é™åˆ¶**ï¼š
  - é’ˆå¯¹ç‰¹å®šå…³é”®å±‚(q_proj, v_proj, k_proj, o_proj, gate_proj, up_proj, down_proj)è¿›è¡ŒLoRAå¾®è°ƒ
  - 32GBå†…å­˜è®¾å¤‡å¯ä»¥å°è¯•ä½¿ç”¨æ›´å¤šå±‚å’Œæ›´å¤§rå€¼(å¦‚r=16)ï¼Œ16GBè®¾å¤‡å»ºè®®4å±‚æˆ–æ›´å°‘ä¸”rå€¼æ›´å°(å¦‚r=8)

#### 3. æŠ€æœ¯ç­–ç•¥å…±è¯†

- **æ¢¯åº¦æ£€æŸ¥ç‚¹**ï¼šé‡‡ç”¨`gradient_checkpointing=True`ä»¥å¹³è¡¡æ€§èƒ½å’Œå†…å­˜å ç”¨
- **ä¸“æ³¨äºLoRA**ï¼šLoRAæŠ€æœ¯æ˜¯ç›®å‰Macè®¾å¤‡ä¸Šå¾®è°ƒå¤§æ¨¡å‹çš„æœ€ä½³é€‰æ‹©ï¼Œæ·»åŠ å‚æ•°å°‘ã€å†…å­˜å ç”¨ä½
- **æ•°æ®ç±»å‹ç®¡ç†**ï¼šåœ¨MPSåç«¯ä¸Š**å¿…é¡»ç¦ç”¨fp16å’Œbf16**ï¼Œç»Ÿä¸€ä½¿ç”¨float32ä»¥é¿å…æ•°æ®ç±»å‹ä¸åŒ¹é…é”™è¯¯ï¼Œè¿™ç‚¹ä¸CUDAç¯å¢ƒä¸åŒ
- **æ•°æ®é›†è§„æ¨¡**ï¼šMacè®¾å¤‡ä¸Šå¾®è°ƒéœ€è°¨æ…æ§åˆ¶æ•°æ®é›†å¤§å°ï¼Œå»ºè®®æ ¹æ®ä»»åŠ¡å¤æ‚åº¦é€‰æ‹©é€‚å½“æ•°é‡ï¼Œåˆå§‹éªŒè¯æ—¶å¯ä»¥åªä½¿ç”¨å°‘é‡æ ·æœ¬(å¦‚200æ¡)

#### 4. æ€§èƒ½ä¸è´¨é‡å¹³è¡¡

- **è®­ç»ƒä¸æ¨ç†æƒè¡¡**ï¼šéœ€è¦åœ¨è®­ç»ƒæ•ˆç‡ä¸æ¨¡å‹æ•ˆæœé—´åšå‡ºæƒè¡¡ï¼Œå‚æ•°è¶Šå°è®­ç»ƒè¶Šå¿«ä½†æ•ˆæœå¯èƒ½è¾ƒå·®
- **è®­ç»ƒæ—¶é—´é¢„æœŸ**ï¼šç›¸æ¯”NVIDIA GPUï¼Œéœ€è¦æ¥å—æ›´é•¿çš„è®­ç»ƒæ—¶é—´
- **æ¨¡å‹è§„æ¨¡é€‰æ‹©**ï¼š
  - 16GBå†…å­˜è®¾å¤‡ï¼šæœ€å¤§å»ºè®®7Bæ¨¡å‹
  - 32GBå†…å­˜è®¾å¤‡ï¼šå¯ä»¥å¾®è°ƒ14Bçº§åˆ«æ¨¡å‹ï¼Œä½†éœ€è°¨æ…è°ƒæ•´å‚æ•°
  - 64GBåŠä»¥ä¸Šå†…å­˜è®¾å¤‡ï¼šå¯æ›´ç¨³å®šåœ°å¾®è°ƒ14Bä»¥ä¸Šæ¨¡å‹

#### 5. æ¡†æ¶é€‰æ‹©è€ƒé‡

- **PyTorch+PEFT**ï¼šæœ¬æ–‡å®è·µé‡‡ç”¨çš„æ–¹æ¡ˆï¼Œå…¼å®¹æ€§æ›´å¥½ï¼Œç”Ÿæ€æ›´ä¸°å¯Œï¼Œä½†éœ€è¦é’ˆå¯¹MPSåç«¯è¿›è¡Œå¤šé¡¹å…¼å®¹æ€§ä¿®å¤
- **MLXæ¡†æ¶**ï¼šè‹¹æœä¸“é—¨å¼€å‘çš„æ›¿ä»£æ–¹æ¡ˆï¼Œé’ˆå¯¹Apple Siliconä¼˜åŒ–ï¼Œæ€§èƒ½å¯èƒ½æ›´å¥½ï¼Œä½†ç”Ÿæ€è¾ƒæ–°ï¼Œå·¥å…·é“¾ä¸å¤Ÿä¸°å¯Œ
- **ç»Ÿä¸€å†…å­˜ä¼˜åŠ¿**ï¼šä¸¤ç§æ–¹æ³•éƒ½èƒ½å……åˆ†åˆ©ç”¨Apple SiliconèŠ¯ç‰‡çš„ç»Ÿä¸€å†…å­˜æ¶æ„

#### 6. å®ç”¨å»ºè®®

- **é¢„çƒ­é˜¶æ®µ**ï¼šä¸ºæ¨¡å‹è®¾ç½®é€‚å½“çš„é¢„çƒ­æ­¥æ•°(5-10æ­¥)ä»¥ç¨³å®šåˆå§‹è®­ç»ƒ
- **KVç¼“å­˜ä¸æ¢¯åº¦æ£€æŸ¥ç‚¹å†²çª**ï¼šåœ¨è®­ç»ƒæ—¶å¿…é¡»è®¾ç½®`model.config.use_cache = False`ä»¥é¿å…ä¸æ¢¯åº¦æ£€æŸ¥ç‚¹å†²çª
- **æ™ºèƒ½æ¨ç†å›é€€**ï¼šå®ç°æ™ºèƒ½æ¨ç†å‡½æ•°ï¼Œåœ¨MPSåç«¯å¤±è´¥æ—¶è‡ªåŠ¨å›é€€åˆ°CPU
- **åŠŸè€—ç®¡ç†**ï¼šæ³¨æ„Macè®¾å¤‡åœ¨é•¿æ—¶é—´è®­ç»ƒä¸­çš„æ•£çƒ­é—®é¢˜ï¼Œå¯èƒ½éœ€è¦å¤–éƒ¨æ•£çƒ­æ”¯æŒ
- **åˆ†æ‰¹è®­ç»ƒ**ï¼šå°†é•¿æ—¶é—´è®­ç»ƒåˆ†ä¸ºå¤šä¸ªçŸ­è®­ç»ƒé˜¶æ®µï¼Œå®šæœŸä¿å­˜æ£€æŸ¥ç‚¹ï¼Œé¿å…å› å†…å­˜é—®é¢˜è®­ç»ƒå¤±è´¥
- **è·¯å¾„å¤„ç†æ­£ç¡®æ€§**ï¼šåœ¨Pythonè„šæœ¬ä¸­ä½¿ç”¨`os.path.expanduser()`å±•å¼€åŒ…å«`~`çš„è·¯å¾„ï¼Œé¿å…è·¯å¾„é”™è¯¯

Macè®¾å¤‡ä¸Šçš„å¤§æ¨¡å‹å¾®è°ƒè™½ç„¶é¢ä¸´èµ„æºé™åˆ¶ï¼Œä½†é€šè¿‡åˆç†çš„å‚æ•°è®¾ç½®å’Œä¼˜åŒ–ç­–ç•¥ï¼Œå®Œå…¨å¯ä»¥å®ç°æœ‰æ•ˆçš„å¾®è°ƒï¼Œå°¤å…¶é€‚åˆåŸå‹å¼€å‘ã€æ¦‚å¿µéªŒè¯å’Œç‰¹å®šé¢†åŸŸçš„å°è§„æ¨¡å®šåˆ¶åŒ–éœ€æ±‚ã€‚
